<!DOCTYPE html>
<html lang="zh, en">







<head>
  






  
    
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Attention Mechanism | Jiawei Lu</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="Attention Mechanism" />
<meta name="author" content="Jiawei Lu" />
<meta property="og:locale" content="zh, en" />
<meta name="description" content="Encoder-Decoder in NLP" />
<meta property="og:description" content="Encoder-Decoder in NLP" />
<link rel="canonical" href="https://jiaweilu1999.github.io/studylog/Attention.html" />
<meta property="og:url" content="https://jiaweilu1999.github.io/studylog/Attention.html" />
<meta property="og:site_name" content="Jiawei Lu" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-04-13T00:00:00-07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Attention Mechanism" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Jiawei Lu"},"dateModified":"2022-04-13T00:00:00-07:00","datePublished":"2022-04-13T00:00:00-07:00","description":"Encoder-Decoder in NLP","headline":"Attention Mechanism","mainEntityOfPage":{"@type":"WebPage","@id":"https://jiaweilu1999.github.io/studylog/Attention.html"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://jiaweilu1999.github.io/assets/img/jiawei.jpg"},"name":"Jiawei Lu"},"url":"https://jiaweilu1999.github.io/studylog/Attention.html"}</script>
<!-- End Jekyll SEO tag -->


  

  
    <meta name="keywords" content="Jiawei Lu,jiawei lu,columbia,Deep Learning,Reinforcement Learning">
  



  <meta name="theme-color" content="rgb(25,55,71)">


<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta http-equiv="x-ua-compatible" content="ie=edge">

<meta name="mobile-web-app-capable" content="yes">

<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-title" content="Jiawei Lu">
<meta name="apple-mobile-web-app-status-bar-style" content="default">

<meta name="application-name" content="Jiawei Lu">

<meta name="generator" content="Hydejack v9.1.5" />



<link rel="alternate" href="https://jiaweilu1999.github.io/studylog/Attention.html" hreflang="zh, en">

<link type="application/atom+xml" rel="alternate" href="https://jiaweilu1999.github.io/feed.xml" title="Jiawei Lu" />


<link rel="shortcut icon"    href="/assets/icons/favicon.png">
<link rel="apple-touch-icon" href="/assets/img/favicon.png">

<link rel="manifest" href="/assets/site.webmanifest">

<link rel="dns-prefetch" href="https://fonts.googleapis.com"><link rel="dns-prefetch" href="https://fonts.gstatic.com">



<link rel="preload" href="/assets/img/swipe.svg" as="image" id="_hrefSwipeSVG">

  <link rel="dns-prefetch" href="/assets/js/search-worker-9.1.5.js" as="worker" id="_hrefSearch">





  <link rel="stylesheet" href="https://unpkg.com/applause-button/dist/applause-button.css">
  <script src="https://unpkg.com/applause-button/dist/applause-button.js"></script>


<!-- For sidebar folder -->
<link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
<script src="/assets/js/sidebar-folder.js"></script>


<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-68290006-3"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-68290006-3');
</script>

<script>!function(r,c){"use strict";function a(e,t,n,o){e.addEventListener?e.addEventListener(t,n,o):e.attachEvent?e.attachEvent("on"+t,n):e["on"+t]=n}r.loadJS=function(e,t){var n=c.createElement("script");n.src=e,t&&a(n,"load",t,{once:!0});t=c.scripts[0];return t.parentNode.insertBefore(n,t),n},r._loaded=!1,r.loadJSDeferred=function(e,t){var n=c.createElement("script");function o(){r._loaded=!0,t&&a(n,"load",t,{once:!0});var e=c.scripts[0];e.parentNode.insertBefore(n,e)}return n.src=e,r._loaded?o():a(r,"load",o,{once:!0}),n},r.setRel=r.setRelStylesheet=function(e){a(c.getElementById(e),"load",function(){this.rel="stylesheet"},{once:!0})}}(window,document);
!function(a){"use strict";var b=function(b,c,d){function e(a){return h.body?a():void setTimeout(function(){e(a)})}function f(){i.addEventListener&&i.removeEventListener("load",f),i.media=d||"all"}var g,h=a.document,i=h.createElement("link");if(c)g=c;else{var j=(h.body||h.getElementsByTagName("head")[0]).childNodes;g=j[j.length-1]}var k=h.styleSheets;i.rel="stylesheet",i.href=b,i.media="only x",e(function(){g.parentNode.insertBefore(i,c?g:g.nextSibling)});var l=function(a){for(var b=i.href,c=k.length;c--;)if(k[c].href===b)return a();setTimeout(function(){l(a)})};return i.addEventListener&&i.addEventListener("load",f),i.onloadcssdefined=l,l(f),i};"undefined"!=typeof exports?exports.loadCSS=b:a.loadCSS=b}("undefined"!=typeof global?global:this);
!function(a){if(a.loadCSS){var b=loadCSS.relpreload={};if(b.support=function(){try{return a.document.createElement("link").relList.supports("preload")}catch(b){return!1}},b.poly=function(){for(var b=a.document.getElementsByTagName("link"),c=0;c<b.length;c++){var d=b[c];"preload"===d.rel&&"style"===d.getAttribute("as")&&(a.loadCSS(d.href,d,d.getAttribute("media")),d.rel=null)}},!b.support()){b.poly();var c=a.setInterval(b.poly,300);a.addEventListener&&a.addEventListener("load",function(){b.poly(),a.clearInterval(c)}),a.attachEvent&&a.attachEvent("onload",function(){a.clearInterval(c)})}}}(this);
!function(w) {
  w._baseURL = '/';
  w._publicPath = '/assets/js/';
  w._noPushState = false;
  w._noDrawer = false;
  w._noNavbar = false;
  w._noToc = false;
  w._noSearch = false;
  w._advertise = false;
  w._search = {
    DATA_URL: '/assets/sitedata.json?no-cache',
    STORAGE_KEY: 'mini-search/',
    INDEX_KEY: 'index--2024-01-04T23:41:52-08:00',
  };
  w._clapButton = false;
}(window);</script>


<script async src="/assets/bower_components/MathJax/es5/tex-mml-chtml.js" id="_MathJax"></script>



<!--[if gt IE 8]><!---->

  




<link rel="stylesheet" href="/assets/css/hydejack-9.1.5.css" id="_stylePreload">
<link rel="stylesheet" href="/assets/icomoon/style.css" id="_iconsPreload">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans+KR%7CJetBrains+Mono%7CNanum+Gothic+Coding&display=swap" id="_fontsPreload">




<!--<![endif]-->





</head>

<body class="dark-mode no-break-layout no-large-headings">
  
<script>
  window._sunrise = 6;
  window._sunset =  18;
  !function(e,s){var d="light-mode",o="dark-mode",a=(new Date).getHours();"matchMedia"in e&&e.matchMedia("(prefers-color-scheme)")||(o=(e=a<=e._sunrise||a>=e._sunset?o:d)==o?d:o,s.body.classList.add(e),s.body.classList.remove(o))}(window,document);

</script>



<hy-push-state
  id="_pushState"
  replace-selector="#_main"
  link-selector="a[href]:not([href^='/assets/']):not(.external):not(.no-push-state)"
  script-selector="script"
  duration="500"
  hashchange
>
  
  
  <div id="_navbar" class="navbar fixed-top">
  <div class="content">
    <span class="sr-only">Jump to:</span>
    <div class="nav-btn-bar">
      <a id="_menu" class="nav-btn no-hover" href="#_drawer--opened">
        <span class="sr-only">Navigation</span>
        <span class="icon-menu"></span>
      </a>
      <div class="nav-span"></div>
    </div>
  </div>
</div>
<hr class="sr-only" hidden />

  <main
  id="_main"
  class="content layout-post"
  role="main"
>
  <nav id="breadcrumbs" class="screen-only"><ul>
  
  
    <li><a href="/">home</a></li>
    
      <li>
        
          <span>/</span>
          
          
          <a href="/studylog/">studylog</a>
        
      </li>
    
      <li>
        
          <span>/</span>
          <span>Attention.html</span>
        
      </li>
    
  
</ul></nav>

  










<article id="post-studylog-Attention" class="page post mb6" role="article">
  <header>
    <h1 class="post-title flip-project-title">
      
        Attention Mechanism
      
    </h1>

    <div class="post-date">
      
      <span class="ellipsis mr1">
        <time datetime="2022-04-13T00:00:00-07:00">13 Apr 2022</time> in <a href="/studylog/" class="flip-title">Study Blog</a> on <a href="/tag-dl/" class="flip-title">Deep Learning</a>
      </span>
      
        
      
    </div>

    
    

    



  <div class="hr pb0"></div>


  </header>

  
    <h2 id="encoder-decoder-in-nlp">Encoder-Decoder in NLP</h2>

<p>Encoder-Decoder This framework is a good illustration of the core ideas of machine learning:</p>

<blockquote>
  <p>Transforming real-world problems into mathematical problems and solving real-world problems by solving mathematical problems.</p>
</blockquote>

<ul>
  <li>
    <p>Encoder’s role is to “transform real problems into mathematical problems.”</p>
  </li>
  <li>
    <p>Decoder’s role is to “solve mathematical problems and transform them into real-world solutions.”</p>
  </li>
</ul>

<h3 id="notes">Notes</h3>
<ul>
  <li>Regardless of the length of the input and output, the length of the vector in the middle (the output of encoder) is fixed (this is also its drawback, as explained below).</li>
  <li>Different encoders and decoders can be selected depending on the task (can be one RNN But usually its variant LSTM Or CRANE).</li>
</ul>

<p>As long as it conforms to the above framework, it can be collectively referred to as the Encoder-Decoder model. Speaking of the Encoder-Decoder model, a term is often mentioned - Seq2Seq.</p>

<h3 id="seq2seq">Seq2Seq</h3>

<p>Seq2Seq (short for Sequence-to-sequence), as literally, enters a sequence and outputs another sequence. The most important aspect of this structure is that the length of the input sequence and the output sequence are variable.</p>

<h3 id="origin-of-seq2seq">Origin of Seq2Seq</h3>

<p>Before the Seq2Seq framework was proposed, deep neural networks achieved very good results in image classification and other issues. In the problem that it is good at solving, the input and output can usually be represented as a fixed-length vector. If the length is slightly changed, the zero-padding operation is used.</p>

<p>However, many important issues, such as machine translation, speech recognition, automatic dialogue, etc., are expressed in sequence, and their length is not known in advance. Therefore, how to break through the limitations of the previous deep neural network, so that it can adapt to these scenarios, has become a research hotspot since 2013, and the Seq2Seq framework came into being.</p>

<h3 id="relationship-between-seq2seq-and-encoder-decoder">Relationship between “Seq2Seq” and “Encoder-Decoder”</h3>

<p>Seq2Seq does not specifically refer to specific methods. For the purpose of “input is a sequence, output is also a sequence”, it can be collectively referred to as Seq2Seq model.</p>

<p>The specific methods used by Seq2Seq are basically in the scope of the Encoder-Decoder model.</p>

<p>To sum up:</p>

<ul>
  <li>Seq2Seq belongs to the broad category of Encoder-Decoder</li>
  <li>Seq2Seq emphasizes the purpose, Encoder-Decoder emphasizes the method</li>
</ul>

<h3 id="defects-of-encoder-decoder">Defects of Encoder-Decoder</h3>

<p>When the input information is too long, some information will be lost in the output.</p>

<p>Attention solves the problem of information loss.</p>

<h2 id="principle-of-attention">Principle of Attention</h2>

<p>Attention is a technique that mimics cognitive attention. The effect enhances some parts of the input data while diminishing other parts — the thought being that the network should devote more focus to that small but important part of the data. Learning which part of the data is more important than others depends on the context and is trained by gradient descent.</p>

<p>In a nutshell, attention in deep learning can be broadly interpreted as a vector of importance weights: in order to predict or infer one element, such as a pixel in an image or a word in a sentence, we estimate using the attention vector how strongly it is correlated with (or “attends to” as you may have read in many papers) other elements and take the sum of their values weighted by the attention vector as the approximation of the target.</p>

<h2 id="definition-of-attention-mechanism-in-neural-machine-translation">Definition of Attention Mechanism in Neural Machine Translation</h2>

<p>Say, we have a source sequence \(\mathbf{x}\) of length \(n\) and try to output a target sequence \(y\) of length \(m\):</p>

\[\begin{align}
    \mathbf{x} &amp; = [x_1, x_2, ..., x_n]\\
    \mathbf{y} &amp; = [y_1, y_2, ..., y_m]
\end{align}\]

<p>The encoder is a bidirectional RNN (or other recurrent network setting of your choice) with a forward hidden state \(\overrightarrow{\mathbf{h}_i}\) and a backward one \(\overleftarrow{\mathbf{h}_i}\). A simple concatenation of two represents the encoder state. The motivation is to include both the preceding and following words in the annotation of one word.</p>

\[\mathbf{h}_i = [\overrightarrow{\mathbf{h}_i}^\top; \overleftarrow{\mathbf{h}_i}^\top]^\top, \ i = 1,...,n\]

<p>The decoder network has hidden state \(\mathbf{s}_t = f(\mathbf{s}_{t-1}, y_{t-1}, \mathbf{c}_t)\) for the output word at position \(t\), \(t = 1, ..., m\), where the context vector \(\mathbf{c}_t\) is a sum of hidden states of the input sequence, weighted by alignment scores:</p>

\[\begin{align}
    \mathbf{c}_t &amp; = \sum_{i=1}^n \alpha_{t,i} \mathbf{h_i} \\
    \alpha_{t,i} &amp; = \text{align}(y_t, x_i) \\
    &amp; = \frac{\exp(\text{score}(\mathbf{s}_{t-1}, \mathbf{h}_i))}{\sum_{i' = 1}^n \exp(\text{score}(\mathbf{s}_{t-1}, \mathbf{h}_{i'}))}
\end{align}\]

<p>The alignment model assigns a score \(\alpha_{t, i}\) to the pair of input at position \(i\) and output at position \(t\), \((y_t, x_i)\),  based on how well they match. The set of \(\{\alpha_{t, i\}\) are weights defining how much of each source hidden state should be considered for each output. In Bahdanau’s paper, the alignment score \(\alpha\) is parametrized by a <strong>feed-forward network</strong> with a single hidden layer and this network is jointly trained with other parts of the model. The score function is therefore in the following form, given that tanh is used as the non-linear activation function:</p>

<p>\(\text{score}(\mathbf{s}_t, \mathbf{h}_i) = \mathbf{v}_a^\top \tanh \left( \mathbf{W}_a \left[ \mathbf{s}_t; \mathbf{h}_i \right] \right)\)
where both \(\mathbf{v}_a\) and \(\mathbf{W}_a\) are weight matrices to be learned in the alignment model.</p>

<h2 id="self-attention">Self-Attention</h2>
<p>Self-attention, also known as intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the same sequence. It has been shown to be very useful in machine reading, abstractive summarization, or image description generation.</p>

<p>The long short-term memory network paper used self-attention to do machine reading. In the example below, the self-attention mechanism enables us to learn the correlation between the current words and the previous part of the sentence.</p>

<h2 id="soft-vs-hard-attention">Soft vs Hard Attention</h2>
<p>In the <a href="http://proceedings.mlr.press/v37/xuc15.pdf">show, attend and tell</a> paper, attention mechanism is applied to images to generate captions. The image is first encoded by a CNN to extract features. Then a LSTM decoder consumes the convolution features to produce descriptive words one by one, where the weights are learned through attention. The visualization of the attention weights clearly demonstrates which regions of the image the model is paying attention to so as to output a certain word.</p>

<p>This paper first proposed the distinction between “soft” vs “hard” attention, based on whether the attention has access to the entire image or only a patch:</p>

<ul>
  <li>Soft Attention: the alignment weights are learned and placed “softly” over all patches in the source image; essentially the same type of attention as in <a href="https://arxiv.org/abs/1409.0473">Bahdanau et al., 2015</a>.
    <ul>
      <li>Pro: the model is smooth and differentiable.</li>
      <li>Con: expensive when the source input is large.</li>
    </ul>
  </li>
  <li>Hard Attention: only selects one patch of the image to attend to at a time.
    <ul>
      <li>Pro: less calculation at the inference time.</li>
      <li>Con: the model is non-differentiable and requires more complicated techniques such as variance reduction or reinforcement learning to train. (<a href="https://arxiv.org/abs/1508.04025">Luong, et al., 2015</a>)</li>
    </ul>
  </li>
</ul>

<h2 id="advantage-of-attention-mechanism">Advantage of Attention mechanism</h2>

<p>3 main reasons:</p>

<ul>
  <li>Less source</li>
</ul>

<p>In comparison, the complexity is smaller and the parameters are less than CNN and RNN based model. Therefore, the requirements for computing power are even smaller.</p>

<ul>
  <li>High speed</li>
</ul>

<p>Attention solves the problem that RNN cannot be computed in parallel. Each step of the Attention mechanism calculation does not depend on the calculation results of the previous step, so it can be processed in parallel as CNN.</p>

<ul>
  <li>Good result</li>
</ul>

<p>Before the introduction of the Attention mechanism, there was a problem that everyone had been annoyed: long-distance information would be weakened, just like people with weak memory, and the same thing could not be remembered in the past.</p>

<p>However, Attention can make model focus without losing important information.</p>

<h2 id="reference">Reference</h2>
<p>[1] Lil’Log: Attention? Attention! (<a href="https://lilianweng.github.io/posts/2018-06-24-attention/#definition">Link</a>)</p>

<p>[2] Zhihu BLOG (<a href="https://zhuanlan.zhihu.com/p/91839581">Link</a>)</p>

  
</article>



<applause-button class="mb6"
    color=rgb(79,177,186)
    url=https://JiaweiLu1999.github.io/studylog/Attention.html >
  </applause-button>






  
     


  <aside class="about related mt4 mb4" role="complementary">
    
    

<div class="author mt4">
  

  

  
  
  <h2  class="page-title hr-bottom">
    About
  </h2>

  <p>MSEE student at <a href="https://www.ee.columbia.edu/">Columbia Univeristy</a>.<br />
Please check my <a href="/resume/">Resume</a> to find out more about me!<br /></p>



  <div class="sidebar-social">
    <span class="sr-only">Social:</span>
<ul>
  
    
      



  

  
  
  
  

  

  

  <li>
    <a href="https://www.linkedin.com/in/jiaweilucolumbia" title="LinkedIn" class="no-mark-external">
      <span class="icon-linkedin2"></span>
      <span class="sr-only">LinkedIn</span>
    </a>
  </li>


    
      



  

  
  
  
  

  

  

  <li>
    <a href="https://github.com/JiaweiLu1999" title="GitHub" class="no-mark-external">
      <span class="icon-github"></span>
      <span class="sr-only">GitHub</span>
    </a>
  </li>


    
      



  

  
  
  
  

  

  

  <li>
    <a href="https://instagram.com/javeylew" title="Instagram" class="no-mark-external">
      <span class="icon-instagram"></span>
      <span class="sr-only">Instagram</span>
    </a>
  </li>


    
      



  

  
  
  
  

  

  

  <li>
    <a href="mailto:jl5999@columbia.edu" title="Email" class="no-mark-external">
      <span class="icon-mail"></span>
      <span class="sr-only">Email</span>
    </a>
  </li>


    
  
</ul>

  </div>
</div>

  </aside>


  

  
    
<aside class="comments related" role="complementary">
  <h2 class="hr-bottom">Comments</h2>
  

  
  
    <script src="https://giscus.app/client.js"
            data-repo=JiaweiLu1999/JiaweiLu1999.github.io
            data-repo-id=R_kgDOGt0vAg
            data-category=Comments
            data-category-id=DIC_kwDOGt0vAs4CA218
            data-mapping=pathname
            data-reactions-enabled=1
            data-emit-metadata=0
            data-theme=dark_dimmed
            data-lang=en
            crossorigin=anonymous
            async>
    </script>
  


</aside>


  


  
<footer class="content" role="contentinfo">
  <hr/>
  
    <p><small class="copyright">© 2022 Jiawei Lu. All rights reserved.
</small></p>
  
  
  
  <hr class="sr-only"/>
</footer>


</main>

  <hy-drawer
  id="_drawer"
  class=""
  side="left"
  threshold="10"
  noscroll
  
>
  <header id="_sidebar" class="sidebar" role="banner">
    




<div class="sidebar-bg sidebar-overlay" style="background-color:rgb(25,55,71);background-image:url(/assets/img/sidebar-bg.jpg)"></div>

    <div class="sidebar-sticky no-fouc">
  <div class="sidebar-about">
    
      <a class="no-hover" href="/" tabindex="-1">
        <img src="/assets/img/jiawei.jpg" class="avatar" alt="Jiawei Lu" width="120" height="120" loading="lazy" />
      </a>
    
    <a class="sidebar-title" href="/"><h2 class="h1">Jiawei Lu</h2></a>
    
    
      <p class="">
        MSEE @ Columbia

      </p>
    
  </div>

  <nav class="sidebar-nav heading" role="navigation">
    <span class="sr-only">Navigation:</span>
<ul>
  
  
  
    
      
    
  
    
      
    
  
    
      
    
  
    
      
    
  
    
      
    
  
    
      
    
  
    
      
    
  
    
      
    
  
    
      
        
        
        <li>
          
          <div class="list-wrapper">
            <a  href="/about/" class="sidebar-nav-item"  >About Me</a>
            
          </div>
          
        </li>
      
    
  
    
      
        
        
        <li>
          
          <div class="list-wrapper">
            <a  href="/resume/" class="sidebar-nav-item"  >Resume</a>
            
          </div>
          
        </li>
      
    
  
    
      
        
        
        <li>
          
          <div class="list-wrapper">
            <a  href="/projects.html" class="sidebar-nav-item"  >Projects</a>
            
          </div>
          
        </li>
      
    
  
    
      
        
        
        <li>
          
            <input type="checkbox" id="folder-checkbox-12" />
          
          <div class="list-wrapper">
            <a  href="/studylog/" class="sidebar-nav-item"  >Study Blog</a>
            
              <button class="spread-btn" onclick="javascript:spread(12)">
                <label id="spread-icon-12" class="material-icons">arrow_right</label>
              </button>
            
          </div>
          
            <ul class="list-body">
                <li>
                  <a class="sidebar-nav-subitem" href="/tag-zklab/">Zoran Kostic Lab</a>
                </li>
            
          
            
                <li>
                  <a class="sidebar-nav-subitem" href="/tag-cv/">Computer Vision</a>
                </li>
            
          
            
                <li>
                  <a class="sidebar-nav-subitem" href="/tag-diary/">Diary</a>
                </li>
            
          
            
                <li>
                  <a class="sidebar-nav-subitem" href="/tag-dl/">Deep Learning</a>
                </li>
            
          
            
                <li>
                  <a class="sidebar-nav-subitem" href="/tag-ml/">Machine Learning</a>
                </li>
            
          
            
                <li>
                  <a class="sidebar-nav-subitem" href="/tag-nlp/">Natrual Laguage Processing</a>
                </li>
            
          
            
                <li>
                  <a class="sidebar-nav-subitem" href="/tag-cn/">Computer Networking</a>
                </li>
            
          
            
                <li>
                  <a class="sidebar-nav-subitem" href="/tag-coding-interview/">Coding Interview</a>
                </li>
            </ul>
          
        </li>
      
    
  
    
      
        
        
        <li>
          
          <div class="list-wrapper">
            <a  href="/tags/" class="sidebar-nav-item"  >Tags</a>
            
          </div>
          
        </li>
      
    
  
</ul>

  </nav>

  
  <div class="sidebar-social">
    <span class="sr-only">Social:</span>
<ul>
  
    
      



  

  
  
  
  

  

  

  <li>
    <a href="https://www.linkedin.com/in/jiaweilucolumbia" title="LinkedIn" class="no-mark-external">
      <span class="icon-linkedin2"></span>
      <span class="sr-only">LinkedIn</span>
    </a>
  </li>


    
      



  

  
  
  
  

  

  

  <li>
    <a href="https://github.com/JiaweiLu1999" title="GitHub" class="no-mark-external">
      <span class="icon-github"></span>
      <span class="sr-only">GitHub</span>
    </a>
  </li>


    
      



  

  
  
  
  

  

  

  <li>
    <a href="https://instagram.com/javeylew" title="Instagram" class="no-mark-external">
      <span class="icon-instagram"></span>
      <span class="sr-only">Instagram</span>
    </a>
  </li>


    
      



  

  
  
  
  

  

  

  <li>
    <a href="mailto:jl5999@columbia.edu" title="Email" class="no-mark-external">
      <span class="icon-mail"></span>
      <span class="sr-only">Email</span>
    </a>
  </li>


    
  
</ul>

  </div>
</div>

  </header>
</hy-drawer>
<hr class="sr-only" hidden />

</hy-push-state>


  <!--[if gt IE 10]><!---->
<script nomodule>!function(){var t,n=document.createElement("script");!("noModule"in n)&&"onbeforeload"in n&&(t=!1,document.addEventListener("beforeload",function(e){if(e.target===n)t=!0;else if(!e.target.hasAttribute("nomodule")||!t)return;e.preventDefault()},!0),n.type="module",n.src=".",document.head.appendChild(n),n.remove())}();
</script>
<script src="/assets/js/hydejack-9.1.5.js" type="module"></script>
<script src="/assets/js/LEGACY-hydejack-9.1.5.js" nomodule defer></script>




<script type="module">
  if ('serviceWorker' in navigator) {
    /**/
    navigator.serviceWorker.getRegistration()
      .then(r => r.unregister())
      .catch(() => {});
    /**/
  }
</script>
<!--<![endif]-->

  


<div hidden>
  
  <h2 class="sr-only">Templates (for web app):</h2>

  <clap-config>
    <clap-text at="1">Keep going!</clap-text>
    <clap-text at="2">Keep going ×2!</clap-text>
    <clap-text at="3">Give me more!</clap-text>
    <clap-text at="5">Thank you, thank you</clap-text>
    <clap-text at="7">Far too kind!</clap-text>
    <clap-text at="10">Never gonna give me up?</clap-text>
    <clap-text at="14">Never gonna let me down?</clap-text>
    <clap-text at="20">Turn around and desert me!</clap-text>
    <clap-text at="30">You're an addict!</clap-text>
    <clap-text at="40">Son of a clapper!</clap-text>
    <clap-text at="50">No way</clap-text>
    <clap-text at="60">Go back to work!</clap-text>
    <clap-text at="70">This is getting out of <em>hand</em></clap-text>
    <clap-text at="80">Unbelievable</clap-text>
    <clap-text at="90">PREPOSTEROUS</clap-text>
    <clap-text at="100">I N S A N I T Y</clap-text>
    <clap-text at="185"><span style="font-family:monospace">FEED ME A STRAY CAT</span></clap-text>
  </clap-config>

  <template id="_animation-template">
  <div class="animation-main fixed-top">
    <nav id="breadcrumbs" class="screen-only"><ul>
  
  
</ul></nav>

    <div class="content">
      <div class="page"></div>
    </div>
  </div>
</template>

  <template id="_loading-template">
  <div class="loading nav-btn fr">
    <span class="sr-only">Loading…</span>
    <span class="icon-cog"></span>
  </div>
</template>

  <template id="_error-template">
  <div class="page">
    <h1 class="page-title">Error</h1>
    
    
    <p class="lead">
      Sorry, an error occurred while loading <a class="this-link" href=""></a>.

    </p>
  </div>
</template>

  <template id="_permalink-template">
  <a href="#" class="permalink">
    <span class="sr-only">Permalink</span>
    <span class="content-hash"></span>
  </a>
</template>

  
    <template id="_cookies-banner-template">
  <div id="_cookies-banner" class="navbar fixed-bottom CookiesOK">
    <div class="content">
      <div class="nav-btn-bar">
        <small class="faded">
          <span>This site uses cookies. <a href="/cookies-policy/">Cookies Policy</a>.
</span>
          <button id="_cookies-ok" class="btn btn-primary btn-sm">Okay</button>
        </small>
      </div>
    </div>
  </div>
</template>

  
  
    <template id="_dark-mode-template">
  <button id="_dark-mode" class="nav-btn no-hover" >
    <span class="sr-only">Dark Mode</span>
    <span class="icon-brightness-contrast"></span>
  </button>
</template>

  
  
    <template id="_search-template">
  <button id="_search" class="nav-btn no-hover">
    <label class="sr-only" for="_search-input">Search</label>
    <span class="icon-search"></span>
  </button>
  <div id="_search-box">
    <div class="nav-btn">
      <span class="icon-search"></span>
    </div>
    <input
      id="_search-input"
      type="search"
      class="form-control form-control-lg nav-btn"
      placeholder="Build with JEKYLL_ENV=production to enable search."
    />
    <button type="reset" class="nav-btn no-hover">
      <span class="sr-only">Close</span>
      <span class="icon-cross"></span>
    </button>
  </div>
  <div id="_hits"></div>
</template>

  
</div>


</body>
</html>
