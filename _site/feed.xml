<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="zh, en"><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="zh, en" /><updated>2022-02-27T00:56:18-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Jiawei Lu</title><subtitle>Hello World!  I&apos;m Jiawei Lu, a Master Student at Columbia University.  My research interest is Computer Vision, Deep Learning and Reinforcement Learning.
</subtitle><author><name>Jiawei Lu</name><email>jl5999@columbia.edu</email></author><entry><title type="html">Object Detection for Dummies Part 1: Gradient Vector, HOG, and SS</title><link href="http://localhost:4000/studylog/ODDP1.html" rel="alternate" type="text/html" title="Object Detection for Dummies Part 1: Gradient Vector, HOG, and SS" /><published>2022-02-09T00:00:00-05:00</published><updated>2022-02-09T00:00:00-05:00</updated><id>http://localhost:4000/studylog/ODDP1</id><content type="html" xml:base="http://localhost:4000/studylog/ODDP1.html">&lt;p&gt;Jan 28, 2021.
The raw blog &lt;a href=&quot;https://lilianweng.github.io/lil-log/2017/10/29/object-recognition-for-dummies-part-1.html&quot;&gt;URL&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;image-gradient-vector&quot;&gt;Image Gradient Vector&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Derivative
    &lt;ul&gt;
      &lt;li&gt;Scalar&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Directional Derivative
    &lt;ul&gt;
      &lt;li&gt;Scalar&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Gradient
    &lt;ul&gt;
      &lt;li&gt;Vector
        &lt;pre&gt;&lt;code class=&quot;language-python=&quot;&gt;import numpy as np
import scipy.signal as sig
data = np.array([[0, 105, 0], [40, 255, 90], [0, 55, 0]])
G_x = sig.convolve2d(data, np.array([[-1, 0, 1]]), mode=&apos;valid&apos;)
G_y = sig.convolve2d(data, np.array([[-1], [0], [1]]), mode=&apos;valid&apos;)
&lt;/code&gt;&lt;/pre&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Common Image Processing Kernels(&lt;a href=&quot;https://zhuanlan.zhihu.com/p/67197912&quot;&gt;Useful URL&lt;/a&gt;)
    &lt;ul&gt;
      &lt;li&gt;Prewitt operator&lt;/li&gt;
      &lt;li&gt;Sobel operator&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;histogram-of-oriented-gradients-hog&quot;&gt;Histogram of Oriented Gradients (HOG)&lt;/h2&gt;
&lt;p&gt;Useful &lt;a href=&quot;https://zhuanlan.zhihu.com/p/85829145&quot;&gt;URL&lt;/a&gt; in zhihu.&lt;/p&gt;

&lt;h3 id=&quot;how-hog-works&quot;&gt;How HOG works&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Preprocess the image, including resizing and color normalization.
    &lt;ul&gt;
      &lt;li&gt;Gamma Correction&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;\begin{equation}
f(x) = x^{\gamma}
\end{equation}&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python=&quot;&gt;# Gamma Correction
import cv2
import numpy as np
img = cv2.imread(&apos;gamma.jpg&apos;, 0)
img2 = np.power(img/float(np.max(img)), 1.5)
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
  &lt;li&gt;Compute the gradient vector of every pixel, as well as its magnitude and direction&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;\begin{equation}
g = \sqrt{g_x^2+g_y^2} \ 
\theta = \arctan \frac{g_y}{g_x}
\end{equation}&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python=&quot;&gt;import cv2
import numpy as np

# Read image
img = cv2.imread(&apos;runner.jpg&apos;)
img = np.float32(img) / 255.0  # 归一化

# x,y gradient
gx = cv2.Sobel(img, cv2.CV_32F, 1, 0, ksize=1)
gy = cv2.Sobel(img, cv2.CV_32F, 0, 1, ksize=1)

# gradient
mag, angle = cv2.cartToPolar(gx, gy, angleInDegrees=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Divide the image into many 8x8 pixel cells. In each cell, the magnitude values of these 64 cells are binned and cumulatively added into 9 buckets of unsigned direction (no sign, so 0-180 degree rather than 0-360 degree; this is a practical choice based on empirical experiments).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Then we slide a 2x2 cells (thus 16x16 pixels) block across the image. In each block region, 4 histograms of 4 cells are concatenated into one-dimensional vector of 36 values and then normalized to have an unit weight. The final HOG feature vector is the concatenation of all the block vectors. It can be fed into a classifier like SVM for learning object recognition tasks.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&quot;language-python=&quot;&gt;# HOG
from skimage import feature, exposure
import cv2
image = cv2.imread(&apos;/home/zxd/Pictures/Selection_018.jpg&apos;)
fd, hog_image = feature.hog(image, orientations=9, pixels_per_cell=(16, 16),
                    cells_per_block=(2, 2), visualize=True)

# Rescale histogram for better display
hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))

cv2.imshow(&apos;img&apos;, image)
cv2.imshow(&apos;hog&apos;, hog_image_rescaled)
cv2.waitKey(0)==ord(&apos;q&apos;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;image-segmentation-felzenszwalbs-algorithm&quot;&gt;Image Segmentation (Felzenszwalb’s Algorithm)&lt;/h2&gt;

&lt;h3 id=&quot;graph-construction&quot;&gt;Graph Construction&lt;/h3&gt;

&lt;p&gt;There are two approaches to constructing a graph out of an image.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Grid Graph: Each pixel is only connected with surrounding neighbours (8 other cells in total). The edge weight is the absolute difference between the intensity values of the pixels.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Nearest Neighbor Graph: Each pixel is a point in the feature space (x, y, r, g, b), in which (x, y) is the pixel location and (r, g, b) is the color values in RGB. The weight is the Euclidean distance between two pixels’ feature vectors.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>Jiawei Lu</name><email>jl5999@columbia.edu</email></author><category term="studylog" /><category term="dl" /><summary type="html">Jan 28, 2021. The raw blog URL.</summary></entry><entry><title type="html">Leetcode</title><link href="http://localhost:4000/studylog/leetcode.html" rel="alternate" type="text/html" title="Leetcode" /><published>2022-02-02T00:00:00-05:00</published><updated>2022-02-02T00:00:00-05:00</updated><id>http://localhost:4000/studylog/leetcode</id><content type="html" xml:base="http://localhost:4000/studylog/leetcode.html">&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Two sum&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Hashmap&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Add two numbers
    &lt;ul&gt;
      &lt;li&gt;Linked List&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Longest Substring Without Repeating Characters
    &lt;ul&gt;
      &lt;li&gt;Two pointers&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Median of Two Sorted Arrays
    &lt;ul&gt;
      &lt;li&gt;Median&lt;/li&gt;
      &lt;li&gt;Binary Search&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Longest Palindromic Substring
    &lt;ul&gt;
      &lt;li&gt;DP&lt;/li&gt;
      &lt;li&gt;Java vs Python&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Regular Expression Matching
    &lt;ul&gt;
      &lt;li&gt;Recursion&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Container With Most Water
    &lt;ul&gt;
      &lt;li&gt;Two pointers&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;3 Sum
    &lt;ul&gt;
      &lt;li&gt;2 sum&lt;/li&gt;
      &lt;li&gt;Hashmap&lt;/li&gt;
      &lt;li&gt;sort&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Letter Combinations of a Phone Number
    &lt;ul&gt;
      &lt;li&gt;Hashmap&lt;/li&gt;
      &lt;li&gt;Recursion&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Remove Nth Node From End of List
    &lt;ul&gt;
      &lt;li&gt;Symmetric&lt;/li&gt;
      &lt;li&gt;Two pointers&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Valid Parentheses
    &lt;ul&gt;
      &lt;li&gt;Stack&lt;/li&gt;
      &lt;li&gt;Hashmap for pair&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Generate Parentheses
    &lt;ul&gt;
      &lt;li&gt;DFS&lt;/li&gt;
      &lt;li&gt;Stack&lt;/li&gt;
      &lt;li&gt;DP&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Merge k Sorted Lists
    &lt;ul&gt;
      &lt;li&gt;Priority Queue
        &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;Queue&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PriorityQueue&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;Python &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sorted(list)&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Reverse Nodes in k-Group
    &lt;ul&gt;
      &lt;li&gt;Reverse list&lt;/li&gt;
      &lt;li&gt;6 pointers: dummy, jump, l, r, prev, cur&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Next Permutation
    &lt;ul&gt;
      &lt;li&gt;Find Pattern: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nums[i-1] &amp;lt; nums[i]&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Decreasing List&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Longest Valid Parentheses
    &lt;ul&gt;
      &lt;li&gt;Find Pattern: If ‘)’ more than ‘(‘, reset&lt;/li&gt;
      &lt;li&gt;Stack&lt;/li&gt;
      &lt;li&gt;Two traverse&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Search in Rotated Sorted Array
    &lt;ul&gt;
      &lt;li&gt;Binary Search&lt;/li&gt;
      &lt;li&gt;Mind &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;left &amp;lt;= mid&lt;/code&gt; since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mid = (left + right)//2&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Find First and Last Position of Element in Sorted Array
    &lt;ul&gt;
      &lt;li&gt;Binary Search&lt;/li&gt;
      &lt;li&gt;Find left most: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;left = mid + 1, right = mid&lt;/code&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;python = 
  while left &amp;lt; right:
      mid = (left + right) // 2
      if nums[mid] &amp;lt; target:
          left = mid + 1
      else:
          right = mid
 &lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Find right most: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;left = mid, right = mid - 1&lt;/code&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;python = 
  while left &amp;lt; right:
      mid = (left + right + 1) // 2
      if nums[mid] &amp;gt; target:
          right = mid - 1
      else:
          left = mid
 &lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Search Insert Position
    &lt;ul&gt;
      &lt;li&gt;Binary Search&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Combination Sum
    &lt;ul&gt;
      &lt;li&gt;Backtracking/DFS
        &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;dfs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;dfs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;First Missing Positive
    &lt;ul&gt;
      &lt;li&gt;Find Pattern: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[1,2,...,n+1]&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Hash &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nums[nums[i]%n] += n&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Trapping Rain Water
    &lt;ul&gt;
      &lt;li&gt;DP: store leftMax and rightMax&lt;/li&gt;
      &lt;li&gt;Two pointers&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Jump Game II
    &lt;ul&gt;
      &lt;li&gt;Two pointers: left for n steps, right for n+1 steps&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Permutations
    &lt;ul&gt;
      &lt;li&gt;DFS&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Rotate Image
    &lt;ul&gt;
      &lt;li&gt;List transportation with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zip()&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rotate = flip + trans&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Group Anagrams
    &lt;ul&gt;
      &lt;li&gt;permutations have the same characters: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sorted()&lt;/code&gt; + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dict&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Maximum Subarray
    &lt;ul&gt;
      &lt;li&gt;Divide and Conquer&lt;/li&gt;
      &lt;li&gt;DP&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Jump Game
    &lt;ul&gt;
      &lt;li&gt;two pointers&lt;/li&gt;
      &lt;li&gt;from end to start: where is the last reachable point&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Merge Intervals
    &lt;ul&gt;
      &lt;li&gt;Graph&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sort()&lt;/code&gt; + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max()&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Unique Paths
    &lt;ul&gt;
      &lt;li&gt;DP&lt;/li&gt;
      &lt;li&gt;Math $C_m^n$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Minimum Path Sum
    &lt;ul&gt;
      &lt;li&gt;DP&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Climbing Stairs
    &lt;ul&gt;
      &lt;li&gt;DP&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Edit Distance
    &lt;ul&gt;
      &lt;li&gt;DP&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Search a 2D Matrix
    &lt;ul&gt;
      &lt;li&gt;Binary Search: $m \times n$ sorted array&lt;/li&gt;
      &lt;li&gt;if element not in list, find the left boundary: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mid = (left + right)//2&lt;/code&gt;,  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;right -= 1&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Sort Colors
    &lt;ul&gt;
      &lt;li&gt;count sort&lt;/li&gt;
      &lt;li&gt;*Dutch National Flag Problem&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Minimum Window Substring
    &lt;ul&gt;
      &lt;li&gt;Sliding window, two pointers&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Subsets
    &lt;ul&gt;
      &lt;li&gt;Backtracking&lt;/li&gt;
      &lt;li&gt;Bitmask&lt;/li&gt;
      &lt;li&gt;DP&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Word Search
    &lt;ul&gt;
      &lt;li&gt;Backtracking / DFS&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;oa&quot;&gt;OA&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;Merge Sort: Counting Inversions&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&quot;language-python=&quot;&gt;#!/bin/python3

import math
import os
import random
import re
import sys

#
# Complete the &apos;countInversions&apos; function below.
#
# The function is expected to return a LONG_INTEGER.
# The function accepts INTEGER_ARRAY arr as parameter.
#

def countInversions(arr):
    # Write your code here
    temp_arr = [0]*len(arr)
    return mergeSort(arr, temp_arr, 0, len(arr) - 1)

def mergeSort(arr, temp_arr, left, right):
    inv_cnt = 0
    if left &amp;lt; right:
        mid = (left + right) // 2
        inv_cnt += mergeSort(arr, temp_arr, left, mid)
        inv_cnt += mergeSort(arr, temp_arr, mid + 1, right)
        inv_cnt += merge(arr, temp_arr, left, mid, right)
    return inv_cnt

def merge(arr, temp_arr, left, mid, right):
    i, j, k = left, mid+1, left
    inv_cnt = 0
    while i &amp;lt;= mid and j &amp;lt;= right:
        if arr[i] &amp;gt; arr[j]:
            inv_cnt += mid - i + 1
            temp_arr[k] = arr[j]
            j += 1

        else:
            temp_arr[k] = arr[i]
            i += 1
        k += 1
    
    while i &amp;lt;= mid:
        temp_arr[k] = arr[i]
        i += 1
        k += 1
    while j &amp;lt;= right:
        temp_arr[k] = arr[j]
        j += 1
        k += 1
    
    for x in range(left, right + 1):
        arr[x] = temp_arr[x]
    
    return inv_cnt
    

if __name__ == &apos;__main__&apos;:
    fptr = open(os.environ[&apos;OUTPUT_PATH&apos;], &apos;w&apos;)

    t = int(input().strip())

    for t_itr in range(t):
        n = int(input().strip())

        arr = list(map(int, input().rstrip().split()))

        result = countInversions(arr)

        fptr.write(str(result) + &apos;\n&apos;)

    fptr.close()

&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Stars and Bars:
```python=
def stars_and_bars(s, startIndex, endIndex):
 bars = {}&lt;/p&gt;

    &lt;p&gt;cnt = 0
 for i in range(len(s)):
     if s[i] == “|”:
         bars[i] = cnt
         cnt += 1
 lst = list(bars)
 startIndex, endIndex = [x-1 for x in startIndex], &lt;br /&gt;
                        [x-1 for x in endIndex]
 start_bar, end_bar = [], []&lt;/p&gt;

    &lt;p&gt;for i in range(len(startIndex)):
     start_bar.append(binary_start(lst, startIndex[i]))
 for i in range(len(endIndex)):
     end_bar.append(binary_end(lst, endIndex[i]))&lt;/p&gt;

    &lt;p&gt;print(bars)
 print(start_bar, end_bar)&lt;/p&gt;

    &lt;p&gt;res = 0
 for i in range(len(start_bar)):&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; res += max(0, (end_bar[i] - start_bar[i] - 1) - \
            (bars[end_bar[i]] - bars[start_bar[i]] - 1))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;return res&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;def binary_start(nums, target):
    left, right = 0, len(nums) -1&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;while left &amp;lt; right:
    mid = (left+right)//2
    if nums[mid] == target:
        return nums[mid]
    elif nums[mid] &amp;gt; target:
        right = mid
    else:
        left = mid + 1
return nums[right]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;def binary_end(nums, target):
    left, right = 0, len(nums) -1&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;while left &amp;lt; right:
    mid = (left+right+1)//2
    if nums[mid] == target:
        return nums[mid]
    elif nums[mid] &amp;gt; target:
        right = mid - 1
    else:
        left = mid  
return nums[left]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;```&lt;/p&gt;</content><author><name>Jiawei Lu</name><email>jl5999@columbia.edu</email></author><category term="studylog" /><category term="coding-interview" /><summary type="html">Two sum</summary></entry><entry><title type="html">NLP Lecture 02</title><link href="http://localhost:4000/studylog/NLP_lecture02.html" rel="alternate" type="text/html" title="NLP Lecture 02" /><published>2022-01-26T00:00:00-05:00</published><updated>2022-01-26T00:00:00-05:00</updated><id>http://localhost:4000/studylog/NLP_lecture02</id><content type="html" xml:base="http://localhost:4000/studylog/NLP_lecture02.html">&lt;p&gt;Keywords: Language Classification, Probability Review, Machine Learning Background, Naive Bayes’ Classifier.&lt;/p&gt;

&lt;h2 id=&quot;linguistic-terminology&quot;&gt;Linguistic Terminology&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Sentence: Unit of written language.&lt;/li&gt;
  &lt;li&gt;Utterance: Unit of spoken language.&lt;/li&gt;
  &lt;li&gt;Word Form: the inflected form as it actually appears in the corpus. “produced”&lt;/li&gt;
  &lt;li&gt;Word Stem: The part of the word that never changes between morphological variations. “produc”&lt;/li&gt;
  &lt;li&gt;Lemma: an abstract base form, shared by word forms, having the same stem, part of speech, and word sense – stands for the class of words with stem. “produce”&lt;/li&gt;
  &lt;li&gt;Type: number of distinct words in a corpus (vocabulary size).&lt;/li&gt;
  &lt;li&gt;Token: Total number of word occurrences.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tokenization&quot;&gt;Tokenization&lt;/h2&gt;
&lt;p&gt;The process of segmenting text (a sequence of characters) into a sequence of tokens (words).&lt;/p&gt;

&lt;h2 id=&quot;lemmatization&quot;&gt;Lemmatization&lt;/h2&gt;
&lt;p&gt;Converting Lemmas into their base form.&lt;/p&gt;

&lt;h2 id=&quot;probabilities-in-nlp&quot;&gt;Probabilities in NLP&lt;/h2&gt;
&lt;p&gt;Probabilities make it possible to combine evidence from multiple sources systematically to (using Bayesian statistics).&lt;/p&gt;

&lt;h3 id=&quot;bayesian-statistics&quot;&gt;Bayesian Statistics&lt;/h3&gt;

&lt;p&gt;Typically, we observe some evidence (for example, words in a document) and the goal is to infer the “correct” interpretation (for example, the topic of a text). Probabilities express the degree of belief we have in the possible interpretations.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Prior probabilities: Probability of an interpretation prior to seeing any evidence.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Conditional (Posterior) probability: Probability of an interpretation after taking evidence into account.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;probability-basics&quot;&gt;Probability Basics&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;sample space \(\Omega\)&lt;/li&gt;
  &lt;li&gt;random variable \(X\)&lt;/li&gt;
  &lt;li&gt;probability distribution \(P(\omega)\)&lt;/li&gt;
  &lt;li&gt;joint probability: \(P(A, B)\)&lt;/li&gt;
  &lt;li&gt;conditional probability: 
\(P(A|B) = \frac{P(A,B)}{P(B)}\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;bayes-rule&quot;&gt;Bayes’ Rule&lt;/h3&gt;
&lt;p&gt;\begin{equation}
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;independence&quot;&gt;Independence&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Independent:
    &lt;ul&gt;
      &lt;li&gt;
\[P(A) = P(A|B)\]
      &lt;/li&gt;
      &lt;li&gt;
\[P(A, B) = P(A) \cdot P(B)\]
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Conditionally Independent
    &lt;ul&gt;
      &lt;li&gt;
\[P(B, C|A) = P(B|A) \cdot P(C|A)\]
      &lt;/li&gt;
      &lt;li&gt;
\[P(B|A,C) =P(B|A)\]
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;probabilities-and-supervised-learning&quot;&gt;Probabilities and Supervised Learning&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Given: Training data consisting of training examples \(data = (x_1, y_1), …, (x_n, y_n)\).&lt;/li&gt;
  &lt;li&gt;Goal: Learn a mapping \(h\) from \(x\) to \(y\).&lt;/li&gt;
  &lt;li&gt;Two approaches:
    &lt;ul&gt;
      &lt;li&gt;Discriminative algorithms learn 
  \(P(y | x)\) 
  directly.&lt;/li&gt;
      &lt;li&gt;Generative algorithms use Bayes rule
  \begin{equation}
  P(y|x) = \frac{P(x|y) \cdot P(y)}{P(x)}
  \end{equation}&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;discriminative-algorithms&quot;&gt;Discriminative Algorithms&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Model conditional distribution of the label given the data&lt;/li&gt;
  &lt;li&gt;Learns decision boundaries that separate instances of the different classes.&lt;/li&gt;
  &lt;li&gt;To predict a new example, check on which side of the decision boundary it falls.&lt;/li&gt;
  &lt;li&gt;Examples:
    &lt;ul&gt;
      &lt;li&gt;support vector machine (SVM)&lt;/li&gt;
      &lt;li&gt;decision trees&lt;/li&gt;
      &lt;li&gt;random forests&lt;/li&gt;
      &lt;li&gt;neural networks&lt;/li&gt;
      &lt;li&gt;log-linear models&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;generative-algorithms&quot;&gt;Generative Algorithms&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Assume the observed data is being “generated” by a “hidden” class label.&lt;/li&gt;
  &lt;li&gt;Build a different model for each class.&lt;/li&gt;
  &lt;li&gt;To predict a new example, check it under each of the models and see which one matches best.&lt;/li&gt;
  &lt;li&gt;Estimate \(P(x|y)\) and \(P(y)\). Then use bases rule
  \begin{equation}
  P(y|x) = \frac{P(x|y) \cdot P(y)}{P(x)}
  \end{equation}&lt;/li&gt;
  &lt;li&gt;Examples:
    &lt;ul&gt;
      &lt;li&gt;Naive Bayes&lt;/li&gt;
      &lt;li&gt;Hidden Markov Models&lt;/li&gt;
      &lt;li&gt;Gaussian Mixture Models&lt;/li&gt;
      &lt;li&gt;PCFGs&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;naive-bayes&quot;&gt;Naive Bayes&lt;/h2&gt;

&lt;h3 id=&quot;rules&quot;&gt;Rules&lt;/h3&gt;
&lt;p&gt;\begin{equation}
P(Label, X_1, …, X_d) = P(Label) \Pi_i P(X_i|Label)
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
\begin{split}
    P(Label|X_1, …, X_d) &amp;amp; = \frac{P(Label) \Pi_i P(X_i|Label)}{\Pi_i P(X_i)} &lt;br /&gt;
    &amp;amp; = \alpha [P(Label) \Pi_i P(X_i|Label)]
\end{split}
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;naive-bayes-classifier&quot;&gt;Naive Bayes Classifier&lt;/h3&gt;
&lt;p&gt;\begin{equation}
y* = \arg \max_y P(y) \Pi_i P(x_i|y)
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;training-the-naive-bayes-classifier&quot;&gt;Training the Naive Bayes’ Classifier&lt;/h3&gt;
&lt;p&gt;Estimate the prior and posterior probabilities using Maximum Likelihood Estimates (MLE)&lt;/p&gt;

&lt;p&gt;\begin{equation}
P(y) = \frac{Count(y)}{\sum_{y’\in Y}Count(y’)}
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
P(x_i|y) = \frac{Count(x_i, y)}{Count(y)}
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;some-issues-to-consider&quot;&gt;Some Issues to Consider&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;What if there are words that do not appear in the training set? What if it appears only once?&lt;/li&gt;
  &lt;li&gt;What if the plural of a word never appears in the training set?&lt;/li&gt;
  &lt;li&gt;How are extremely common words (e.g., “the”, “a”) handled?&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jiawei Lu</name><email>jl5999@columbia.edu</email></author><category term="studylog" /><category term="nlp" /><summary type="html">Keywords: Language Classification, Probability Review, Machine Learning Background, Naive Bayes’ Classifier.</summary></entry><entry><title type="html">NLP Lecture 03</title><link href="http://localhost:4000/studylog/NLP_lecture03.html" rel="alternate" type="text/html" title="NLP Lecture 03" /><published>2022-01-26T00:00:00-05:00</published><updated>2022-01-26T00:00:00-05:00</updated><id>http://localhost:4000/studylog/NLP_lecture03</id><content type="html" xml:base="http://localhost:4000/studylog/NLP_lecture03.html">&lt;p&gt;Keywords: n-gram language models&lt;/p&gt;

&lt;h2 id=&quot;language-modeling&quot;&gt;Language Modeling&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Task: predict the next word given the context.&lt;/li&gt;
  &lt;li&gt;Used in speech recognition, handwritten character recognition, spelling correction, text entry UI, machine translation,…&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;probability-of-the-next-word&quot;&gt;Probability of the Next Word&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Idea: We do not need to model domain, syntactic, and lexical knowledge perfectly.&lt;/li&gt;
  &lt;li&gt;Instead, we can rely on the notion of probability of a sequence (letters, words…).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;markov-assumption&quot;&gt;Markov Assumption&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;\(P(w_n|w_1, w_2, ..., w_{n-1})\)
  is difficult to estimate.&lt;/li&gt;
  &lt;li&gt;The longer the sequence becomes, the less likely \(w_1 w_2 w_3 ... w_{n-1}\) will appear in training data.&lt;/li&gt;
  &lt;li&gt;Instead, we make the following simple independence assumption (Markov assumption): The probability to see wn depends only on the previous \(k-1\) words.
  \begin{equation}
  P(w_n|w_1, w_2, w_3, …, w_{n-1}) \approx P(w_n|w_{n-k+1}, …, w_{n-1})
  \end{equation}&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;n-grams&quot;&gt;n-grams&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;The sequence \(w_n\) is a unigram.&lt;/li&gt;
  &lt;li&gt;The sequence \(w_{n-1}, w_n\) is a bigram.&lt;/li&gt;
  &lt;li&gt;The sequence \(w_{n-2}, w_{n-1}, w_n\) is a trigram.&lt;/li&gt;
  &lt;li&gt;The sequence \(w_{n-3}, w_{n-2}, w_{n-1}, w_n\) is a quadrigram…&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;bi-gram-language-model&quot;&gt;Bi-gram Language Model&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Using the Markov assumption and the chain rule:
  \begin{equation}
  P(w_1, w_2, w_3, …, w_n) \approx P(w_1) \cdot P(w_2|w_1) \cdot P(w_3|w_2) \cdots P(w_n|w_{n-1})
  \end{equation}&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;More consistent to use only bigrams:
  \begin{equation}
  P(w_1|start) \cdot P(w_2|w_1) \cdot P(w_3|w_2) \cdots P(w_n|w_{n-1}) \cdot P(end|w_n)
  \end{equation}&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;variable-length-language-models&quot;&gt;Variable-Length Language Models&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;We typically don’t know what the length of the sentence is.&lt;/li&gt;
  &lt;li&gt;Instead, we use a special marker END/STOP that indicates the end of a sentence.&lt;/li&gt;
  &lt;li&gt;We typically just augment the sentence with START and END/STOP markers to provide the appropriate context. 
(START i want to eat Chinese food END)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;log-probabilities&quot;&gt;Log Probabilities&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Probabilities can become very small (a few orders of magnitude per token).&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We often work with log probabilities in practice.
  \begin{equation}
  p(w_1…w_n) = \Pi_{i=1}^np(w_i|w_{i-1})
  \end{equation}&lt;/p&gt;

    &lt;p&gt;\begin{equation}
  \log p(w_1…w_n) = \sum_{i=1}^n \log p(w_i|w_{i-1})
  \end{equation}&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;estimating-n-gram-probabilities&quot;&gt;Estimating n-gram Probabilities&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;We can estimate n-gram probabilities using maximum likelihood estimates.
  \begin{equation}
  p(w|u) = \frac{count(u,w)}{count(u)}
  \end{equation}&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Or for trigrams:
  \begin{equation}
  p(w|u, v) = \frac{count(w,u,w)}{count(u, v)}
  \end{equation}&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;unseen-tokens&quot;&gt;Unseen Tokens&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Typical approach to unseen tokens:
    &lt;ul&gt;
      &lt;li&gt;Start with a specific lexicon of known tokens.&lt;/li&gt;
      &lt;li&gt;Replace all tokens in the training and testing corpus that are not in the lexicon with an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UNK&lt;/code&gt; token.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Practical approach:
    &lt;ul&gt;
      &lt;li&gt;Lexicon contains all words that appear more than &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k&lt;/code&gt; times in the training corpus.&lt;/li&gt;
      &lt;li&gt;Replace all other tokens with UNK.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;unseen-contexts&quot;&gt;Unseen Contexts&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Two basic approaches:
    &lt;ul&gt;
      &lt;li&gt;Smoothing / Discounting: Move some probability mass from seen trigrams to unseen trigrams.&lt;/li&gt;
      &lt;li&gt;Back-off: Use n-1-…, n-2-… grams to compute n-gram probability.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Other techniques:
    &lt;ul&gt;
      &lt;li&gt;Class-based backoff, use back-off probability for a specific word class / part-of-speech.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;zipfs-law&quot;&gt;Zipf’s Law&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Problem: n-grams (and most other linguistic phenomena) follow a Zipfian distribution.&lt;/li&gt;
  &lt;li&gt;A few words occur very frequently.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Most words occur very rarely. Many are seen only once.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Zipf’s law: a word’s frequency is approximately inversely proportional to its rank in the word distribution list.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;smoothing&quot;&gt;Smoothing&lt;/h2&gt;
&lt;p&gt;Smoothing flattens spiky distributions.&lt;/p&gt;

&lt;h3 id=&quot;additive-smoothing&quot;&gt;Additive Smoothing&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Classic approach: Laplacian, a.k.a. additive smoothing.&lt;/p&gt;

    &lt;p&gt;\begin{equation}
  P(w_i) = \frac{count(w_i) + 1}{N+V}
  \end{equation}&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;N is the number of tokens&lt;/li&gt;
      &lt;li&gt;V is the number of types (i.e. size of the vocabulary)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;\begin{equation}
  P(w|u) = \frac{count(u, w) + 1}{count(u) + V}
  \end{equation}&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Inaccurate in practice.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;linear-interpolation&quot;&gt;Linear Interpolation&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Use denser distributions of shorter ngrams to “fill in” sparse ngram distributions.&lt;/p&gt;

    &lt;p&gt;\begin{equation}
  P(w|u, v) = \lambda_1 \cdot p_{mle}(w|u,v) + \lambda_2 \cdot p_{mle}(w|v) + \lambda_3 \cdot p_{mle}(w)
  \end{equation}&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Where, \(\lambda_1, \lambda_2, \lambda_3 &amp;gt; 0\) and \(\lambda_1 + \lambda_2 + \lambda_3 = 1\).&lt;/li&gt;
  &lt;li&gt;Works well in practice (but not a lot of theoretical justification why).&lt;/li&gt;
  &lt;li&gt;Parameters can be estimated on development data (for example, using Expectation Maximization).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;discounting&quot;&gt;Discounting&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Idea: set aside some probability mass, then fill in the missing mass using back-off.&lt;/li&gt;
  &lt;li&gt;\(count^*(v, w) = count(v, w) - \beta\) where \(0&amp;lt;\beta&amp;lt;1\).&lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;Then for all seen bigrams: $$ p(w&lt;/td&gt;
          &lt;td&gt;v) = \frac{count^*(v, w)}{count(v)}.&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;For each context v the missing probability mass is
  \begin{equation}
  \alpha(v) = 1 - \sum_{w:c(v,w)&amp;gt;0} \frac{count^*(v, w)}{count(v)}
  \end{equation}&lt;/li&gt;
  &lt;li&gt;We can now divide this held-out mass between the unseen words (evenly or using back-off).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;katz-backoff&quot;&gt;Katz’ Backoff&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Divide the held-out probability mass proportionally to the unigram probability of the unseen words in context v.&lt;/p&gt;

    &lt;p&gt;\begin{equation}
  p(w|v) = 
  \begin{cases}
  &amp;amp; \frac{count^*(v, w)}{count(v)} &amp;amp; if count(v,w) &amp;gt; 0, &lt;br /&gt;
  &amp;amp; \alpha(v) \times \frac{p_{mle}(w)}{\sum_{u:count(v,u) = 0}p_{mle(u)}} &amp;amp; otherwise.
  \end{cases}
  \end{equation}&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;evaluating-n-gram-models&quot;&gt;Evaluating n-gram models&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Extrinsic evaluation: Apply the model in an application (for example language classification). Evaluate the application.&lt;/li&gt;
  &lt;li&gt;Intrinsic evaluation: measure how well the model approximates unseen language data.
    &lt;ul&gt;
      &lt;li&gt;Can compute the probability of each sentence according to the model. Higher probability -&amp;gt; better model.&lt;/li&gt;
      &lt;li&gt;Typically we compute Perplexity instead.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;perplexity&quot;&gt;Perplexity&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Perplexity (per word) measures how well the ngram model predicts the sample.&lt;/li&gt;
  &lt;li&gt;Given a corpus of ‘m’ sentences ‘\(s_i\)’, where ‘M’ is total number of tokens in the corpus&lt;/li&gt;
  &lt;li&gt;Perplexity is defined as \(2^{-l}\), where \(l = \frac{1}{M} \sum_{i=1}^m \log_2 p(s_i)\).&lt;/li&gt;
  &lt;li&gt;Lower perplexity = better model. Intuition:
    &lt;ul&gt;
      &lt;li&gt;Assume we are predicting one word at a time.&lt;/li&gt;
      &lt;li&gt;With uniform distribution, all successor words are equally likely. Perplexity is equal to vocabulary size.
• Perplexity can be thought of as “effective vocabulary size”.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jiawei Lu</name><email>jl5999@columbia.edu</email></author><category term="studylog" /><category term="nlp" /><summary type="html">Keywords: n-gram language models</summary></entry></feed>