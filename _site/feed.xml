<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="zh, en"><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="https://jiaweilu1999.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://jiaweilu1999.github.io/" rel="alternate" type="text/html" hreflang="zh, en" /><updated>2023-12-27T23:53:43-08:00</updated><id>https://jiaweilu1999.github.io/feed.xml</id><title type="html">Jiawei Lu</title><subtitle>Hello World!  I&apos;m Jiawei Lu, a Master Student at Columbia University.  My research interest is Computer Vision, Deep Learning and Reinforcement Learning.
</subtitle><author><name>Jiawei Lu</name><email>jl5999@columbia.edu</email></author><entry><title type="html">Dec 2023</title><link href="https://jiaweilu1999.github.io/studylog/diary.html" rel="alternate" type="text/html" title="Dec 2023" /><published>2023-12-01T00:00:00-08:00</published><updated>2023-12-01T00:00:00-08:00</updated><id>https://jiaweilu1999.github.io/studylog/diary</id><content type="html" xml:base="https://jiaweilu1999.github.io/studylog/diary.html">&lt;h2 id=&quot;dec-13-2023&quot;&gt;Dec 13, 2023&lt;/h2&gt;
&lt;p&gt;Done:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Review &lt;a href=&quot;https://papers.nips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf&quot;&gt;AlexNet&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Review &lt;a href=&quot;https://arxiv.org/pdf/1512.03385.pdf&quot;&gt;ResNet&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Todo:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Improve personal website&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dec-14-2023&quot;&gt;Dec 14, 2023&lt;/h2&gt;
&lt;p&gt;Done:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Learn Jekyll
    &lt;ul&gt;
      &lt;li&gt;Liquid&lt;/li&gt;
      &lt;li&gt;Front Matter&lt;/li&gt;
      &lt;li&gt;Layouts&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dec-15-2023&quot;&gt;Dec 15, 2023&lt;/h2&gt;
&lt;p&gt;Done:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Continue to learn Javascript&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dec-16-2023&quot;&gt;Dec 16, 2023&lt;/h2&gt;
&lt;p&gt;Done:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Continue to learn Javascript&lt;/li&gt;
  &lt;li&gt;Leetcode week contest 12/18&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dec-17-2023&quot;&gt;Dec 17, 2023&lt;/h2&gt;
&lt;p&gt;Done:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Start learning C in a structured way (let’s go!)&lt;/li&gt;
  &lt;li&gt;Review Git commands
    &lt;ul&gt;
      &lt;li&gt;remote, merge, fork, pull request related&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Dive into the architecture of Walmart Seller Vetting System&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dec-18-2023&quot;&gt;Dec 18, 2023&lt;/h2&gt;
&lt;p&gt;Done:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Workday:
    &lt;ul&gt;
      &lt;li&gt;setup new laptop&lt;/li&gt;
      &lt;li&gt;prepare for the daily standup of ETF&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Todo:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;learn JIRA&lt;/li&gt;
  &lt;li&gt;learn React&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dec-19-2023&quot;&gt;Dec 19, 2023&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Done:
    &lt;ul&gt;
      &lt;li&gt;Workday:
        &lt;ul&gt;
          &lt;li&gt;set up two microservices and get success response&lt;/li&gt;
          &lt;li&gt;debug:
            &lt;ul&gt;
              &lt;li&gt;maven build need specify the JAVA_HOME env var&lt;/li&gt;
              &lt;li&gt;lombok needs annotation processing&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Todo:
    &lt;ul&gt;
      &lt;li&gt;learn JIRA&lt;/li&gt;
      &lt;li&gt;learn React&lt;/li&gt;
      &lt;li&gt;learn WCNP&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dec-20-2023&quot;&gt;Dec 20, 2023&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Done:
    &lt;ul&gt;
      &lt;li&gt;Workday:
        &lt;ul&gt;
          &lt;li&gt;Discuss cloud migration&lt;/li&gt;
          &lt;li&gt;Create new branch, merge main&lt;/li&gt;
          &lt;li&gt;commit and push to trigger CI/CD&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dec-21-2023&quot;&gt;Dec 21, 2023&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Done:
    &lt;ul&gt;
      &lt;li&gt;Workday:
        &lt;ul&gt;
          &lt;li&gt;Get access permission&lt;/li&gt;
          &lt;li&gt;Check yaml file&lt;/li&gt;
          &lt;li&gt;Build and deploy to stage mode successfully&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Todo:
    &lt;ul&gt;
      &lt;li&gt;Dive into EDS architecture&lt;/li&gt;
      &lt;li&gt;UT coverage&lt;/li&gt;
      &lt;li&gt;Review React&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dec-22-2023&quot;&gt;Dec 22, 2023&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Done:
    &lt;ul&gt;
      &lt;li&gt;Workday:
        &lt;ul&gt;
          &lt;li&gt;Examine service code for UT&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dec-23---25-2023&quot;&gt;Dec 23 - 25, 2023&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Christmas Holiday&lt;/li&gt;
  &lt;li&gt;Move from Sunnyvale to Santa Clara&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dec-26-2023&quot;&gt;Dec 26, 2023&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Done:
    &lt;ul&gt;
      &lt;li&gt;Workday:
        &lt;ul&gt;
          &lt;li&gt;Sonar check failed dut to copy &amp;amp; paste code rather than git revert, solved by creating a new branch by merging&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Learn Mockito for SpringBoot Unit Test&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Todo:
    &lt;ul&gt;
      &lt;li&gt;Review React&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dec-27-2023&quot;&gt;Dec 27, 2023&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Done:
    &lt;ul&gt;
      &lt;li&gt;Workday:
        &lt;ul&gt;
          &lt;li&gt;Start Unit Test with Mockito&lt;/li&gt;
          &lt;li&gt;Local setup React UI FE and BE&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jiawei Lu</name><email>jl5999@columbia.edu</email></author><category term="studylog" /><category term="diary" /><summary type="html">Dec 13, 2023 Done: Review AlexNet Review ResNet</summary></entry><entry><title type="html">[ZK Lab] #7: Tool: Demo of YOLO V4 + W&amp;amp;B</title><link href="https://jiaweilu1999.github.io/studylog/tool_yolov4_wnb_demo.html" rel="alternate" type="text/html" title="[ZK Lab] #7: Tool: Demo of YOLO V4 + W&amp;amp;B" /><published>2022-06-01T00:00:00-07:00</published><updated>2022-06-01T00:00:00-07:00</updated><id>https://jiaweilu1999.github.io/studylog/tool_yolov4_wnb_demo</id><content type="html" xml:base="https://jiaweilu1999.github.io/studylog/tool_yolov4_wnb_demo.html">&lt;ul&gt;
  &lt;li&gt;Follow the Instrctions in &lt;a href=&quot;https://docs.google.com/document/d/1EFezfWFGqiPyRJZg4xyv0d0knfPTdvLabWUzX6JuR34/edit#heading=h.9uriig7xeant&quot;&gt;this file&lt;/a&gt; to set up YOLO V4 environment and workflow on GCP.
    &lt;ul&gt;
      &lt;li&gt;Personal Vertex AI instance &lt;a href=&quot;https://console.cloud.google.com/vertex-ai/workbench/details/locations/us-central1/runtimes/yolov4-pytorch-jl5999?project=ecbm4040-ta&quot;&gt;[Link]&lt;/a&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Clone the &lt;a href=&quot;https://github.com/zk2172-columbia/ProjectTrafficIntersection.COSMOS.V2.git&quot;&gt;GitHub repository&lt;/a&gt; related to this project.&lt;/li&gt;
  &lt;li&gt;Run demo.ipynb.
    &lt;ul&gt;
      &lt;li&gt;Screenshots:&lt;/li&gt;
      &lt;li&gt;&lt;img alt=&quot;Screenshots 1&quot; src=&quot;../../../assets/img/blog/2022-06-01/1.png&quot; width=&quot;400&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;&lt;img alt=&quot;Screenshots 2&quot; src=&quot;../../../assets/img/blog/2022-06-01/2.png&quot; width=&quot;400&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Update W&amp;amp;B Workspace Log:
    &lt;ul&gt;
      &lt;li&gt;Screenshots:&lt;/li&gt;
      &lt;li&gt;&lt;img alt=&quot;Screenshots 3&quot; src=&quot;../../../assets/img/blog/2022-06-01/3.png&quot; width=&quot;400&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;&lt;img alt=&quot;Screenshots 4&quot; src=&quot;../../../assets/img/blog/2022-06-01/4.png&quot; width=&quot;500&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jiawei Lu</name><email>jl5999@columbia.edu</email></author><category term="studylog" /><category term="zklab" /><summary type="html">Follow the Instrctions in this file to set up YOLO V4 environment and workflow on GCP. Personal Vertex AI instance [Link]. Clone the GitHub repository related to this project. Run demo.ipynb. Screenshots:</summary></entry><entry><title type="html">Chapter 1: Computer Networks and the Internet</title><link href="https://jiaweilu1999.github.io/studylog/cn_1.html" rel="alternate" type="text/html" title="Chapter 1: Computer Networks and the Internet" /><published>2022-05-28T00:00:00-07:00</published><updated>2022-05-28T00:00:00-07:00</updated><id>https://jiaweilu1999.github.io/studylog/cn_1</id><content type="html" xml:base="https://jiaweilu1999.github.io/studylog/cn_1.html">&lt;p&gt;Done:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;1.1 What Is the Internet?
    &lt;ul&gt;
      &lt;li&gt;1.1.1 A Nuts-and-Bolts Description&lt;/li&gt;
      &lt;li&gt;1.1.2 A Services Description&lt;/li&gt;
      &lt;li&gt;1.1.3 What Is a Protocol?
        &lt;ul&gt;
          &lt;li&gt;A Human Analogy&lt;/li&gt;
          &lt;li&gt;Network Protocols&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;1.2 The Network Edge
    &lt;ul&gt;
      &lt;li&gt;1.2.1 Access Networks
        &lt;ul&gt;
          &lt;li&gt;Home Access: DSL, Cable, FTTH, Dial-Up, and Satellite&lt;/li&gt;
          &lt;li&gt;Access in the Enterprise (and the Home): Ethernet and WiFi&lt;/li&gt;
          &lt;li&gt;Wide-Area Wireless Access: 3G and LTE&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;1.2.2 Physical Media
        &lt;ul&gt;
          &lt;li&gt;Twisted-Pair Copper Wire&lt;/li&gt;
          &lt;li&gt;Coaxial Cable&lt;/li&gt;
          &lt;li&gt;Fiber Optics&lt;/li&gt;
          &lt;li&gt;Terrestrial Radio Channels&lt;/li&gt;
          &lt;li&gt;Satellite Radio Channels&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;1.3 The Network Core
    &lt;ul&gt;
      &lt;li&gt;1.3.1 Packet Switching&lt;/li&gt;
      &lt;li&gt;1.3.2 Circuit Switching&lt;/li&gt;
      &lt;li&gt;1.3.3 A Network of Networks&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;TODO:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;1.4 Delay, Loss, and Throughput in Packet-Switched Networks
    &lt;ul&gt;
      &lt;li&gt;1.4.1 Overview of Delay in Packet-Switched Networks&lt;/li&gt;
      &lt;li&gt;1.4.2 Queuing Delay and Packet Loss&lt;/li&gt;
      &lt;li&gt;1.4.3 End-to-End Delay&lt;/li&gt;
      &lt;li&gt;1.4.4 Throughput in Computer Networks&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;1.5 Protocol Layers and Their Service Models
    &lt;ul&gt;
      &lt;li&gt;1.5.1 Layered Architecture&lt;/li&gt;
      &lt;li&gt;1.5.2 Encapsulation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;1.6 Networks Under Attack&lt;/li&gt;
  &lt;li&gt;1.7 History of Computer Networking and the Internet
    &lt;ul&gt;
      &lt;li&gt;1.7.1 The Development of Packet Switching: 1961–1972&lt;/li&gt;
      &lt;li&gt;1.7.2 Proprietary Networks and Internetworking: 1972–1980&lt;/li&gt;
      &lt;li&gt;1.7.3 A Proliferation of Networks: 1980–1990&lt;/li&gt;
      &lt;li&gt;1.7.4 The Internet Explosion: The 1990s&lt;/li&gt;
      &lt;li&gt;1.7.5 The New Millennium&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;1.8 Summary&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jiawei Lu</name><email>jl5999@columbia.edu</email></author><category term="studylog" /><category term="cn" /><summary type="html">Done: 1.1 What Is the Internet? 1.1.1 A Nuts-and-Bolts Description 1.1.2 A Services Description 1.1.3 What Is a Protocol? A Human Analogy Network Protocols 1.2 The Network Edge 1.2.1 Access Networks Home Access: DSL, Cable, FTTH, Dial-Up, and Satellite Access in the Enterprise (and the Home): Ethernet and WiFi Wide-Area Wireless Access: 3G and LTE 1.2.2 Physical Media Twisted-Pair Copper Wire Coaxial Cable Fiber Optics Terrestrial Radio Channels Satellite Radio Channels 1.3 The Network Core 1.3.1 Packet Switching 1.3.2 Circuit Switching 1.3.3 A Network of Networks</summary></entry><entry><title type="html">[ZK Lab] #6: Concept - Re-identification</title><link href="https://jiaweilu1999.github.io/studylog/concept_reid.html" rel="alternate" type="text/html" title="[ZK Lab] #6: Concept - Re-identification" /><published>2022-05-25T00:00:00-07:00</published><updated>2022-05-25T00:00:00-07:00</updated><id>https://jiaweilu1999.github.io/studylog/concept_reid</id><content type="html" xml:base="https://jiaweilu1999.github.io/studylog/concept_reid.html">&lt;h2 id=&quot;definition&quot;&gt;Definition&lt;/h2&gt;
&lt;p&gt;Person re-identification (Re-ID) aims at retrieving a person of interest across multiple non-overlapping cameras.&lt;/p&gt;

&lt;h2 id=&quot;previous-work&quot;&gt;Previous Work&lt;/h2&gt;
&lt;h3 id=&quot;re-identification-based-on-direct-linear-transform&quot;&gt;&lt;a href=&quot;https://docs.google.com/document/d/1D8OzTOfXJi62jOoa7rm4GZXMlpZbsgyxPaNRN2eSjFA&quot;&gt;Re-identification Based on Direct Linear Transform&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;@author: Chenbo Zang&lt;/p&gt;

&lt;p&gt;Intro: The project proposed and experimented reidentification between first and second floor cameras by performing 3D coordinate reconstruction using direct linear transform (DLT). Camera calibration, Yolov4 detection and deepsort tracking models are adopted from previous work. The feasibility of the proposed method was validated in small scale experiments, and generalizability was tested on self-constructed dataset with unsatisfactory outcome due to poor calibration, naive distance metrics and susceptible assignment algorithm. New methodologies are required for future work.&lt;/p&gt;

&lt;h3 id=&quot;unsupervised-re-identification-in-intersections&quot;&gt;&lt;a href=&quot;https://docs.google.com/document/d/16sdFSG-x9RjIrUeexb57Dh0AVXAMqG1_z-sIhj8dvio&quot;&gt;Unsupervised re-identification in intersections&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;@author: Mingzhe Hu&lt;/p&gt;

&lt;p&gt;Intro: We built an unsupervised re-identification model for first and second camera intersection re-identification of pedestrians and vehicles. We used DukeMTMC and Veri-776 as training sets and constructed a customized test set from synchronized video recordings. We leveraged both CNN and transformer as Reid’s backbone and inference tricks to enhance performance. We achieved mAP of 73.3% in vehicle Reid and 89.2% in person Reid, with an inference speed of 25.26 images/sec for vehicle Reid and 31.18 images/sec for person Reid. Code can be accessed on GitHub.&lt;/p&gt;

&lt;h2 id=&quot;ucf-related-work&quot;&gt;UCF Related Work&lt;/h2&gt;

&lt;p&gt;UCF (University of Central Florida) Centerfor Research in Computer Vision has a project named &lt;a href=&quot;https://www.crcv.ucf.edu/projects/multi-camera.php&quot;&gt;Multi-Camera Video Analysis&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;aifi-paper&quot;&gt;AiFi Paper&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Cross-View_Tracking_for_Multi-Human_3D_Pose_Estimation_at_Over_100_CVPR_2020_paper.pdf&quot;&gt;Cross-View Tracking for Multi-Human 3D Pose Estimation at over 100 FPS&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;abstract&quot;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Estimating 3D poses of multiple humans in real-time is a classic but still challenging task in computer vision. Its major difficulty lies in the ambiguity in cross-view association of 2D poses and the huge state space when there are multiple people in multiple views. In this paper, we present a novel solution for multi-human 3D pose estimation from multiple calibrated camera views. It takes 2D poses in different camera coordinates as inputs and aims for the accurate 3D poses in the global coordinate. Unlike previous methods that associate 2D poses among all pairs of views from scratch at every frame, we exploit the temporal consistency in videos to match the 2D inputs with 3D poses directly in 3-space. More specifically, we propose to retain the 3D pose for each person and update them iteratively via the cross-view multi-human tracking. This novel formulation improves both accuracy and efficiency, as we demonstrated on widely-used public datasets. To further verify the scalability of our method, we propose a new large-scale multihuman dataset with 12 to 28 camera views. Without bells and whistles, our solution achieves 154 FPS on 12 cameras and 34 FPS on 28 cameras, indicating its ability to handle large-scale real-world applications. The proposed dataset will be released at &lt;a href=&quot;https://github.com/longcw/crossview_3d_pose_tracking&quot;&gt;https://github.com/longcw/crossview_3d_pose_tracking&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;cps-iot-week-competition&quot;&gt;CPS-IoT Week Competition&lt;/h2&gt;
&lt;p&gt;Github &lt;a href=&quot;https://github.com/JoaoDiogoFalcao/AutoCheckout&quot;&gt;[Link]&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;This repository will help you get started with examples on how to use the Public Datasets available at http://aifi.io/research under Sample Data.&lt;/li&gt;
  &lt;li&gt;To start please download the data labelled as: “Simple Example” without depth (For your first example).&lt;/li&gt;
  &lt;li&gt;During the competition you will need a competitor token that will distinguish your submission from all other competitors. However you do
not need this token for your local testing environment.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;attention--vision-transformer&quot;&gt;Attention &amp;amp; Vision Transformer&lt;/h2&gt;

&lt;iframe width=&quot;735&quot; height=&quot;450&quot; src=&quot;https://www.youtube.com/embed/YAgjfMR9R_M&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;</content><author><name>Jiawei Lu</name><email>jl5999@columbia.edu</email></author><category term="studylog" /><category term="zklab" /><summary type="html">Definition Person re-identification (Re-ID) aims at retrieving a person of interest across multiple non-overlapping cameras.</summary></entry><entry><title type="html">[ZK Lab] #5: Tool - Prior Projects</title><link href="https://jiaweilu1999.github.io/studylog/tool_prior_work.html" rel="alternate" type="text/html" title="[ZK Lab] #5: Tool - Prior Projects" /><published>2022-05-22T00:00:00-07:00</published><updated>2022-05-22T00:00:00-07:00</updated><id>https://jiaweilu1999.github.io/studylog/tool_prior_work</id><content type="html" xml:base="https://jiaweilu1999.github.io/studylog/tool_prior_work.html">&lt;h2 id=&quot;outline&quot;&gt;Outline&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#2022-spring&quot;&gt;2022 Spring&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#re-identification-based-on-direct-linear-transform&quot;&gt;Re-identification Based on Direct Linear Transform&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#unsupervised-re-identification-in-intersections&quot;&gt;Unsupervised re-identification in intersections&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2021-fall&quot;&gt;2021 Fall&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#face-and-license-plate-blurring-in-street-level-intersection-videos&quot;&gt;Face and License Plate Blurring in Street Level Intersection Videos&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#data-augmentation-with-causal-inference-to-improve-small-object-detection&quot;&gt;Data Augmentation with Causal Inference to Improve Small Object Detection&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#2-stage-2d-and-3d-approach-for-small-pedestrian-detection&quot;&gt;2-Stage (2D and 3D) Approach for Small Pedestrian Detection&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2021-summer&quot;&gt;2021 Summer&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#using-data-augmentation-to-improve-pedestrian-detection&quot;&gt;Using Data Augmentation to Improve Pedestrian Detection&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#efficient-det-comparative-study&quot;&gt;Efficient Det: Comparative Study&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#software-engineering-support-for-rtsp-streaming-deepstream-pipeline&quot;&gt;Software engineering support for RTSP streaming Deepstream pipeline&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#cosmos-vehicle-counting-stage-report&quot;&gt;COSMOS Vehicle Counting Stage Report&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2021-spring&quot;&gt;2021 Spring&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#using-data-augmentation-to-improve-pedestrian-detection-1&quot;&gt;Using Data Augmentation to Improve Pedestrian Detection&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#face-and-license-plate-blurring-for-cosmos-traffic-intersection&quot;&gt;Face and License Plate Blurring for COSMOS Traffic Intersection&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2020-fall&quot;&gt;2020 Fall&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#multi-resolution-and-density-study-for-object-detection-using-yolov4&quot;&gt;Multi-Resolution and Density Study for Object Detection using YOLOv4&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#injection-of-efficientdet-model-into-object-detection-pipeline-and-setting-vpn-to-connect-any-device-to-the-cosmos-server&quot;&gt;Injection of EfficientDet model into Object Detection Pipeline and setting VPN to connect any device to the COSMOS server&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#research-about-ssd-832-adjustment-non-maximum-suppression-and-tensorrt-conversion&quot;&gt;Research about SSD-832 Adjustment, Non-Maximum Suppression and TensorRT Conversion&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#research-about-bounding-box-transformation-832-for-multi-camera-fusion--resolution-study-and-density-study&quot;&gt;Research about bounding box transformation 832 for Multi-camera-fusion , Resolution Study and Density Study&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2020-summer&quot;&gt;2020 Summer&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#profiling-and-deploying-mask-r-cnn-with-tensorrt-and-deepstream&quot;&gt;Profiling and Deploying Mask R-CNN with TensorRT and Deepstream&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#research-about-bounding-box-transformation-for-multiple-cameras-fusion-robust-pca-implementation-and-cropping-images-for-yolov4&quot;&gt;Research about bounding box transformation for multiple cameras fusion, Robust PCA implementation and cropping images for YOLOv4&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#object-detection-with-temporal-information&quot;&gt;Object Detection with Temporal Information&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#2020-summer-report-tracking&quot;&gt;2020 Summer Report: Tracking&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#data-management-using-dvc-data-version-control&quot;&gt;Data Management using DVC (Data Version Control)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#2020-summer-report-tracking-with-fairmot&quot;&gt;2020 Summer Report: Tracking with FairMOT&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#deepstream-deployment-and-custom-ssd-object-detection-model&quot;&gt;DeepStream Deployment and Custom SSD Object Detection Model&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#research-report-on-data-management-using-dvc-data-version-control&quot;&gt;Research Report on Data Management Using DVC (Data Version Control)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#single-shot-multibox-detector-ssd-implementation-for-traffic-intersection&quot;&gt;Single Shot Multibox Detector (SSD) Implementation for Traffic Intersection&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#summer-2020-report-measurements-for-object-detection-and-tracking-for-cosmos-smart-intersections&quot;&gt;Summer 2020 Report: Measurements for Object Detection and Tracking for COSMOS Smart Intersections&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#cosmos-traffic-intersection-research-final-report&quot;&gt;COSMOS Traffic Intersection Research Final report&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2022-spring&quot;&gt;2022 Spring&lt;/h2&gt;

&lt;h3 id=&quot;re-identification-based-on-direct-linear-transform&quot;&gt;&lt;a href=&quot;https://docs.google.com/document/d/1D8OzTOfXJi62jOoa7rm4GZXMlpZbsgyxPaNRN2eSjFA&quot;&gt;Re-identification Based on Direct Linear Transform&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;@author: Chenbo Zang&lt;/p&gt;

&lt;h4 id=&quot;abstract&quot;&gt;Abstract&lt;/h4&gt;
&lt;p&gt;The project proposed and experimented reidentification between first and second floor cameras by performing 3D coordinate reconstruction using direct linear transform (DLT). Camera calibration, Yolov4 detection and deepsort tracking models are adopted from previous work. The feasibility of the proposed method was validated in small scale experiments, and generalizability was tested on self-constructed dataset with unsatisfactory outcome due to poor calibration, naive distance metrics and susceptible assignment algorithm. New methodologies are required for future work.&lt;/p&gt;

&lt;h4 id=&quot;keywords&quot;&gt;Keywords&lt;/h4&gt;
&lt;p&gt;re-identification, DLT, Yolov4, deepsort&lt;/p&gt;

&lt;h3 id=&quot;unsupervised-re-identification-in-intersections&quot;&gt;&lt;a href=&quot;https://docs.google.com/document/d/16sdFSG-x9RjIrUeexb57Dh0AVXAMqG1_z-sIhj8dvio&quot;&gt;Unsupervised re-identification in intersections&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;@author: Mingzhe Hu&lt;/p&gt;

&lt;h4 id=&quot;abstract-1&quot;&gt;Abstract&lt;/h4&gt;
&lt;p&gt;We built an unsupervised re-identification model for first and second camera intersection re-identification of pedestrians and vehicles. We used DukeMTMC and Veri-776 as training sets and constructed a customized test set from synchronized video recordings. We leveraged both CNN and transformer as Reid’s backbone and inference tricks to enhance performance. We achieved mAP of 73.3% in vehicle Reid and 89.2% in person Reid, with an inference speed of 25.26 images/sec for vehicle Reid and 31.18 images/sec for person Reid. Code can be accessed on GitHub.&lt;/p&gt;

&lt;h4 id=&quot;keywords-1&quot;&gt;Keywords&lt;/h4&gt;
&lt;p&gt;unsupervised learning, re-identification&lt;/p&gt;

&lt;h2 id=&quot;2021-fall&quot;&gt;2021 Fall&lt;/h2&gt;

&lt;h3 id=&quot;face-and-license-plate-blurring-in-street-level-intersection-videos&quot;&gt;&lt;a href=&quot;https://drive.google.com/file/d/1rxqgTZA0x-dmiXHPGvFCgDXIK6OPTqMS&quot;&gt;Face and License Plate Blurring in Street Level Intersection Videos&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;@author: Alex Angus, Zhuoxu Duan, Adit Deshmukh&lt;/p&gt;

&lt;h4 id=&quot;abstract-2&quot;&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Project COSMOS relies heavily on large image datasets that are gathered from street-level cameras. In the process of collecting real-time images and videos of public spaces, cameras also inadvertently capture sensitive information such as faces and license plates. To avoid the compromise of large amounts of private information in the followup research involving our image datasets, we generate a pipeline to systematically blur the faces of pedestrians and the license plates of vehicles in street level intersection videos. We train various YOLOv4 object detection models, using the Darknet framework, for the detection of pedestrians and vehicles as well as for the detection of faces and license plates. We train the models with our own custom intersection video dataset annotated Summer 2021. Additionally, we implement two models, MobileNetV3 and WPOD-net, for the detection of faces and license plates, respectively. The face detection model performs well with a mean IoU above 70% on our COSMOS dataset while the license plate model still has some problems since the confidence loss in training tends to be stuck at 36.048. Ultimately, we are able to automatically blur 99% of visible faces and license plates in any given 1st floor intersection video.&lt;/p&gt;

&lt;h4 id=&quot;keywords-2&quot;&gt;Keywords&lt;/h4&gt;
&lt;p&gt;face blurring, license plate blurring, object detection&lt;/p&gt;

&lt;h3 id=&quot;data-augmentation-with-causal-inference-to-improve-small-object-detection&quot;&gt;&lt;a href=&quot;https://docs.google.com/document/d/1WtgWKL8gWHjtdCmXvNerKf9fNt9bwhXlrnjiu8yLqZQ&quot;&gt;Data Augmentation with Causal Inference to Improve Small Object Detection&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;@author: Lingxiao Li&lt;/p&gt;

&lt;h4 id=&quot;abstract-3&quot;&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Causal inference is a critical research topic across many domains for decades. Nowadays, estimating causal effects from observational data has become an appealing research direction owing to the large amount of available data and low budget requirement, compared with randomized controlled trials. At the same time, small object detection remains a challenging task in deep learning. In this project, we aim to study the causal effect from observational data to the performance of the YOLOv4 model on small object(pedestrian) detection and use data augmentation to improve the accuracy of pedestrian detection using causal inference methods. Our evaluation results show that the average precision for pedestrians of the model trained by the augmented data and unaugmented data improves 67.89%  compared to the average precision of the model only trained by the unaugmented data, which indicates that our approach successfully improved the accuracy of small object detection of the YOLOv4 model by decorrelate co-founders in training dataset.&lt;/p&gt;

&lt;h4 id=&quot;keywords-3&quot;&gt;Keywords&lt;/h4&gt;
&lt;p&gt;causal inference, data augmentation, small object detection&lt;/p&gt;

&lt;h3 id=&quot;2-stage-2d-and-3d-approach-for-small-pedestrian-detection&quot;&gt;&lt;a href=&quot;https://docs.google.com/document/d/1Tp3CQ2Dl8hqGf-9WBlW5wnt5Cg55aAKxnZlVxWqSnl8&quot;&gt;2-Stage (2D and 3D) Approach for Small Pedestrian Detection&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;@author: Sung Jun Won&lt;/p&gt;

&lt;h4 id=&quot;abstract-4&quot;&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Small object detection remains a challenging task in deep learning. In this project, we primarily attempted to replicate the work done in Dogfight: Detecting Drones from Drones Videos. We experimented with both the spatial model and the spatio-temporal model using the 12th floor videos dataset. For the spatial model, we divided each frame into 9 overlapping regions to account for the small size of pedestrians compared to the whole image. PSPNet is then trained on the cropped images and the ground truth images. For the spatio-temporal model, in order to discover pedestrian locations, we generated motion boundaries images and tracked candidate pedestrian locations for a few frames using cuboids. Then, the Inflated 3D model is used to extract the 3D convolution feature maps for the model to train on. We ended up not producing meaningful results for the experiments due to many technical challenges regarding the model overfitting and interpreting the model architecture.&lt;/p&gt;

&lt;h4 id=&quot;keywords-4&quot;&gt;Keywords&lt;/h4&gt;
&lt;p&gt;PSPNet, Overlapping regions, Conditional Random Fields, Convolution features, Inflated 3D&lt;/p&gt;

&lt;h2 id=&quot;2021-summer&quot;&gt;2021 Summer&lt;/h2&gt;

&lt;h3 id=&quot;using-data-augmentation-to-improve-pedestrian-detection&quot;&gt;&lt;a href=&quot;https://docs.google.com/document/d/1MN5BJrRNADjJCHNc5TMKTbK1df6bJx6-y60whkBzmAw&quot;&gt;Using Data Augmentation to Improve Pedestrian Detection&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;@author: Sung Jun Won, Shambhavi Roy&lt;/p&gt;

&lt;h4 id=&quot;abstract-5&quot;&gt;Abstract&lt;/h4&gt;
&lt;p&gt;You Only Look Once (YOLO) is deemed a state-of-the-art model for object detection today. In this project, images taken from the 12th floor of the Mudd building are used, and detecting pedestrians is the primary challenge. A scarce number of pedestrians in each frame causes class imbalance as the vehicles outnumber pedestrians in many frames. Also, pedestrians are very small in size compared to the size of each frame. To resolve these problems and perform pedestrian detection successfully, we have experimented with data augmentation and finally achieved realistic data augmentation by rescaling. The main goal of our project is to evaluate this augmentation technique in 832 x 832 images using the YOLOv4 model for any improvement in performance. The current results indicate that there’s an error in training, demonstrated by the unrealistically high AP results - 100% for vehicles and nearly 100% for pedestrians when trained with an unaugmented dataset .&lt;/p&gt;

&lt;h4 id=&quot;keywords-5&quot;&gt;Keywords&lt;/h4&gt;
&lt;p&gt;Data augmentation, Small object detection, Pedestrian detection, Darknet, YOLOv4&lt;/p&gt;

&lt;h3 id=&quot;efficient-det-comparative-study&quot;&gt;&lt;a href=&quot;https://docs.google.com/document/d/13x432QAHevbpWDZiR8qLdnDVngrmJ_-b&quot;&gt;Efficient Det: Comparative Study&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;@author: Adit Deshmukh, Tushar Gupta&lt;/p&gt;

&lt;h4 id=&quot;abstract-6&quot;&gt;Abstract&lt;/h4&gt;
&lt;p&gt;EfficientDet is a novel deep-learning framework that aims at developing a formal approach towards scaling object detection networks in terms of parameters and speed. Our work explores the network’s performance on the 12th floor and Visdrone 2019 dataset. We aim to develop an accurate and efficient object detection pipeline using the work done on EfficientNet as the backbone. We obtained the highest MAP of 0.7 with the 12th-floor dataset.&lt;/p&gt;

&lt;h4 id=&quot;keywords-6&quot;&gt;Keywords&lt;/h4&gt;
&lt;p&gt;scaling coefficient, object-detection, Bi-directional Feature pyramid network, Efficient Net&lt;/p&gt;

&lt;h3 id=&quot;software-engineering-support-for-rtsp-streaming-deepstream-pipeline&quot;&gt;&lt;a href=&quot;https://docs.google.com/document/d/1W7MQMVM0I9u47I-ctbmNgT-Fr-fucKas&quot;&gt;Software engineering support for RTSP streaming Deepstream pipeline&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;@author: Tushar Gupta&lt;/p&gt;

&lt;h4 id=&quot;abstract-7&quot;&gt;Abstract&lt;/h4&gt;
&lt;p&gt;The project involved working on various components of an RTSP streaming pipeline that enables distributed interference in support of video-based object detection and tracking. Tasks included software engineering in support of networked streaming to enable frame synchronization, latency measurements, and video segmentation.&lt;/p&gt;

&lt;h4 id=&quot;keywords-7&quot;&gt;Keywords&lt;/h4&gt;
&lt;p&gt;Deepstream, GStreamer, Video streaming, FFmpeg&lt;/p&gt;

&lt;h3 id=&quot;cosmos-vehicle-counting-stage-report&quot;&gt;&lt;a href=&quot;https://docs.google.com/document/d/1OWbmhPi6h6ceY16eAK_PBIcGwjFxQlR0o-mqueWdQpU&quot;&gt;COSMOS Vehicle Counting Stage Report&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;@author: Lingxiao Li, Xingxing Geng&lt;/p&gt;

&lt;h4 id=&quot;abstract-8&quot;&gt;Abstract&lt;/h4&gt;
&lt;p&gt;One of the important goals for the COSMOS (Cloud Enhanced Open Software-Defined Mobile Wireless Testbed) project is analyzing traffic flow in real time. To achieve this goal, we need to count and classify vehicles and pedestrians crossing the traffic intersection. This report summarizes the work we have done for the 2021 summer. We applied YOLOv4 which was implemented in PyTorch with pre-trained Darknet YOLOv4 models to detect objects in 832x832 cropped videos. We utilize the DeepSORT algorithm to track objects. We designed a simple but efficient algorithm to classify vehicles by their directions and turning movements. We evaluated the performance of our model on 12th floor videos using MOT metrics. We designed a piece of software which can be used on 12th floor cameras. The average accuracy for 12 test videos is 95% which satisfies the requirement for this project.  The limiting factors are the small object detection.&lt;/p&gt;

&lt;h4 id=&quot;keywords-8&quot;&gt;Keywords&lt;/h4&gt;
&lt;p&gt;Object Detection, Object Tracking, Object Counting, Yolov4&lt;/p&gt;

&lt;h2 id=&quot;2021-spring&quot;&gt;2021 Spring&lt;/h2&gt;

&lt;h3 id=&quot;using-data-augmentation-to-improve-pedestrian-detection-1&quot;&gt;&lt;a href=&quot;https://docs.google.com/document/d/1mNmre_IdVJ7WUGaeYMASNb8_d3qMfCazQR09iGyvB4c&quot;&gt;Using Data Augmentation to Improve Pedestrian Detection&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;@author: Kevin Murning&lt;/p&gt;

&lt;h4 id=&quot;abstract-9&quot;&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Robust detection of small objects in large videos and images remains a significant challenge in the field of object detection. In the case of the Cosmos traffic intersection research, this challenge is present in the task of robust detection of pedestrians. The reasons for this are two-fold. First, the pedestrians present in the 1080p images only occupy a very small pixel range. Second, there is a significant class imbalance present in the data. The number of pedestrians present is far less than the number of vehicles. The objective of this study is to evaluate the efficacy of  a data augmentation technique in improving the detection of pedestrians. Software was written to artificially augment the number of pedestrians present in the data. This dataset was then used to train YOLOv4. It was found that using a combination of augmented and unaugmentated data in training YoloV4 improved the average precision of detecting pedestrians by 8.61% in comparison to a model trained only on the unaugmented data.&lt;/p&gt;

&lt;h4 id=&quot;keywords-9&quot;&gt;Keywords&lt;/h4&gt;
&lt;p&gt;Small Object Detection, Data Augmentation, YOLOv4&lt;/p&gt;

&lt;h3 id=&quot;face-and-license-plate-blurring-for-cosmos-traffic-intersection&quot;&gt;&lt;a href=&quot;https://docs.google.com/document/d/1UvYZEFo7y8Dh5z2uuK8r7NtkgyXUUNJ-sTXi5a8fP5w&quot;&gt;Face and License Plate Blurring for COSMOS Traffic Intersection&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;@author:  Zhuoxu Duan, Jingyuan Liu, Wenjun Chen, Joseph Yang, Zoran Kostic&lt;/p&gt;

&lt;h4 id=&quot;abstract-10&quot;&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Project COSMOS relies heavily on the large image datasets that are gathered from street-level cameras. In the process of collecting the real-time images/videos of public spaces, the cameras also inevitably captured information like licence plates, faces, and many other information that would be considered sensitive from a privacy perspective. To avoid compromise of massive personal information in the follow-up research that works with our image dataset, we need to blur the images on pedestrian faces and license plates. In this project, we present a system that combines object detectors like Yolov4 and Mask RCNN with manually trained CNN to blur the sensitive areas of an image/video. We also carefully designed a series of annotation rules on first-floor camera videos and created an annotated first-floor camera image dataset with the annotation rules.&lt;/p&gt;

&lt;h4 id=&quot;keywords-10&quot;&gt;Keywords&lt;/h4&gt;
&lt;p&gt;face blurring, license plate blurring, image annotation, object detection&lt;/p&gt;

&lt;h2 id=&quot;2020-fall&quot;&gt;2020 Fall&lt;/h2&gt;

&lt;h3 id=&quot;multi-resolution-and-density-study-for-object-detection-using-yolov4&quot;&gt;&lt;a href=&quot;https://docs.google.com/document/d/1wAtXJjL9FVSu-Itc1hiAFp_82SgNzFN88jX8w5Kbtts&quot;&gt;Multi-Resolution and Density Study for Object Detection using YOLOv4&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;@author: Richard Samoilenko, Dwiref Oza, Ashvin Jagadeesan&lt;/p&gt;

&lt;h4 id=&quot;abstract-11&quot;&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Deploying deep learning inference for object detection with video streaming presents significant challenges involving efficiency. For the task of traffic intersection monitoring, a high throughput, low latency pipeline is essential for accurate real time detection. Our tasks were to determine object detection performance with YOLOv4 trends with different video resolutions, square vs. 16:9 aspect ratios; and finally object densities on a per-frame basis. Our results indicate clear trends in how each input resolution and aspect ratio affect inference. All results are supported with CUDA profiling, through which we also found that the lab’s Hikvision cameras give better results than GoPro videos. We also report some linear trends in how object density may affect inference times.&lt;/p&gt;

&lt;h4 id=&quot;keywords-11&quot;&gt;Keywords&lt;/h4&gt;
&lt;p&gt;traffic intersection, object detection, YOLOv4, CUDA, profiling, resolution, object density&lt;/p&gt;

&lt;h3 id=&quot;injection-of-efficientdet-model-into-object-detection-pipeline-and-setting-vpn-to-connect-any-device-to-the-cosmos-server&quot;&gt;&lt;a href=&quot;https://docs.google.com/document/d/1vIgWTE-n0fGd6TABP6hEJL7-0V0z18Zo5oRCpL4E4wI&quot;&gt;Injection of EfficientDet model into Object Detection Pipeline and setting VPN to connect any device to the COSMOS server&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;@author: Vedant Dave&lt;/p&gt;

&lt;h4 id=&quot;abstract-12&quot;&gt;Abstract&lt;/h4&gt;
&lt;p&gt;This report aims to highlight the work that has been done throughout the Fall semester. The work has mainly been divided into two parts:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Coming up with a way to connect linux based edge devices to the COSMOS server in order to send and receive data.&lt;/li&gt;
  &lt;li&gt;Injecting EfficientDet object detection model into the object detection pipeline in order to analyze the performance of the model on the custom traffic intersection dataset.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;research-about-ssd-832-adjustment-non-maximum-suppression-and-tensorrt-conversion&quot;&gt;&lt;a href=&quot;https://docs.google.com/document/d/1rmjpAsZ_oj6XKx02YJ74l-uvtG8KKt7Isz0ULwM3Mzk&quot;&gt;Research about SSD-832 Adjustment, Non-Maximum Suppression and TensorRT Conversion&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;@author: Zhuoxu Duan, Zoran Kostic&lt;/p&gt;

&lt;h4 id=&quot;abstract-13&quot;&gt;Abstract&lt;/h4&gt;
&lt;p&gt;As part of the smart city, the traffic intersection program is aiming at detecting pedestrians and vehicles on the street to provide further information about crowd density and traffic busyness. The SSD model is chosen to perform this object detection task as well as building up a pipeline with TensorRT and DeepStream for higher inference speed. In this report, we managed to adjust the SSD model to fit our application and conducted model conversions to take advantage of TensorRT. Corresponding profiling results and problems remaining are also discussed.&lt;/p&gt;

&lt;h4 id=&quot;keywords-12&quot;&gt;Keywords&lt;/h4&gt;
&lt;p&gt;Object Detection, Single Shot Multibox Detector (SSD), TensorRT, DeepStream, Smart City&lt;/p&gt;

&lt;h3 id=&quot;research-about-bounding-box-transformation-832-for-multi-camera-fusion--resolution-study-and-density-study&quot;&gt;&lt;a href=&quot;https://docs.google.com/document/d/1jSOkrcT1Ku8p_QkphiHisBF-4KxgRb9aOBS7iVK6Gnc&quot;&gt;Research about bounding box transformation 832 for Multi-camera-fusion , Resolution Study and Density Study&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;@author: Zihao Xiong, Hongzhe Ye&lt;/p&gt;
&lt;h4 id=&quot;abstract-14&quot;&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Smart-city intersections will play an important role in automated traffic management, help protect pedestrians, and decrease the possibility of accidents. Multiple cameras might be useful to either get more information of the position we are looking for or help to improve the accuracy of detection. In this report, the work in the COSMOS project that we have finished in 2020 Fall will be shown. Bounding box transformation for 832 is evaluated. Also, study about performance of YoloV4 in different model settings, input size and density is presented.&lt;/p&gt;

&lt;h2 id=&quot;2020-summer&quot;&gt;2020 Summer&lt;/h2&gt;

&lt;h3 id=&quot;profiling-and-deploying-mask-r-cnn-with-tensorrt-and-deepstream&quot;&gt;&lt;a href=&quot;https://docs.google.com/document/d/1bF9VKhOJuCHwq7WlV2c_QNsT1RHJWf73MY5ASDPIU3I&quot;&gt;Profiling and Deploying Mask R-CNN with TensorRT and Deepstream&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;@author: Ananye Pandey&lt;/p&gt;
&lt;h4 id=&quot;abstract-15&quot;&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Deploying deep learning inference for object detec tion with video streaming presents significant challenges involving efficiency. For the task of traffic intersection monitoring, a high throughput, low latency pipeline is essential for accurate real time detection. Our task was to learn and demonstrate how to convert a trained Mask R-CNN model into a serialized TensorRT .engine, profile inference on a CUDA dGPU, optimize the engine where needed, and then deploy the engine within the Nvidia Deepstream SDK pipeline, a streaming analytics toolkit. In this report, we outline the key elements of our work, namely the Mask R-CNN network, CUDA profiling, setting up the right working environment, some pitfalls to be avoided with regards to package versioning, TensorRT and UFF engine conversion, and deploying the model with Deepstream. This work was done during the Summer of 2020, for the Traffic Intersection research project under COSMOS.&lt;/p&gt;
&lt;h4 id=&quot;keywords-13&quot;&gt;Keywords&lt;/h4&gt;
&lt;p&gt;Mask R-CNN, CUDA, TensorRT, nvprof, profil ing, Deepstream, Tensorflow, docker&lt;/p&gt;

&lt;h3 id=&quot;research-about-bounding-box-transformation-for-multiple-cameras-fusion-robust-pca-implementation-and-cropping-images-for-yolov4&quot;&gt;&lt;a href=&quot;https://docs.google.com/document/d/1bF9VKhOJuCHwq7WlV2c_QNsT1RHJWf73MY5ASDPIU3I&quot;&gt;Research about bounding box transformation for multiple cameras fusion, Robust PCA implementation and cropping images for YOLOv4&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;@author: Hongzhe Ye&lt;/p&gt;
&lt;h4 id=&quot;abstract-16&quot;&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Smart-city intersections will play an important role in automated traffic management, help protect pedestrians, and decrease the possibility of accidents. Multiple cameras might be useful to either get more information of the position we are looking for or help to improve the accuracy of detection. In this report, the work in the COSMOS project that I have finished in 2020 Summer will be shown. Based on new calibration, Bounding box transformation can help combine information from two cameras. Also, experiment of Robust PCA background subtraction and cropping images for YOLOv4 is represented.&lt;/p&gt;

&lt;h3 id=&quot;object-detection-with-temporal-information&quot;&gt;&lt;a href=&quot;https://docs.google.com/document/d/1oTVBDE3s-sbcprsyv8LcSHr50SoQRH0_o7g4zFR3vIU&quot;&gt;Object Detection with Temporal Information&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;@author: Jeswanth Yadagani&lt;/p&gt;
&lt;h4 id=&quot;abstract-17&quot;&gt;Abstract&lt;/h4&gt;
&lt;p&gt;This report includes information about various Temporal Yolo models that we have tried during summer 2020. Improving the detection accuracy of pedestrians is our main objective. By leveraging temporal information, there is a high chance of improvement in terms of object detection when we have a static background. Certain models in this fashion are explored in this report and the directions in which this study can be taken forward are explained at the end. Also, a structured environment has been created for coming up with new architectures and evaluating performance of each model.&lt;/p&gt;

&lt;h3 id=&quot;2020-summer-report-tracking&quot;&gt;&lt;a href=&quot;https://docs.google.com/document/d/1e0wGHWpvy1OgaXrqtF8SeLaEU42ak_OjpgB8rr1E7l0&quot;&gt;2020 Summer Report: Tracking&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;@author: Mingfei Sun&lt;/p&gt;
&lt;h4 id=&quot;abstract-18&quot;&gt;Abstract&lt;/h4&gt;
&lt;p&gt;In this report, I will discuss work accomplished during the 2020 Summer on the Traffic Intersection research project, under COSMOS. Mainly three things, discriminative correlation filter-based (DCF) tracker, FairMOT tracker, and some other useful experiments.&lt;/p&gt;
&lt;h4 id=&quot;keywords-14&quot;&gt;Keywords&lt;/h4&gt;
&lt;p&gt;Tracking, DCF tracker, FairMOT, DeepStream&lt;/p&gt;

&lt;h3 id=&quot;data-management-using-dvc-data-version-control&quot;&gt;&lt;a href=&quot;https://docs.google.com/document/d/1-lh9PRnH3KRC6nYyMNROdoCXP21I4kU2&quot;&gt;Data Management using DVC (Data Version Control)&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;@author: Tian Yang&lt;/p&gt;
&lt;h4 id=&quot;abstract-19&quot;&gt;Abstract&lt;/h4&gt;
&lt;p&gt;In this report, we will discuss the data management work of machine learning models using DVC (Data Version Control). We first introduce the background, explain why this tool is needed and helpful to be applied on ML projects, then introduce the basic working principles of DVC and part of the workflow of the neural network, YOLO darknet, that will be involved in our experiment. Next we detail the steps of generating stages, connecting them as pipeline, tagging experiments and comparisons of metrics. We end this report by outlining recommended next steps for future work.&lt;/p&gt;
&lt;h4 id=&quot;keywords-15&quot;&gt;Keywords&lt;/h4&gt;
&lt;p&gt;Data Management, Data Versioning Tool, DVC&lt;/p&gt;

&lt;h3 id=&quot;2020-summer-report-tracking-with-fairmot&quot;&gt;&lt;a href=&quot;https://docs.google.com/document/d/1fkSgKhrnxFJmCinJpXydUrUMzUTrGUIzVbCfOwkBFKY&quot;&gt;2020 Summer Report: Tracking with FairMOT&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;@author: Yi Chen&lt;/p&gt;
&lt;h4 id=&quot;abstract-20&quot;&gt;Abstract&lt;/h4&gt;
&lt;p&gt;This report summarizes the accomplishments in 2020 Summer for tracking in the COSMOS Traffic Intersection research project. FairMOT is  a new open source tracking algorithm proposed by the paper ’A Simple Baseline for Multi-Object Tracking’ in 2020. The main tasks of my work is to implement FairMOT to achieve better vehicles and pedestrians tracking performance in our Traffic Intersection project.&lt;/p&gt;

&lt;h3 id=&quot;deepstream-deployment-and-custom-ssd-object-detection-model&quot;&gt;&lt;a href=&quot;https://docs.google.com/document/d/11FtuNB_kXLcVfLeItCQs7VaAUUOCAMvFoK-u5oJUU4g&quot;&gt;DeepStream Deployment and Custom SSD Object Detection Model&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;@author: Yiping Pan&lt;/p&gt;
&lt;h4 id=&quot;abstract-21&quot;&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Traffic Intersection project’s goal is to implement fast object detection and tracking of vehicles and pedestrians on videos collected from real  traffic intersections. In the object detection part, one of the most popular algorithms SSD hasn’t been implemented by our lab researchers. We at the first time built the SSD model on our own so that it provides a good base for researchers to do further improvement, training and testing for SSD-based algorithms. Our trained fine-tuned VGG-SSD model is able to detect the object with an acceptable average precision and it is promising to obtain better results when investing more research on that. On the other hand, we also provide analysis and test on Nvidia DeepStream SDK where we could implement custom YoloV3 and build accelerated video processing pipelines. A well-organized summary of DeepStream configuration concepts and methods is also created.&lt;/p&gt;
&lt;h4 id=&quot;keywords-16&quot;&gt;Keywords&lt;/h4&gt;
&lt;p&gt;SSD, YoloV3, DeepStream5.0, Object Detection&lt;/p&gt;

&lt;h3 id=&quot;research-report-on-data-management-using-dvc-data-version-control&quot;&gt;&lt;a href=&quot;https://docs.google.com/document/d/1DU236v_eak9dC1K8wVv4Vl4axlK3zgAJ24q4p2nAu_E&quot;&gt;Research Report on Data Management Using DVC (Data Version Control)&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;@author: Yanqi Zheng&lt;/p&gt;
&lt;h4 id=&quot;abstract-22&quot;&gt;Abstract&lt;/h4&gt;
&lt;p&gt;DVC can bring agility, reproducibility, and collaboration into the existing data flow. To ease data management, we implement DVC features like storage agnostic, reproducible, and metric tracking.&lt;/p&gt;

&lt;p&gt;According to our study, there are mainly two problems we can fix if implementing DVC in the current YOLO model. One is data versioning, solved by using DVC to keep track of different versions of data. Another is data lineage, solved by constructing deep learning pipelines with DVC.&lt;/p&gt;

&lt;p&gt;DVC improves collaboration among deep learning developers. It helps them to keep a well-organized collection of data sets and allows them to share data versions with each other. It also provides an efficient way to reproduce experiments and show a comparison between different models.&lt;/p&gt;

&lt;h3 id=&quot;single-shot-multibox-detector-ssd-implementation-for-traffic-intersection&quot;&gt;&lt;a href=&quot;https://docs.google.com/document/d/1tDipRQKaZ9v3pLIQcHNasX_9AjwFW_k6tcMUSkrS02A&quot;&gt;Single Shot Multibox Detector (SSD) Implementation for Traffic Intersection&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;@author: Zhuoxu Duan&lt;/p&gt;
&lt;h4 id=&quot;abstract-23&quot;&gt;Abstract&lt;/h4&gt;
&lt;p&gt;This report first makes a description of the current stage of the traffic intersection program. Expectations for the SSD group are clarified. For the DeepStream model conversion task, the report introduced the motivation, possible procedures and challenges of various ways to move SSD models to DeepStream. Finally succeeded in converting PyTorch models to DeepStream plugins. For the custom &amp;amp; training task, SSD model structure, data flow details and prediction principles are illustrated. Implemented SSD300 models with both VGG16 and ResNet50 backbones on PyTorch. Data preparation and feature mining was done for our traffic dataset with the following training processes. Large parts of vehicles can be detected while few pedestrians are recognized. Example results and reasoning for the failure are presented.&lt;/p&gt;
&lt;h4 id=&quot;keywords-17&quot;&gt;Keywords&lt;/h4&gt;
&lt;p&gt;object detection, Single Shot Multibox Detector (SSD), DeepStream&lt;/p&gt;

&lt;h3 id=&quot;summer-2020-report-measurements-for-object-detection-and-tracking-for-cosmos-smart-intersections&quot;&gt;&lt;a href=&quot;https://docs.google.com/document/d/1kQxKhYx2DeSAZT6LpTT87QXxGOltnPinwvH_44qE9KA&quot;&gt;Summer 2020 Report: Measurements for Object Detection and Tracking for COSMOS Smart Intersections&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;@author: Zihao Xiong&lt;/p&gt;
&lt;h4 id=&quot;abstract-24&quot;&gt;Abstract&lt;/h4&gt;
&lt;p&gt;In this report, we will discuss work accomplished from May - August 2020 on the lab’s custom measurement code supporting both object detection and tracking models. We first detail the performance improvements made for both detection measurement and multiple object tracking measurement, each of which now runs approximately 80-90% faster. We next describe the diagnosis and fix of a critical error found in the original detection measurement code. This work also yielded full coverage unit testing for the detection measurement code and additional visualization tooling in support of understanding detection measurement metrics. However, there remain important open questions and therefore there is still work to be done. We end this report by outlining recommended next steps for work on the lab’s native measurement code.&lt;/p&gt;

&lt;h3 id=&quot;cosmos-traffic-intersection-research-final-report&quot;&gt;&lt;a href=&quot;https://docs.google.com/document/d/1GYK7ALfPJ3hdZoWXaljmG52uqOjUwkGP3SP7jm-ulbg&quot;&gt;COSMOS Traffic Intersection Research Final report&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;@author: Zhengye Yang&lt;/p&gt;
&lt;h4 id=&quot;abstract-25&quot;&gt;Abstract&lt;/h4&gt;
&lt;p&gt;This report summarizes the work done during 2020 summer term. In this report, the following fields will be described:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;(i) The summary of COSMOS traffic intersection current process.&lt;/li&gt;
  &lt;li&gt;(ii) Social distancing analysis system (SDAS).&lt;/li&gt;
  &lt;li&gt;(iii) Current status towards real-time detection.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This report can be seen as a summary of the current status of COSMOS smart intersection research.&lt;/p&gt;
&lt;h4 id=&quot;keywords-18&quot;&gt;Keywords&lt;/h4&gt;
&lt;p&gt;COSMOS traffic intersection, object detection, social distancing ,  real-time implementation.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;p&gt;[1] &lt;a href=&quot;https://docs.google.com/document/d/1D8OzTOfXJi62jOoa7rm4GZXMlpZbsgyxPaNRN2eSjFA&quot;&gt;Re-identification Based on Direct Linear Transform&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&quot;https://docs.google.com/document/d/16sdFSG-x9RjIrUeexb57Dh0AVXAMqG1_z-sIhj8dvio&quot;&gt;Unsupervised re-identification in intersections&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] &lt;a href=&quot;https://drive.google.com/file/d/1rxqgTZA0x-dmiXHPGvFCgDXIK6OPTqMS&quot;&gt;Face and License Plate Blurring in Street Level Intersection Videos&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[4] &lt;a href=&quot;https://docs.google.com/document/d/1WtgWKL8gWHjtdCmXvNerKf9fNt9bwhXlrnjiu8yLqZQ&quot;&gt;Data Augmentation with Causal Inference to Improve Small Object Detection&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[5] &lt;a href=&quot;https://docs.google.com/document/d/1Tp3CQ2Dl8hqGf-9WBlW5wnt5Cg55aAKxnZlVxWqSnl8&quot;&gt;2-Stage (2D and 3D) Approach for Small Pedestrian Detection&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[6] &lt;a href=&quot;https://docs.google.com/document/d/1MN5BJrRNADjJCHNc5TMKTbK1df6bJx6-y60whkBzmAw&quot;&gt;Using Data Augmentation to Improve Pedestrian Detection&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[7] &lt;a href=&quot;https://docs.google.com/document/d/13x432QAHevbpWDZiR8qLdnDVngrmJ_-b&quot;&gt;Efficient Det: Comparative Study&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[8] &lt;a href=&quot;https://docs.google.com/document/d/1W7MQMVM0I9u47I-ctbmNgT-Fr-fucKas&quot;&gt;Software engineering support for RTSP streaming Deepstream pipeline&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[9] &lt;a href=&quot;https://docs.google.com/document/d/1OWbmhPi6h6ceY16eAK_PBIcGwjFxQlR0o-mqueWdQpU&quot;&gt;COSMOS Vehicle Counting Stage Report&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[10] &lt;a href=&quot;https://docs.google.com/document/d/1mNmre_IdVJ7WUGaeYMASNb8_d3qMfCazQR09iGyvB4c&quot;&gt;Using Data Augmentation to Improve Pedestrian Detection&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[11] &lt;a href=&quot;https://docs.google.com/document/d/1UvYZEFo7y8Dh5z2uuK8r7NtkgyXUUNJ-sTXi5a8fP5w&quot;&gt;Face and License Plate Blurring for COSMOS Traffic Intersection&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[12] &lt;a href=&quot;https://docs.google.com/document/d/1wAtXJjL9FVSu-Itc1hiAFp_82SgNzFN88jX8w5Kbtts&quot;&gt;Multi-Resolution and Density Study for Object Detection using YOLOv4&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[13] &lt;a href=&quot;https://docs.google.com/document/d/1vIgWTE-n0fGd6TABP6hEJL7-0V0z18Zo5oRCpL4E4wI&quot;&gt;Injection of EfficientDet model into Object Detection Pipeline and setting VPN to connect any device to the COSMOS server&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[14] &lt;a href=&quot;https://docs.google.com/document/d/1rmjpAsZ_oj6XKx02YJ74l-uvtG8KKt7Isz0ULwM3Mzk&quot;&gt;Research about SSD-832 Adjustment, Non-Maximum Suppression and TensorRT Conversion&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[15] &lt;a href=&quot;https://docs.google.com/document/d/1jSOkrcT1Ku8p_QkphiHisBF-4KxgRb9aOBS7iVK6Gnc&quot;&gt;Research about bounding box transformation 832 for Multi-camera-fusion , Resolution Study and Density Study&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[16] &lt;a href=&quot;https://docs.google.com/document/d/1bF9VKhOJuCHwq7WlV2c_QNsT1RHJWf73MY5ASDPIU3I&quot;&gt;Profiling and Deploying Mask R-CNN with TensorRT and Deepstream&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[17] &lt;a href=&quot;https://docs.google.com/document/d/1bF9VKhOJuCHwq7WlV2c_QNsT1RHJWf73MY5ASDPIU3I&quot;&gt;Research about bounding box transformation for multiple cameras fusion, Robust PCA implementation and cropping images for YOLOv4&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[18] &lt;a href=&quot;https://docs.google.com/document/d/1oTVBDE3s-sbcprsyv8LcSHr50SoQRH0_o7g4zFR3vIU&quot;&gt;Object Detection with Temporal Information&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[19] &lt;a href=&quot;https://docs.google.com/document/d/1e0wGHWpvy1OgaXrqtF8SeLaEU42ak_OjpgB8rr1E7l0&quot;&gt;2020 Summer Report: Tracking&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[20] &lt;a href=&quot;https://docs.google.com/document/d/1-lh9PRnH3KRC6nYyMNROdoCXP21I4kU2&quot;&gt;Data Management using DVC (Data Version Control)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[21] &lt;a href=&quot;https://docs.google.com/document/d/1fkSgKhrnxFJmCinJpXydUrUMzUTrGUIzVbCfOwkBFKY&quot;&gt;2020 Summer Report: Tracking with FairMOT&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[22] &lt;a href=&quot;https://docs.google.com/document/d/11FtuNB_kXLcVfLeItCQs7VaAUUOCAMvFoK-u5oJUU4g&quot;&gt;DeepStream Deployment and Custom SSD Object Detection Model&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[23] &lt;a href=&quot;https://docs.google.com/document/d/1DU236v_eak9dC1K8wVv4Vl4axlK3zgAJ24q4p2nAu_E&quot;&gt;Research Report on Data Management Using DVC (Data Version Control)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[24] &lt;a href=&quot;https://docs.google.com/document/d/1tDipRQKaZ9v3pLIQcHNasX_9AjwFW_k6tcMUSkrS02A&quot;&gt;Single Shot Multibox Detector (SSD) Implementation for Traffic Intersection&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[25] &lt;a href=&quot;https://docs.google.com/document/d/1kQxKhYx2DeSAZT6LpTT87QXxGOltnPinwvH_44qE9KA&quot;&gt;Summer 2020 Report: Measurements for Object Detection and Tracking for COSMOS Smart Intersections&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[26] &lt;a href=&quot;https://docs.google.com/document/d/1GYK7ALfPJ3hdZoWXaljmG52uqOjUwkGP3SP7jm-ulbg&quot;&gt;COSMOS Traffic Intersection Research Final report&lt;/a&gt;&lt;/p&gt;</content><author><name>Jiawei Lu</name><email>jl5999@columbia.edu</email></author><category term="studylog" /><category term="zklab" /><summary type="html">Outline 2022 Spring Re-identification Based on Direct Linear Transform Unsupervised re-identification in intersections 2021 Fall Face and License Plate Blurring in Street Level Intersection Videos Data Augmentation with Causal Inference to Improve Small Object Detection 2-Stage (2D and 3D) Approach for Small Pedestrian Detection 2021 Summer Using Data Augmentation to Improve Pedestrian Detection Efficient Det: Comparative Study Software engineering support for RTSP streaming Deepstream pipeline COSMOS Vehicle Counting Stage Report 2021 Spring Using Data Augmentation to Improve Pedestrian Detection Face and License Plate Blurring for COSMOS Traffic Intersection 2020 Fall Multi-Resolution and Density Study for Object Detection using YOLOv4 Injection of EfficientDet model into Object Detection Pipeline and setting VPN to connect any device to the COSMOS server Research about SSD-832 Adjustment, Non-Maximum Suppression and TensorRT Conversion Research about bounding box transformation 832 for Multi-camera-fusion , Resolution Study and Density Study 2020 Summer Profiling and Deploying Mask R-CNN with TensorRT and Deepstream Research about bounding box transformation for multiple cameras fusion, Robust PCA implementation and cropping images for YOLOv4 Object Detection with Temporal Information 2020 Summer Report: Tracking Data Management using DVC (Data Version Control) 2020 Summer Report: Tracking with FairMOT DeepStream Deployment and Custom SSD Object Detection Model Research Report on Data Management Using DVC (Data Version Control) Single Shot Multibox Detector (SSD) Implementation for Traffic Intersection Summer 2020 Report: Measurements for Object Detection and Tracking for COSMOS Smart Intersections COSMOS Traffic Intersection Research Final report</summary></entry><entry><title type="html">[ZK Lab] #4: Concept - Validation</title><link href="https://jiaweilu1999.github.io/studylog/concept_validation.html" rel="alternate" type="text/html" title="[ZK Lab] #4: Concept - Validation" /><published>2022-05-21T00:00:00-07:00</published><updated>2022-05-21T00:00:00-07:00</updated><id>https://jiaweilu1999.github.io/studylog/concept_validation</id><content type="html" xml:base="https://jiaweilu1999.github.io/studylog/concept_validation.html">&lt;h2 id=&quot;validation-dataset&quot;&gt;Validation Dataset&lt;/h2&gt;

&lt;p&gt;A validation data set is a data-set of examples used to tune the hyperparameters (i.e. the architecture) of a classifier. It is sometimes also called the development set or the “dev set”. An example of a hyperparameter for artificial neural networks includes the number of hidden units in each layer. It, as well as the testing set (as mentioned below), should follow the same probability distribution as the training data set.&lt;/p&gt;

&lt;p&gt;In order to avoid overfitting, when any classification parameter needs to be adjusted, it is necessary to have a validation data set in addition to the training and test datasets. For example, if the most suitable classifier for the problem is sought, the training data set is used to train the different candidate classifiers, the validation data set is used to compare their performances and decide which one to take and, finally, the test data set is used to obtain the performance characteristics such as accuracy, sensitivity, specificity, F-measure, and so on. The validation data set functions as a hybrid: it is training data used for testing, but neither as part of the low-level training nor as part of the final testing.&lt;/p&gt;

&lt;p&gt;The basic process of using a validation data set for model selection (as part of training data set, validation data set, and test data set) is:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Since our goal is to find the network having the best performance on new data, the simplest approach to the comparison of different networks is to evaluate the error function using data which is independent of that used for training. Various networks are trained by minimization of an appropriate error function defined with respect to a training data set. The performance of the networks is then compared by evaluating the error function using an independent validation set, and the network having the smallest error with respect to the validation set is selected. This approach is called the hold out method. Since this procedure can itself lead to some overfitting to the validation set, the performance of the selected network should be confirmed by measuring its performance on a third independent set of data called a test set.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;An application of this process is in early stopping, where the candidate models are successive iterations of the same network, and training stops when the error on the validation set grows, choosing the previous model (the one with minimum error).&lt;/p&gt;

&lt;h2 id=&quot;cross-validation&quot;&gt;Cross-validation&lt;/h2&gt;
&lt;p&gt;In order to get more stable results and use all valuable data for training, a data set can be repeatedly split into several training and a validation datasets. This is known as cross-validation. To validate the model performance, an additional test data set held out from cross-validation is normally used.&lt;/p&gt;

&lt;h3 id=&quot;k-fold-cross-validation&quot;&gt;k-Fold Cross-Validation&lt;/h3&gt;
&lt;p&gt;Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample.&lt;/p&gt;

&lt;p&gt;The procedure has a single parameter called \(k\) that refers to the number of groups that a given data sample is to be split into. As such, the procedure is often called \(k\)-fold cross-validation. When a specific value for \(k\) is chosen, it may be used in place of \(k\) in the reference to the model, such as \(k=10\) becoming 10-fold cross-validation.&lt;/p&gt;

&lt;p&gt;Cross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data. That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model.&lt;/p&gt;

&lt;p&gt;It is a popular method because it is simple to understand and because it generally results in a less biased or less optimistic estimate of the model skill than other methods, such as a simple train/test split.&lt;/p&gt;

&lt;p&gt;The general procedure is as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Shuffle the dataset randomly.&lt;/li&gt;
  &lt;li&gt;Split the dataset into \(k\) groups&lt;/li&gt;
  &lt;li&gt;For each unique group:
    &lt;ul&gt;
      &lt;li&gt;Take the group as a hold out or test data set&lt;/li&gt;
      &lt;li&gt;Take the remaining groups as a training data set&lt;/li&gt;
      &lt;li&gt;Fit a model on the training set and evaluate it on the test set&lt;/li&gt;
      &lt;li&gt;Retain the evaluation score and discard the model&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Summarize the skill of the model using the sample of model evaluation scores&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Importantly, each observation in the data sample is assigned to an individual group and stays in that group for the duration of the procedure. This means that each sample is given the opportunity to be used in the hold out set 1 time and used to train the model \(k-1\) times.&lt;/p&gt;

&lt;p&gt;It is also important that any preparation of the data prior to fitting the model occur on the CV-assigned training dataset within the loop rather than on the broader data set. This also applies to any tuning of hyperparameters. A failure to perform these operations within the loop may result in data leakage and an optimistic estimate of the model skill.&lt;/p&gt;

&lt;p&gt;The results of a k-fold cross-validation run are often summarized with the mean of the model skill scores. It is also good practice to include a measure of the variance of the skill scores, such as the standard deviation or standard error.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;p&gt;[1] &lt;a href=&quot;https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets&quot;&gt;Wikipedia: Training, validation, and test data sets&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&quot;https://machinelearningmastery.com/k-fold-cross-validation/&quot;&gt;A Gentle Introduction to k-fold Cross-Validation&lt;/a&gt;&lt;/p&gt;</content><author><name>Jiawei Lu</name><email>jl5999@columbia.edu</email></author><category term="studylog" /><category term="zklab" /><summary type="html">Validation Dataset</summary></entry><entry><title type="html">[ZK Lab] #3: Platform - Weights &amp;amp; Biases</title><link href="https://jiaweilu1999.github.io/studylog/platform_weights_and_biases.html" rel="alternate" type="text/html" title="[ZK Lab] #3: Platform - Weights &amp;amp; Biases" /><published>2022-05-20T00:00:00-07:00</published><updated>2022-05-20T00:00:00-07:00</updated><id>https://jiaweilu1999.github.io/studylog/platform_weights_and_biases</id><content type="html" xml:base="https://jiaweilu1999.github.io/studylog/platform_weights_and_biases.html">&lt;p&gt;Weights &amp;amp; Biases is the machine learning platform for developers to build better models faster.&lt;/p&gt;

&lt;p&gt;Use W&amp;amp;B’s lightweight, interoperable tools to&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;track experiments and version quickly&lt;/li&gt;
  &lt;li&gt;iterate on datasets&lt;/li&gt;
  &lt;li&gt;evaluate model performance&lt;/li&gt;
  &lt;li&gt;reproduce models&lt;/li&gt;
  &lt;li&gt;visualize results&lt;/li&gt;
  &lt;li&gt;spot regressions&lt;/li&gt;
  &lt;li&gt;share findings with colleagues&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiment-tracking&quot;&gt;Experiment Tracking&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wandb&lt;/code&gt; Python library to track machine learning experiments with a few lines of code.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Have lightweight &lt;a href=&quot;https://docs.wandb.ai/guides/integrations&quot;&gt;integrations&lt;/a&gt; for PyTorch and Keras.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;integrations&quot;&gt;Integrations&lt;/h2&gt;
&lt;p&gt;Weights &amp;amp; Biases integrations make it fast and easy to set up experiment tracking and data versioning inside existing projects.&lt;/p&gt;

&lt;p&gt;Useful Links:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/wandb/examples&quot;&gt;Examples&lt;/a&gt;: GitHub repo with working, end-to-end code examples for all of our integrations&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/wandb/examples/tree/master/colabs&quot;&gt;Colab&lt;/a&gt;: Try out W&amp;amp;B inside different frameworks, such as PyTorch Lightning, in an interactive notebook – no installation required&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PLD80i8An1OEGajeVo15ohAQYF1Ttle0lk&quot;&gt;Video Tutorials&lt;/a&gt;: Learn to use W&amp;amp;B with YouTube videos for PyTorch, Keras, and more.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;collaborative-reports&quot;&gt;Collaborative Reports&lt;/h2&gt;
&lt;p&gt;
    &lt;iframe width=&quot;735&quot; height=&quot;414&quot; src=&quot;https://www.youtube.com/embed/2xeJIv_K_eI&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/p&gt;

&lt;p&gt;Reports let you organize and embed visualizations, describe your findings, share updates with collaborators, and more.&lt;/p&gt;

&lt;h3 id=&quot;typical-use-cases-for-reports&quot;&gt;Typical use cases for reports&lt;/h3&gt;

&lt;h4 id=&quot;notes-add-a-visualization-with-a-quick-summary&quot;&gt;Notes: Add a visualization with a quick summary&lt;/h4&gt;

&lt;p&gt;Capture an important observation, an idea for future work, or a milestone reached in the development of a project. All experiment runs in your report will link to their parameters, metrics, logs, and code, so you can save the full context of your work.&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Labeling and Crowdsourcing&quot; src=&quot;../../../assets/img/blog/2022-05-20/wnb_1.png&quot; width=&quot;550&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;collaboration-share-findings-with-your-colleagues&quot;&gt;Collaboration: Share findings with your colleagues&lt;/h4&gt;

&lt;p&gt;Explain how to get started with a project, share what you’ve observed so far, and synthesize the latest findings. Your colleagues can make suggestions or discuss details using comments on any panel or at the end of the report.&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Labeling and Crowdsourcing&quot; src=&quot;../../../assets/img/blog/2022-05-20/wnb_2.png&quot; width=&quot;550&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;work-log-track-what-youve-tried-and-plan-next-steps&quot;&gt;Work log: Track what you’ve tried and plan next steps&lt;/h4&gt;

&lt;p&gt;Write down your thoughts on experiments, your findings, and any gotchas and next steps as you work through a project, keeping everything organized in one place. This lets you “document” all the important pieces beyond your scripts.&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Labeling and Crowdsourcing&quot; src=&quot;../../../assets/img/blog/2022-05-20/wnb_3.png&quot; width=&quot;550&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;data--model-versioning&quot;&gt;Data + Model Versioning&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://wandb.me/artifacts-quickstart&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Use W&amp;amp;B Artifacts for dataset versioning, model versioning, and tracking dependencies and results across machine learning pipelines. Think of an artifact as a versioned folder of data. You can store entire datasets directly in artifacts, or use artifact references to point to data in other systems like S3, GCP, or your own system.&lt;/p&gt;

&lt;h2 id=&quot;data-visualization&quot;&gt;Data Visualization&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://wandb.me/tables-colab&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Use W&amp;amp;B Tables to log, query, and analyze tabular data. Understand your datasets, visualize model predictions, and share insights in a central dashboard.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Compare changes precisely across models, epochs, or individual examples&lt;/li&gt;
  &lt;li&gt;Understand higher-level patterns in your data&lt;/li&gt;
  &lt;li&gt;Capture and communicate your insights with visual samples&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hyperparameter-tuning&quot;&gt;Hyperparameter Tuning&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://wandb.me/sweeps-colab&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;
&lt;iframe width=&quot;735&quot; height=&quot;414&quot; src=&quot;https://www.youtube.com/embed/9zrmUIlScdY&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/p&gt;

&lt;p&gt;Use Weights &amp;amp; Biases Sweeps to automate hyperparameter optimization and explore the space of possible models.&lt;/p&gt;

&lt;h2 id=&quot;model-management&quot;&gt;Model Management&lt;/h2&gt;
&lt;p&gt;A Model Registry is a system of record for organizing ML Models - often serving as an interface between model producers and consumers. This guide will show you how to use W&amp;amp;B as a Model Registry to track and report on the complete workflow of developing a model:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Model Catalog &amp;amp; Versioning: Save and restore every version of your model &amp;amp; learned parameters - organize versions by use case and objective.&lt;/li&gt;
  &lt;li&gt;Model Metadata: Track training metrics, assign custom metadata, and document rich markdown descriptions of your models.&lt;/li&gt;
  &lt;li&gt;Model Lineage Tracking &amp;amp; Reproducibility: Track the exact code, hyperparameters, &amp;amp; training dataset used to produce the model.&lt;/li&gt;
  &lt;li&gt;Model Lifecycle: Promote promising models to positions like “staging” or “production” - allowing downstream users to fetch the best model automatically.
Model Reporting: Create a Report dashboard to summarize model progression and performance over time&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jiawei Lu</name><email>jl5999@columbia.edu</email></author><category term="studylog" /><category term="zklab" /><summary type="html">Weights &amp;amp; Biases is the machine learning platform for developers to build better models faster.</summary></entry><entry><title type="html">[ZK Lab] #2: Platform - Hugging Face</title><link href="https://jiaweilu1999.github.io/studylog/platform_hugging_face.html" rel="alternate" type="text/html" title="[ZK Lab] #2: Platform - Hugging Face" /><published>2022-05-20T00:00:00-07:00</published><updated>2022-05-20T00:00:00-07:00</updated><id>https://jiaweilu1999.github.io/studylog/platform_hugging_face</id><content type="html" xml:base="https://jiaweilu1999.github.io/studylog/platform_hugging_face.html">&lt;h2 id=&quot;hugging-face&quot;&gt;&lt;a href=&quot;https://huggingface.co/&quot;&gt;Hugging Face&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&quot;goal&quot;&gt;Goal&lt;/h3&gt;
&lt;p&gt;Hugging Face Hub is building the largest collection of models, datasets and metrics in order to democratize and advance AI for everyone.&lt;/p&gt;

&lt;h3 id=&quot;repository&quot;&gt;Repository&lt;/h3&gt;
&lt;p&gt;The Hugging Face Hub hosts Git-based repositories which are storage spaces that can contain all your files.&lt;/p&gt;

&lt;p&gt;The Hub currently hosts three different repo types:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;models&lt;/li&gt;
  &lt;li&gt;datasets&lt;/li&gt;
  &lt;li&gt;Spaces, which are ML demo apps&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These repositories have multiple advantages over other hosting solutions:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;versioning&lt;/li&gt;
  &lt;li&gt;commit history and diffs&lt;/li&gt;
  &lt;li&gt;branches&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;On top of that, Hugging Face Hub repositories have many other advantages, for instance for models:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Model repos provide useful metadata about their tasks, languages, metrics, etc.&lt;/li&gt;
  &lt;li&gt;Anyone can play with the model directly in the browser!&lt;/li&gt;
  &lt;li&gt;Training metrics charts are displayed if the repository contains TensorBoard traces.&lt;/li&gt;
  &lt;li&gt;An API is provided to use the models in production settings.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/docs/hub/libraries&quot;&gt;Over 10 frameworks&lt;/a&gt; such as Transformers, Asteroid and ESPnet support using models from the Hugging Face Hub.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;widget&quot;&gt;widget&lt;/h3&gt;
&lt;p&gt;Many model repos have a widget that allows anyone to do inference directly in the browser.&lt;/p&gt;

&lt;p&gt;Some examples:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/spacy/en_core_web_sm?text=My+name+is+Sarah+and+I+live+in+London&quot;&gt;Named Entity Recognition&lt;/a&gt; using spaCy[https://spacy.io/].&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/google/vit-base-patch16-224&quot;&gt;Image Classification&lt;/a&gt; using &lt;a href=&quot;https://github.com/huggingface/transformers&quot;&gt;Transformers&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/julien-c/ljspeech_tts_train_tacotron2_raw_phn_tacotron_g2p_en_no_space_train&quot;&gt;Text to Speech&lt;/a&gt; using &lt;a href=&quot;https://github.com/espnet/espnet&quot;&gt;ESPnet&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/osanseviero/full-sentence-distillroberta3&quot;&gt;Sentence Similarity&lt;/a&gt; using &lt;a href=&quot;https://github.com/UKPLab/sentence-transformers&quot;&gt;Sentence Transformers&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can try out all the widgets &lt;a href=&quot;https://huggingface-widgets.netlify.app/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;inference-api&quot;&gt;Inference API&lt;/h3&gt;
&lt;p&gt;The Inference API allows you to send HTTP requests to models in the Hugging Face Hub. The Inference API is 2x to 10x faster than the widgets.&lt;/p&gt;

&lt;h3 id=&quot;explore-hugging-face-hub&quot;&gt;Explore Hugging Face Hub&lt;/h3&gt;

&lt;iframe width=&quot;735&quot; height=&quot;414&quot; src=&quot;https://www.youtube.com/embed/XvSGPZFEjDY&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h3 id=&quot;models-type-of-inference-api-and-widget&quot;&gt;Model’s Type of Inference API and Widget&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pipeline_tag&lt;/code&gt;: determine which pipeline and widget to display&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;config.json&lt;/code&gt;: transformers&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;loadpush-fromto-the-hub&quot;&gt;Load/Push from/to the Hub&lt;/h3&gt;

&lt;iframe width=&quot;735&quot; height=&quot;414&quot; src=&quot;https://www.youtube.com/embed/rkCly_cbMBk&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;</content><author><name>Jiawei Lu</name><email>jl5999@columbia.edu</email></author><category term="studylog" /><category term="zklab" /><summary type="html">Hugging Face</summary></entry><entry><title type="html">[ZK Lab] #1: Concept - Data Centric AI</title><link href="https://jiaweilu1999.github.io/studylog/concept_data_centric_AI.html" rel="alternate" type="text/html" title="[ZK Lab] #1: Concept - Data Centric AI" /><published>2022-05-19T00:00:00-07:00</published><updated>2022-05-19T00:00:00-07:00</updated><id>https://jiaweilu1999.github.io/studylog/concept_data_centric_AI</id><content type="html" xml:base="https://jiaweilu1999.github.io/studylog/concept_data_centric_AI.html">&lt;h2 id=&quot;data-centric-ai&quot;&gt;&lt;a href=&quot;https://datacentricai.org/&quot;&gt;Data Centric AI&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&quot;definition&quot;&gt;Definition&lt;/h3&gt;
&lt;p&gt;Data-centric AI is the discipline of systematically engineering the data used to build an AI system.&lt;/p&gt;

&lt;h3 id=&quot;data-centric-ai-resource-hub&quot;&gt;Data-centric AI Resource Hub&lt;/h3&gt;
&lt;p&gt;A place to share cutting edge techniques and best practices for using data centric AI methods to build successful machine learning systems.&lt;/p&gt;

&lt;h3 id=&quot;topics&quot;&gt;Topics&lt;/h3&gt;
&lt;h4 id=&quot;labeling-and-crowdsourcing&quot;&gt;&lt;a href=&quot;https://datacentricai.org/labeling-and-crowdsourcing/&quot;&gt;Labeling and Crowdsourcing&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;&lt;img alt=&quot;Labeling and Crowdsourcing&quot; src=&quot;../../../assets/img/blog/2022-05-19/label-and-crowdsourcing.webp&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Suggestions:
    &lt;ul&gt;
      &lt;li&gt;Label many of the examples yourself before designing the task.&lt;/li&gt;
      &lt;li&gt;Pay and treat your workers fairly.&lt;/li&gt;
      &lt;li&gt;Always start with small pilots.&lt;/li&gt;
      &lt;li&gt;Always assume that the annotators are trying hard to build a model of your intentions: when something goes wrong, your reaction should be “what did I do wrong in communicating my intent?”, not “why weren’t they paying attention?”&lt;/li&gt;
      &lt;li&gt;Train with feedback.&lt;/li&gt;
      &lt;li&gt;It can often make sense to hire fewer people, more full time.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;data-augmentation&quot;&gt;&lt;a href=&quot;https://datacentricai.org/data-augmentation/&quot;&gt;Data Augmentation&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;&lt;img alt=&quot;Labeling and Crowdsourcing&quot; src=&quot;../../../assets/img/blog/2022-05-19/data-augmentation.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Data limitations:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Domain gaps&lt;/strong&gt;: The data you train your model with is quite different from the data you have to predict on in the real world.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Data bias&lt;/strong&gt;: When the data you collect has imbalances due to societal bias, how can you design methods that can overcome them?&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Data noise&lt;/strong&gt;: Noise can come from a variety of sources, including where labels are ambiguous, cluttered, or otherwise corrupted.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Definition of Data Augmentation:
    &lt;ul&gt;
      &lt;li&gt;Self-Supervision: When you have limited labeled data, you can try combining it with unlabeled data.
        &lt;ul&gt;
          &lt;li&gt;rotation&lt;/li&gt;
          &lt;li&gt;cropping&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Synthetic Data: While synthetic data is still in its infancy, there has been ongoing advances in generative models and it will become hugely important in the future for testing systems such as autonomous driving or robot learning.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Data-centric principles in data augmentation:
    &lt;ul&gt;
      &lt;li&gt;Core: the balance of positive and negative examples.&lt;/li&gt;
      &lt;li&gt;Also: combine self-supervision with weak supervision.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;data-in-deployment&quot;&gt;&lt;a href=&quot;https://datacentricai.org/data-in-deployment/&quot;&gt;Data in Deployment&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;&lt;img alt=&quot;Labeling and Crowdsourcing&quot; src=&quot;../../../assets/img/blog/2022-05-19/data-in-deployment.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Technical debt in machine learning:
    &lt;ul&gt;
      &lt;li&gt;The concept of technical debt originally comes from the world of software engineering, where it has often been found that pushing to develop software very quickly can create long term maintenance costs that must be paid back later, and that if left unaddressed can compound over time.&lt;/li&gt;
      &lt;li&gt;ML code – the bit that we tend to think of as the cool part – is actually a small component of the overall system.&lt;/li&gt;
      &lt;li&gt;In a lot of settings it is not actually statistically useful to get more data.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Three places to start:
    &lt;ul&gt;
      &lt;li&gt;Audit and monitor data quality.&lt;/li&gt;
      &lt;li&gt;Create data sheets for data sets.&lt;/li&gt;
      &lt;li&gt;Create and apply stress tests using data.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jiawei Lu</name><email>jl5999@columbia.edu</email></author><category term="studylog" /><category term="zklab" /><summary type="html">Data Centric AI</summary></entry><entry><title type="html">Transformer</title><link href="https://jiaweilu1999.github.io/studylog/Transformer.html" rel="alternate" type="text/html" title="Transformer" /><published>2022-04-14T00:00:00-07:00</published><updated>2022-04-14T00:00:00-07:00</updated><id>https://jiaweilu1999.github.io/studylog/Transformer</id><content type="html" xml:base="https://jiaweilu1999.github.io/studylog/Transformer.html">&lt;p&gt;&lt;img src=&quot;/assets/img/blog/2022-04-14/1.png&quot; alt=&quot;transformer struct&quot; /&gt;&lt;/p&gt;

&lt;p class=&quot;figcaption&quot;&gt;The full model architecture of the transformer.[3]&lt;/p&gt;

&lt;p&gt;“&lt;a href=&quot;https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf&quot;&gt;Attention is All you Need&lt;/a&gt;” (Vaswani, et al., 2017), without a doubt, is one of the most impactful and interesting paper in 2017. It presented a lot of improvements to the soft attention and make it possible to do seq2seq modeling without recurrent network units. The proposed “transformer” model is entirely built on the self-attention mechanisms without using sequence-aligned recurrent architecture.&lt;/p&gt;

&lt;p&gt;The secret recipe is carried in its model architecture.&lt;/p&gt;

&lt;h2 id=&quot;key-value-and-query&quot;&gt;Key, Value and Query&lt;/h2&gt;
&lt;p&gt;The major component in the transformer is the unit of multi-head self-attention mechanism. The transformer views the encoded representation of the input as a set of &lt;strong&gt;key-value&lt;/strong&gt; pairs, \((\mathbf{K}, \mathbf{V})\), both of dimension \(n\) (input sequence length); in the context of NMT, both the keys and values are the encoder hidden states. In the decoder, the previous output is compressed into a &lt;strong&gt;query&lt;/strong&gt; (\(\mathbf{Q}\) of dimension \(m\)) and the next output is produced by mapping this query and the set of keys and values.&lt;/p&gt;

&lt;p&gt;The transformer adopts the scaled dot-product attention: the output is a weighted sum of the values, where the weight assigned to each value is determined by the dot-product of the query with all the keys:&lt;/p&gt;

\[\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left( \frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{n}}\right) \mathbf{V}\]

&lt;h2 id=&quot;multi-head-self-attention&quot;&gt;Multi-Head Self-Attention&lt;/h2&gt;
&lt;p&gt;Rather than only computing the attention once, the multi-head mechanism runs through the scaled dot-product attention multiple times in parallel. The independent attention outputs are simply concatenated and linearly transformed into the expected dimensions. I assume the motivation is because ensembling always helps? ;) According to the paper, “multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.”&lt;/p&gt;

\[\begin{align}
\text{MultiHead}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) &amp;amp; = [\text{head}_1; ...; \text{head}_h]\mathbf{W}^O \\
\text{where head_i} &amp;amp; = \text{Attention}(\mathbf{QW}_i^Q, \mathbf{KW}_i^K, \mathbf{VW}_i^V)
\end{align}\]

&lt;p&gt;where \(\mathbf{W}_i^Q\), \(\mathbf{W}_i^K\), \(\mathbf{W}_i^V\) and \(\mathbf{W}_i^O\) are parameter matrices to be learned.&lt;/p&gt;

&lt;h2 id=&quot;encoder&quot;&gt;Encoder&lt;/h2&gt;
&lt;p&gt;The encoder generates an attention-based representation with capability to locate a specific piece of information from a potentially infinitely-large context.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A stack of \(N=6\) identical layers.&lt;/li&gt;
  &lt;li&gt;Each layer has a &lt;strong&gt;multi-head self-attention layer&lt;/strong&gt; and a simple position-wise &lt;strong&gt;fully connected feed-forward network&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Each sub-layer adopts a &lt;strong&gt;residual&lt;/strong&gt; connection and a layer &lt;strong&gt;normalization&lt;/strong&gt;. All the sub-layers output data of the same dimension \(d_\text{model} = 512\).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;decoder&quot;&gt;Decoder&lt;/h2&gt;
&lt;p&gt;The decoder is able to retrieval from the encoded representation.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A stack of N = 6 identical layers&lt;/li&gt;
  &lt;li&gt;Each layer has two sub-layers of multi-head attention mechanisms and one sub-layer of fully-connected feed-forward network.&lt;/li&gt;
  &lt;li&gt;Similar to the encoder, each sub-layer adopts a residual connection and a layer normalization.&lt;/li&gt;
  &lt;li&gt;The first multi-head attention sub-layer is modified to prevent positions from attending to subsequent positions, as we don’t want to look into the future of the target sequence when predicting the current position.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;full-architecture&quot;&gt;Full Architecture&lt;/h2&gt;
&lt;p&gt;Finally here is the complete view of the transformer’s architecture:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Both the source and target sequences first go through embedding layers to produce data of the same dimension \(d_\text{model} = 512\).&lt;/li&gt;
  &lt;li&gt;To preserve the position information, a sinusoid-wave-based positional encoding is applied and summed with the embedding output.&lt;/li&gt;
  &lt;li&gt;A softmax and linear layer are added to the final decoder output.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;p&gt;[1] &lt;a href=&quot;https://lilianweng.github.io/posts/2018-06-24-attention/&quot;&gt;Lil’Log: Attention? Attention!&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&quot;https://jalammar.github.io/illustrated-transformer/&quot;&gt;The Illustrated Transformer&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] &lt;a href=&quot;https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf&quot;&gt;Attention Is All You Need&lt;/a&gt;&lt;/p&gt;</content><author><name>Jiawei Lu</name><email>jl5999@columbia.edu</email></author><category term="studylog" /><category term="dl" /><summary type="html"></summary></entry></feed>