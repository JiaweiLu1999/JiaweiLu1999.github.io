<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="zh, en"><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="zh, en" /><updated>2022-04-13T22:09:48-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Jiawei Lu</title><subtitle>Hello World!  I&apos;m Jiawei Lu, a Master Student at Columbia University.  My research interest is Computer Vision, Deep Learning and Reinforcement Learning.
</subtitle><author><name>Jiawei Lu</name><email>jl5999@columbia.edu</email></author><entry><title type="html">Attention Mechanism</title><link href="http://localhost:4000/studylog/Attention.html" rel="alternate" type="text/html" title="Attention Mechanism" /><published>2022-04-13T00:00:00-04:00</published><updated>2022-04-13T00:00:00-04:00</updated><id>http://localhost:4000/studylog/Attention</id><content type="html" xml:base="http://localhost:4000/studylog/Attention.html">&lt;h2 id=&quot;encoder-decoder-in-nlp&quot;&gt;Encoder-Decoder in NLP&lt;/h2&gt;

&lt;p&gt;Encoder-Decoder This framework is a good illustration of the core ideas of machine learning:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Transforming real-world problems into mathematical problems and solving real-world problems by solving mathematical problems.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Encoder’s role is to “transform real problems into mathematical problems.”&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Decoder’s role is to “solve mathematical problems and transform them into real-world solutions.”&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;notes&quot;&gt;Notes&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Regardless of the length of the input and output, the length of the vector in the middle (the output of encoder) is fixed (this is also its drawback, as explained below).&lt;/li&gt;
  &lt;li&gt;Different encoders and decoders can be selected depending on the task (can be one RNN But usually its variant LSTM Or CRANE).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As long as it conforms to the above framework, it can be collectively referred to as the Encoder-Decoder model. Speaking of the Encoder-Decoder model, a term is often mentioned - Seq2Seq.&lt;/p&gt;

&lt;h3 id=&quot;seq2seq&quot;&gt;Seq2Seq&lt;/h3&gt;

&lt;p&gt;Seq2Seq (short for Sequence-to-sequence), as literally, enters a sequence and outputs another sequence. The most important aspect of this structure is that the length of the input sequence and the output sequence are variable.&lt;/p&gt;

&lt;h3 id=&quot;origin-of-seq2seq&quot;&gt;Origin of Seq2Seq&lt;/h3&gt;

&lt;p&gt;Before the Seq2Seq framework was proposed, deep neural networks achieved very good results in image classification and other issues. In the problem that it is good at solving, the input and output can usually be represented as a fixed-length vector. If the length is slightly changed, the zero-padding operation is used.&lt;/p&gt;

&lt;p&gt;However, many important issues, such as machine translation, speech recognition, automatic dialogue, etc., are expressed in sequence, and their length is not known in advance. Therefore, how to break through the limitations of the previous deep neural network, so that it can adapt to these scenarios, has become a research hotspot since 2013, and the Seq2Seq framework came into being.&lt;/p&gt;

&lt;h3 id=&quot;relationship-between-seq2seq-and-encoder-decoder&quot;&gt;Relationship between “Seq2Seq” and “Encoder-Decoder”&lt;/h3&gt;

&lt;p&gt;Seq2Seq does not specifically refer to specific methods. For the purpose of “input is a sequence, output is also a sequence”, it can be collectively referred to as Seq2Seq model.&lt;/p&gt;

&lt;p&gt;The specific methods used by Seq2Seq are basically in the scope of the Encoder-Decoder model.&lt;/p&gt;

&lt;p&gt;To sum up:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Seq2Seq belongs to the broad category of Encoder-Decoder&lt;/li&gt;
  &lt;li&gt;Seq2Seq emphasizes the purpose, Encoder-Decoder emphasizes the method&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;defects-of-encoder-decoder&quot;&gt;Defects of Encoder-Decoder&lt;/h3&gt;

&lt;p&gt;When the input information is too long, some information will be lost in the output.&lt;/p&gt;

&lt;p&gt;Attention solves the problem of information loss.&lt;/p&gt;

&lt;h2 id=&quot;principle-of-attention&quot;&gt;Principle of Attention&lt;/h2&gt;

&lt;p&gt;Attention is a technique that mimics cognitive attention. The effect enhances some parts of the input data while diminishing other parts — the thought being that the network should devote more focus to that small but important part of the data. Learning which part of the data is more important than others depends on the context and is trained by gradient descent.&lt;/p&gt;

&lt;p&gt;In a nutshell, attention in deep learning can be broadly interpreted as a vector of importance weights: in order to predict or infer one element, such as a pixel in an image or a word in a sentence, we estimate using the attention vector how strongly it is correlated with (or “attends to” as you may have read in many papers) other elements and take the sum of their values weighted by the attention vector as the approximation of the target.&lt;/p&gt;

&lt;h2 id=&quot;definition-of-attention-mechanism-in-neural-machine-translation&quot;&gt;Definition of Attention Mechanism in Neural Machine Translation&lt;/h2&gt;

&lt;p&gt;Say, we have a source sequence \(\mathbf{x}\) of length \(n\) and try to output a target sequence \(y\) of length \(m\):&lt;/p&gt;

\[\begin{align}
    \mathbf{x} &amp;amp; = [x_1, x_2, ..., x_n]\\
    \mathbf{y} &amp;amp; = [y_1, y_2, ..., y_m]
\end{align}\]

&lt;p&gt;The encoder is a bidirectional RNN (or other recurrent network setting of your choice) with a forward hidden state \(\overrightarrow{\mathbf{h}_i}\) and a backward one \(\overleftarrow{\mathbf{h}_i}\). A simple concatenation of two represents the encoder state. The motivation is to include both the preceding and following words in the annotation of one word.&lt;/p&gt;

\[\mathbf{h}_i = [\overrightarrow{\mathbf{h}_i}^\top; \overleftarrow{\mathbf{h}_i}^\top]^\top, \ i = 1,...,n\]

&lt;p&gt;The decoder network has hidden state \(\mathbf{s}_t = f(\mathbf{s}_{t-1}, y_{t-1}, \mathbf{c}_t)\) for the output word at position \(t\), \(t = 1, ..., m\), where the context vector \(\mathbf{c}_t\) is a sum of hidden states of the input sequence, weighted by alignment scores:&lt;/p&gt;

\[\begin{align}
    \mathbf{c}_t &amp;amp; = \sum_{i=1}^n \alpha_{t,i} \mathbf{h_i} \\
    \alpha_{t,i} &amp;amp; = \text{align}(y_t, x_i) \\
    &amp;amp; = \frac{\exp(\text{score}(\mathbf{s}_{t-1}, \mathbf{h}_i))}{\sum_{i&apos; = 1}^n \exp(\text{score}(\mathbf{s}_{t-1}, \mathbf{h}_{i&apos;}))}
\end{align}\]

&lt;p&gt;The alignment model assigns a score \(\alpha_{t, i}\) to the pair of input at position \(i\) and output at position \(t\), \((y_t, x_i)\),  based on how well they match. The set of \(\{\alpha_{t, i\}\) are weights defining how much of each source hidden state should be considered for each output. In Bahdanau’s paper, the alignment score \(\alpha\) is parametrized by a &lt;strong&gt;feed-forward network&lt;/strong&gt; with a single hidden layer and this network is jointly trained with other parts of the model. The score function is therefore in the following form, given that tanh is used as the non-linear activation function:&lt;/p&gt;

&lt;p&gt;\(\text{score}(\mathbf{s}_t, \mathbf{h}_i) = \mathbf{v}_a^\top \tanh \left( \mathbf{W}_a \left[ \mathbf{s}_t; \mathbf{h}_i \right] \right)\)
where both \(\mathbf{v}_a\) and \(\mathbf{W}_a\) are weight matrices to be learned in the alignment model.&lt;/p&gt;

&lt;h2 id=&quot;self-attention&quot;&gt;Self-Attention&lt;/h2&gt;
&lt;p&gt;Self-attention, also known as intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the same sequence. It has been shown to be very useful in machine reading, abstractive summarization, or image description generation.&lt;/p&gt;

&lt;p&gt;The long short-term memory network paper used self-attention to do machine reading. In the example below, the self-attention mechanism enables us to learn the correlation between the current words and the previous part of the sentence.&lt;/p&gt;

&lt;h2 id=&quot;soft-vs-hard-attention&quot;&gt;Soft vs Hard Attention&lt;/h2&gt;
&lt;p&gt;In the &lt;a href=&quot;http://proceedings.mlr.press/v37/xuc15.pdf&quot;&gt;show, attend and tell&lt;/a&gt; paper, attention mechanism is applied to images to generate captions. The image is first encoded by a CNN to extract features. Then a LSTM decoder consumes the convolution features to produce descriptive words one by one, where the weights are learned through attention. The visualization of the attention weights clearly demonstrates which regions of the image the model is paying attention to so as to output a certain word.&lt;/p&gt;

&lt;p&gt;This paper first proposed the distinction between “soft” vs “hard” attention, based on whether the attention has access to the entire image or only a patch:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Soft Attention: the alignment weights are learned and placed “softly” over all patches in the source image; essentially the same type of attention as in &lt;a href=&quot;https://arxiv.org/abs/1409.0473&quot;&gt;Bahdanau et al., 2015&lt;/a&gt;.
    &lt;ul&gt;
      &lt;li&gt;Pro: the model is smooth and differentiable.&lt;/li&gt;
      &lt;li&gt;Con: expensive when the source input is large.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Hard Attention: only selects one patch of the image to attend to at a time.
    &lt;ul&gt;
      &lt;li&gt;Pro: less calculation at the inference time.&lt;/li&gt;
      &lt;li&gt;Con: the model is non-differentiable and requires more complicated techniques such as variance reduction or reinforcement learning to train. (&lt;a href=&quot;https://arxiv.org/abs/1508.04025&quot;&gt;Luong, et al., 2015&lt;/a&gt;)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;advantage-of-attention-mechanism&quot;&gt;Advantage of Attention mechanism&lt;/h2&gt;

&lt;p&gt;3 main reasons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Less source&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In comparison, the complexity is smaller and the parameters are less than CNN and RNN based model. Therefore, the requirements for computing power are even smaller.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;High speed&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Attention solves the problem that RNN cannot be computed in parallel. Each step of the Attention mechanism calculation does not depend on the calculation results of the previous step, so it can be processed in parallel as CNN.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Good result&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Before the introduction of the Attention mechanism, there was a problem that everyone had been annoyed: long-distance information would be weakened, just like people with weak memory, and the same thing could not be remembered in the past.&lt;/p&gt;

&lt;p&gt;However, Attention can make model focus without losing important information.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;p&gt;[1] Lil’Log: Attention? Attention! (&lt;a href=&quot;https://lilianweng.github.io/posts/2018-06-24-attention/#definition&quot;&gt;Link&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;[2] Zhihu BLOG (&lt;a href=&quot;https://zhuanlan.zhihu.com/p/91839581&quot;&gt;Link&lt;/a&gt;)&lt;/p&gt;</content><author><name>Jiawei Lu</name><email>jl5999@columbia.edu</email></author><category term="studylog" /><category term="dl" /><summary type="html">Encoder-Decoder in NLP</summary></entry><entry><title type="html">CV II Lecture 06</title><link href="http://localhost:4000/studylog/CV_lecture06.html" rel="alternate" type="text/html" title="CV II Lecture 06" /><published>2022-03-04T00:00:00-05:00</published><updated>2022-03-04T00:00:00-05:00</updated><id>http://localhost:4000/studylog/CV_lecture06</id><content type="html" xml:base="http://localhost:4000/studylog/CV_lecture06.html">&lt;h2 id=&quot;behavior-continuum&quot;&gt;Behavior Continuum&lt;/h2&gt;

&lt;h2 id=&quot;representing-video&quot;&gt;Representing Video&lt;/h2&gt;

&lt;h2 id=&quot;3d-convolutional-networks&quot;&gt;3D convolutional Networks&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;2D convolutions vs. 3D convolutions&lt;/li&gt;
  &lt;li&gt;3D filters at the first layer.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-stream-network&quot;&gt;2-Stream Network&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Spatial Stream ConvNet&lt;/li&gt;
  &lt;li&gt;Tenporal Stream ConvNet&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;recurrent-neural-network&quot;&gt;Recurrent Neural Network&lt;/h2&gt;

&lt;h3 id=&quot;forward-function&quot;&gt;forward function&lt;/h3&gt;

&lt;p&gt;\begin{equation}
h_i = f(w_{hx}^T x_i + w_{hh}^Th_{i-1})
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
y_i = f(w_{yh}^T h_i)
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;vanishingexploding-gradient&quot;&gt;Vanishing/Exploding Gradient&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Forward pass:
  \begin{equation}
  z_i = f(w_x^T x_i + w^T z_{i-1})
  \end{equation}&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Gradients:
  \begin{equation}
  \frac{dL}{dw} = \frac{dL}{dz_{i+1}} \frac{dz_{i+1}}{dz_i} \frac{z_i}{dw}
  \end{equation}&lt;/p&gt;

    &lt;p&gt;\begin{equation}
  \frac{dL}{dw} = \frac{dL}{dz_T} (\prod_{j=i}^{T-1}\frac{dz_{j+1}}{dz_j}) \frac{dz_i}{dw}
  \end{equation}&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;long-short-term-memory-lstm&quot;&gt;Long Short-Term Memory (LSTM)&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;&gt;colah’s blog&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;future-generation&quot;&gt;Future Generation&lt;/h2&gt;
&lt;p&gt;\begin{equation}
x_t \rightarrow f(x_t; w) \rightarrow x_{t+1}
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;minimize-euclidean-distance&quot;&gt;Minimize Euclidean distance:&lt;/h3&gt;
&lt;p&gt;\begin{equation}
\min_w \sum_i || f(x_t^i;w) - x_{t+1}^i||_2^2
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;train&quot;&gt;Train&lt;/h3&gt;
&lt;p&gt;\begin{equation}
\min_f \sum_i ||f(x_t^i) - g(x_{t+1}^i)||_2^2
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
\min_{f,\delta} \sum_i \sum_k^K \delta_k^i \cdot ||f_k(x_t^i) - g(x_{t+1}^i)||_2^2
\end{equation}
s.t. \(\delta^i \in{0,1}^K\) and \(||\delta^i||_1 = 1 \ \forall_i\)&lt;/p&gt;

&lt;h3 id=&quot;expectation-maximization&quot;&gt;Expectation-Maximization&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;E Step: Fill in missing variables and estimate latent variables&lt;/li&gt;
  &lt;li&gt;M Step: Fit the model (with back-propagation in this case)&lt;/li&gt;
  &lt;li&gt;Repeat&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Jiawei Lu</name><email>jl5999@columbia.edu</email></author><category term="studylog" /><category term="cv" /><summary type="html">Behavior Continuum</summary></entry><entry><title type="html">Object Detection for Dummies Part 1: Gradient Vector, HOG, and SS</title><link href="http://localhost:4000/studylog/ODDP1.html" rel="alternate" type="text/html" title="Object Detection for Dummies Part 1: Gradient Vector, HOG, and SS" /><published>2022-02-09T00:00:00-05:00</published><updated>2022-02-09T00:00:00-05:00</updated><id>http://localhost:4000/studylog/ODDP1</id><content type="html" xml:base="http://localhost:4000/studylog/ODDP1.html">&lt;p&gt;Jan 28, 2021.
The raw blog &lt;a href=&quot;https://lilianweng.github.io/lil-log/2017/10/29/object-recognition-for-dummies-part-1.html&quot;&gt;URL&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;image-gradient-vector&quot;&gt;Image Gradient Vector&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Derivative
    &lt;ul&gt;
      &lt;li&gt;Scalar&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Directional Derivative
    &lt;ul&gt;
      &lt;li&gt;Scalar&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Gradient
    &lt;ul&gt;
      &lt;li&gt;Vector
        &lt;pre&gt;&lt;code class=&quot;language-python=&quot;&gt;import numpy as np
import scipy.signal as sig
data = np.array([[0, 105, 0], [40, 255, 90], [0, 55, 0]])
G_x = sig.convolve2d(data, np.array([[-1, 0, 1]]), mode=&apos;valid&apos;)
G_y = sig.convolve2d(data, np.array([[-1], [0], [1]]), mode=&apos;valid&apos;)
&lt;/code&gt;&lt;/pre&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Common Image Processing Kernels(&lt;a href=&quot;https://zhuanlan.zhihu.com/p/67197912&quot;&gt;Useful URL&lt;/a&gt;)
    &lt;ul&gt;
      &lt;li&gt;Prewitt operator&lt;/li&gt;
      &lt;li&gt;Sobel operator&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;histogram-of-oriented-gradients-hog&quot;&gt;Histogram of Oriented Gradients (HOG)&lt;/h2&gt;
&lt;p&gt;Useful &lt;a href=&quot;https://zhuanlan.zhihu.com/p/85829145&quot;&gt;URL&lt;/a&gt; in zhihu.&lt;/p&gt;

&lt;h3 id=&quot;how-hog-works&quot;&gt;How HOG works&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Preprocess the image, including resizing and color normalization.
    &lt;ul&gt;
      &lt;li&gt;Gamma Correction&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;\begin{equation}
f(x) = x^{\gamma}
\end{equation}&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python=&quot;&gt;# Gamma Correction
import cv2
import numpy as np
img = cv2.imread(&apos;gamma.jpg&apos;, 0)
img2 = np.power(img/float(np.max(img)), 1.5)
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
  &lt;li&gt;Compute the gradient vector of every pixel, as well as its magnitude and direction&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;\begin{equation}
g = \sqrt{g_x^2+g_y^2} \ 
\theta = \arctan \frac{g_y}{g_x}
\end{equation}&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python=&quot;&gt;import cv2
import numpy as np

# Read image
img = cv2.imread(&apos;runner.jpg&apos;)
img = np.float32(img) / 255.0  # 归一化

# x,y gradient
gx = cv2.Sobel(img, cv2.CV_32F, 1, 0, ksize=1)
gy = cv2.Sobel(img, cv2.CV_32F, 0, 1, ksize=1)

# gradient
mag, angle = cv2.cartToPolar(gx, gy, angleInDegrees=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Divide the image into many 8x8 pixel cells. In each cell, the magnitude values of these 64 cells are binned and cumulatively added into 9 buckets of unsigned direction (no sign, so 0-180 degree rather than 0-360 degree; this is a practical choice based on empirical experiments).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Then we slide a 2x2 cells (thus 16x16 pixels) block across the image. In each block region, 4 histograms of 4 cells are concatenated into one-dimensional vector of 36 values and then normalized to have an unit weight. The final HOG feature vector is the concatenation of all the block vectors. It can be fed into a classifier like SVM for learning object recognition tasks.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&quot;language-python=&quot;&gt;# HOG
from skimage import feature, exposure
import cv2
image = cv2.imread(&apos;/home/zxd/Pictures/Selection_018.jpg&apos;)
fd, hog_image = feature.hog(image, orientations=9, pixels_per_cell=(16, 16),
                    cells_per_block=(2, 2), visualize=True)

# Rescale histogram for better display
hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))

cv2.imshow(&apos;img&apos;, image)
cv2.imshow(&apos;hog&apos;, hog_image_rescaled)
cv2.waitKey(0)==ord(&apos;q&apos;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;image-segmentation-felzenszwalbs-algorithm&quot;&gt;Image Segmentation (Felzenszwalb’s Algorithm)&lt;/h2&gt;

&lt;h3 id=&quot;graph-construction&quot;&gt;Graph Construction&lt;/h3&gt;

&lt;p&gt;There are two approaches to constructing a graph out of an image.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Grid Graph: Each pixel is only connected with surrounding neighbours (8 other cells in total). The edge weight is the absolute difference between the intensity values of the pixels.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Nearest Neighbor Graph: Each pixel is a point in the feature space (x, y, r, g, b), in which (x, y) is the pixel location and (r, g, b) is the color values in RGB. The weight is the Euclidean distance between two pixels’ feature vectors.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>Jiawei Lu</name><email>jl5999@columbia.edu</email></author><category term="studylog" /><category term="dl" /><summary type="html">Jan 28, 2021. The raw blog URL.</summary></entry><entry><title type="html">NLP Lecture 05: Introduction to Syntax and Formal Languages</title><link href="http://localhost:4000/studylog/NLP_lecture05.html" rel="alternate" type="text/html" title="NLP Lecture 05: Introduction to Syntax and Formal Languages" /><published>2022-02-09T00:00:00-05:00</published><updated>2022-02-09T00:00:00-05:00</updated><id>http://localhost:4000/studylog/NLP_lecture05</id><content type="html" xml:base="http://localhost:4000/studylog/NLP_lecture05.html">&lt;p&gt;Keywords: Introduction to Syntax and Formal Languages.&lt;/p&gt;

&lt;h2 id=&quot;syntax&quot;&gt;Syntax&lt;/h2&gt;
&lt;h3 id=&quot;syntax-as-an-interface&quot;&gt;Syntax as an Interface&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Syntax can be seen as the interface between morphology (structure of words) and semantics.&lt;/li&gt;
  &lt;li&gt;Can judge if a sentence is grammatical or not, even if it doesn’t make sense semantically.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;key-concepts-of-syntax&quot;&gt;Key Concepts of Syntax&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Constituency and Recursion.&lt;/li&gt;
  &lt;li&gt;Dependency.&lt;/li&gt;
  &lt;li&gt;Grammatical Relations.&lt;/li&gt;
  &lt;li&gt;Subcategorization.&lt;/li&gt;
  &lt;li&gt;Long-distance dependencies&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;constituents&quot;&gt;Constituents&lt;/h2&gt;
&lt;p&gt;A constituent is a group of words that behave as a single unit (within a hierarchical structure).&lt;/p&gt;

&lt;h3 id=&quot;constituency&quot;&gt;Constituency&lt;/h3&gt;
&lt;p&gt;There is a great number of constituency tests. They typically involve moving constituents around or replacing them.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Topicalization&lt;/li&gt;
  &lt;li&gt;Pro-form Substitution&lt;/li&gt;
  &lt;li&gt;Wh-question test.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;constituent-labels&quot;&gt;Constituent Labels&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Choose constituents so each one has one non-bracketed word: the head.&lt;/li&gt;
  &lt;li&gt;Category of Constituent: XP, where X is the part-of-speech of the head: NP, VP, AdjP, AdvP, DetP&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;recursion-in-language&quot;&gt;Recursion in Language&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;One of the most important attributes of Natural Languages is that they are recursive.&lt;/li&gt;
  &lt;li&gt;There are infinitely many sentences in a language, but in predictable structures.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;context-free-grammars-cfg&quot;&gt;Context Free Grammars (CFG)&lt;/h2&gt;
&lt;p&gt;A context free grammar is defined by:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Set of &lt;strong&gt;terminal symbols&lt;/strong&gt; \(\Sigma\).&lt;/li&gt;
  &lt;li&gt;Set of &lt;strong&gt;non-terminal symbols&lt;/strong&gt; \(N\).&lt;/li&gt;
  &lt;li&gt;A &lt;strong&gt;start symbol&lt;/strong&gt; \(S \in N\).&lt;/li&gt;
  &lt;li&gt;Set \(R\) of &lt;strong&gt;productions&lt;/strong&gt; of the form \(A \rightarrow \beta\), where \(A \in N\) and \(\beta \in (\Sigma \cup N)^*\), i.e. \(\beta\) is a string of terminals and non-terminals.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;language-of-a-cfg&quot;&gt;Language of a CFG&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Given a CFG \(G=(N, \Sigma,R,S)\):
    &lt;ul&gt;
      &lt;li&gt;Given a string \(\alpha A \gamma\), where \(A \in N\), we can derive \(\alpha \beta \gamma\) if there is a production \(A \rightarrow \beta \in R\).&lt;/li&gt;
      &lt;li&gt;\(\alpha \implies \beta\) means that \(G\) can derive \(\beta\) from \(\alpha\) in a single step.&lt;/li&gt;
      &lt;li&gt;\(\alpha \implies \beta^*\) means that \(G\) can derive \(\beta\) from \(\alpha\) in a finite number of steps.&lt;/li&gt;
      &lt;li&gt;The language of G is defined as the set of all terminal strings that can be derived from the start symbol.&lt;/li&gt;
      &lt;li&gt;
\[L(G) = \{\beta \in \Sigma ^*, s.t. S\implies ^* \beta\}\]
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;derivations-and-derived-strings&quot;&gt;Derivations and Derived Strings&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;CFG is a string rewriting formalism, so the derived objects are strings.&lt;/li&gt;
  &lt;li&gt;A derivation is a sequence of rewriting steps.&lt;/li&gt;
  &lt;li&gt;CFGs are context free: applicability of a rule depends only on the nonterminal symbol, not on its context.&lt;/li&gt;
  &lt;li&gt;Therefore, the order in which multiple non-terminals in a partially derived string are replaced does not matter. We can represent identical derivations in a derivation tree.&lt;/li&gt;
  &lt;li&gt;The derivation tree implies a parse tree.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;regular-grammars&quot;&gt;Regular Grammars&lt;/h2&gt;
&lt;p&gt;A regular grammar is defined by:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Set of &lt;strong&gt;terminal symbols&lt;/strong&gt; \(\Sigma\).&lt;/li&gt;
  &lt;li&gt;Set of &lt;strong&gt;non-terminal symbols&lt;/strong&gt; \(N\).&lt;/li&gt;
  &lt;li&gt;A &lt;strong&gt;start symbol&lt;/strong&gt; \(S \in N\).&lt;/li&gt;
  &lt;li&gt;Set \(R\) of &lt;strong&gt;productions&lt;/strong&gt; of the form \(A \rightarrow aB\) or \(A \rightarrow a\), where \(A,B \in N\) and \(a \in \Sigma\).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;finite-state-automata&quot;&gt;Finite State Automata&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Regular grammars can be implemented as finite state automata.&lt;/li&gt;
  &lt;li&gt;The set of all regular languages is strictly smaller than the set of context-free languages.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;center-embeddings&quot;&gt;Center Embeddings&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Problem: Regular grammars cannot capture long-distance dependencies.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;formal-grammar-and-parsing&quot;&gt;Formal Grammar and Parsing&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Formal Grammars are used in linguistics, NLP, programming languages.&lt;/li&gt;
  &lt;li&gt;We want to build a compact model that describes a complete language.&lt;/li&gt;
  &lt;li&gt;Need efficient algorithms to determine if a sentence is in the language or not (recognition problem).&lt;/li&gt;
  &lt;li&gt;We also want to recover the structure imposed by the grammar (parsing problem).&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jiawei Lu</name><email>jl5999@columbia.edu</email></author><category term="studylog" /><category term="nlp" /><summary type="html">Keywords: Introduction to Syntax and Formal Languages.</summary></entry><entry><title type="html">Leetcode</title><link href="http://localhost:4000/studylog/leetcode.html" rel="alternate" type="text/html" title="Leetcode" /><published>2022-02-02T00:00:00-05:00</published><updated>2022-02-02T00:00:00-05:00</updated><id>http://localhost:4000/studylog/leetcode</id><content type="html" xml:base="http://localhost:4000/studylog/leetcode.html">&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Two sum&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Hashmap&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Add two numbers
    &lt;ul&gt;
      &lt;li&gt;Linked List&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Longest Substring Without Repeating Characters
    &lt;ul&gt;
      &lt;li&gt;Two pointers&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Median of Two Sorted Arrays
    &lt;ul&gt;
      &lt;li&gt;Median&lt;/li&gt;
      &lt;li&gt;Binary Search&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Longest Palindromic Substring
    &lt;ul&gt;
      &lt;li&gt;DP&lt;/li&gt;
      &lt;li&gt;Java vs Python&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Regular Expression Matching
    &lt;ul&gt;
      &lt;li&gt;Recursion&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Container With Most Water
    &lt;ul&gt;
      &lt;li&gt;Two pointers&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;3 Sum
    &lt;ul&gt;
      &lt;li&gt;2 sum&lt;/li&gt;
      &lt;li&gt;Hashmap&lt;/li&gt;
      &lt;li&gt;sort&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Letter Combinations of a Phone Number
    &lt;ul&gt;
      &lt;li&gt;Hashmap&lt;/li&gt;
      &lt;li&gt;Recursion&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Remove Nth Node From End of List
    &lt;ul&gt;
      &lt;li&gt;Symmetric&lt;/li&gt;
      &lt;li&gt;Two pointers&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Valid Parentheses
    &lt;ul&gt;
      &lt;li&gt;Stack&lt;/li&gt;
      &lt;li&gt;Hashmap for pair&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Generate Parentheses
    &lt;ul&gt;
      &lt;li&gt;DFS&lt;/li&gt;
      &lt;li&gt;Stack&lt;/li&gt;
      &lt;li&gt;DP&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Merge k Sorted Lists
    &lt;ul&gt;
      &lt;li&gt;Priority Queue
        &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;Queue&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PriorityQueue&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;Python &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sorted(list)&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Reverse Nodes in k-Group
    &lt;ul&gt;
      &lt;li&gt;Reverse list&lt;/li&gt;
      &lt;li&gt;6 pointers: dummy, jump, l, r, prev, cur&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Next Permutation
    &lt;ul&gt;
      &lt;li&gt;Find Pattern: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nums[i-1] &amp;lt; nums[i]&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Decreasing List&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Longest Valid Parentheses
    &lt;ul&gt;
      &lt;li&gt;Find Pattern: If ‘)’ more than ‘(‘, reset&lt;/li&gt;
      &lt;li&gt;Stack&lt;/li&gt;
      &lt;li&gt;Two traverse&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Search in Rotated Sorted Array
    &lt;ul&gt;
      &lt;li&gt;Binary Search&lt;/li&gt;
      &lt;li&gt;Mind &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;left &amp;lt;= mid&lt;/code&gt; since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mid = (left + right)//2&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Find First and Last Position of Element in Sorted Array
    &lt;ul&gt;
      &lt;li&gt;Binary Search&lt;/li&gt;
      &lt;li&gt;Find left most: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;left = mid + 1, right = mid&lt;/code&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;python = 
  while left &amp;lt; right:
      mid = (left + right) // 2
      if nums[mid] &amp;lt; target:
          left = mid + 1
      else:
          right = mid
 &lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Find right most: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;left = mid, right = mid - 1&lt;/code&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;python = 
  while left &amp;lt; right:
      mid = (left + right + 1) // 2
      if nums[mid] &amp;gt; target:
          right = mid - 1
      else:
          left = mid
 &lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Search Insert Position
    &lt;ul&gt;
      &lt;li&gt;Binary Search&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Combination Sum
    &lt;ul&gt;
      &lt;li&gt;Backtracking/DFS
        &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;dfs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;dfs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;First Missing Positive
    &lt;ul&gt;
      &lt;li&gt;Find Pattern: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[1,2,...,n+1]&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Hash &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nums[nums[i]%n] += n&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Trapping Rain Water
    &lt;ul&gt;
      &lt;li&gt;DP: store leftMax and rightMax&lt;/li&gt;
      &lt;li&gt;Two pointers&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Jump Game II
    &lt;ul&gt;
      &lt;li&gt;Two pointers: left for n steps, right for n+1 steps&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Permutations
    &lt;ul&gt;
      &lt;li&gt;DFS&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Rotate Image
    &lt;ul&gt;
      &lt;li&gt;List transportation with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zip()&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rotate = flip + trans&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Group Anagrams
    &lt;ul&gt;
      &lt;li&gt;permutations have the same characters: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sorted()&lt;/code&gt; + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dict&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Maximum Subarray
    &lt;ul&gt;
      &lt;li&gt;Divide and Conquer&lt;/li&gt;
      &lt;li&gt;DP&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Jump Game
    &lt;ul&gt;
      &lt;li&gt;two pointers&lt;/li&gt;
      &lt;li&gt;from end to start: where is the last reachable point&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Merge Intervals
    &lt;ul&gt;
      &lt;li&gt;Graph&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sort()&lt;/code&gt; + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max()&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Unique Paths
    &lt;ul&gt;
      &lt;li&gt;DP&lt;/li&gt;
      &lt;li&gt;Math $C_m^n$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Minimum Path Sum
    &lt;ul&gt;
      &lt;li&gt;DP&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Climbing Stairs
    &lt;ul&gt;
      &lt;li&gt;DP&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Edit Distance
    &lt;ul&gt;
      &lt;li&gt;DP&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Search a 2D Matrix
    &lt;ul&gt;
      &lt;li&gt;Binary Search: $m \times n$ sorted array&lt;/li&gt;
      &lt;li&gt;if element not in list, find the left boundary: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mid = (left + right)//2&lt;/code&gt;,  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;right -= 1&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Sort Colors
    &lt;ul&gt;
      &lt;li&gt;count sort&lt;/li&gt;
      &lt;li&gt;*Dutch National Flag Problem&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Minimum Window Substring
    &lt;ul&gt;
      &lt;li&gt;Sliding window, two pointers&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Subsets
    &lt;ul&gt;
      &lt;li&gt;Backtracking&lt;/li&gt;
      &lt;li&gt;Bitmask&lt;/li&gt;
      &lt;li&gt;DP&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Word Search
    &lt;ul&gt;
      &lt;li&gt;Backtracking / DFS&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;oa&quot;&gt;OA&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;Merge Sort: Counting Inversions&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&quot;language-python=&quot;&gt;#!/bin/python3

import math
import os
import random
import re
import sys

#
# Complete the &apos;countInversions&apos; function below.
#
# The function is expected to return a LONG_INTEGER.
# The function accepts INTEGER_ARRAY arr as parameter.
#

def countInversions(arr):
    # Write your code here
    temp_arr = [0]*len(arr)
    return mergeSort(arr, temp_arr, 0, len(arr) - 1)

def mergeSort(arr, temp_arr, left, right):
    inv_cnt = 0
    if left &amp;lt; right:
        mid = (left + right) // 2
        inv_cnt += mergeSort(arr, temp_arr, left, mid)
        inv_cnt += mergeSort(arr, temp_arr, mid + 1, right)
        inv_cnt += merge(arr, temp_arr, left, mid, right)
    return inv_cnt

def merge(arr, temp_arr, left, mid, right):
    i, j, k = left, mid+1, left
    inv_cnt = 0
    while i &amp;lt;= mid and j &amp;lt;= right:
        if arr[i] &amp;gt; arr[j]:
            inv_cnt += mid - i + 1
            temp_arr[k] = arr[j]
            j += 1

        else:
            temp_arr[k] = arr[i]
            i += 1
        k += 1
    
    while i &amp;lt;= mid:
        temp_arr[k] = arr[i]
        i += 1
        k += 1
    while j &amp;lt;= right:
        temp_arr[k] = arr[j]
        j += 1
        k += 1
    
    for x in range(left, right + 1):
        arr[x] = temp_arr[x]
    
    return inv_cnt
    

if __name__ == &apos;__main__&apos;:
    fptr = open(os.environ[&apos;OUTPUT_PATH&apos;], &apos;w&apos;)

    t = int(input().strip())

    for t_itr in range(t):
        n = int(input().strip())

        arr = list(map(int, input().rstrip().split()))

        result = countInversions(arr)

        fptr.write(str(result) + &apos;\n&apos;)

    fptr.close()

&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Stars and Bars:
```python=
def stars_and_bars(s, startIndex, endIndex):
 bars = {}&lt;/p&gt;

    &lt;p&gt;cnt = 0
 for i in range(len(s)):
     if s[i] == “|”:
         bars[i] = cnt
         cnt += 1
 lst = list(bars)
 startIndex, endIndex = [x-1 for x in startIndex], &lt;br /&gt;
                        [x-1 for x in endIndex]
 start_bar, end_bar = [], []&lt;/p&gt;

    &lt;p&gt;for i in range(len(startIndex)):
     start_bar.append(binary_start(lst, startIndex[i]))
 for i in range(len(endIndex)):
     end_bar.append(binary_end(lst, endIndex[i]))&lt;/p&gt;

    &lt;p&gt;print(bars)
 print(start_bar, end_bar)&lt;/p&gt;

    &lt;p&gt;res = 0
 for i in range(len(start_bar)):&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; res += max(0, (end_bar[i] - start_bar[i] - 1) - \
            (bars[end_bar[i]] - bars[start_bar[i]] - 1))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;return res&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;def binary_start(nums, target):
    left, right = 0, len(nums) -1&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;while left &amp;lt; right:
    mid = (left+right)//2
    if nums[mid] == target:
        return nums[mid]
    elif nums[mid] &amp;gt; target:
        right = mid
    else:
        left = mid + 1
return nums[right]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;def binary_end(nums, target):
    left, right = 0, len(nums) -1&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;while left &amp;lt; right:
    mid = (left+right+1)//2
    if nums[mid] == target:
        return nums[mid]
    elif nums[mid] &amp;gt; target:
        right = mid - 1
    else:
        left = mid  
return nums[left] ```
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Jiawei Lu</name><email>jl5999@columbia.edu</email></author><category term="studylog" /><category term="coding-interview" /><summary type="html">Two sum</summary></entry><entry><title type="html">NLP Lecture 04: Sequence Labeling with HMMs, P.O.S Tagging</title><link href="http://localhost:4000/studylog/NLP_lecture04.html" rel="alternate" type="text/html" title="NLP Lecture 04: Sequence Labeling with HMMs, P.O.S Tagging" /><published>2022-02-02T00:00:00-05:00</published><updated>2022-02-02T00:00:00-05:00</updated><id>http://localhost:4000/studylog/NLP_lecture04</id><content type="html" xml:base="http://localhost:4000/studylog/NLP_lecture04.html">&lt;p&gt;Keywords: Sequence Labeling with Hidden Markov Models, Part-of-Speech Tagging.&lt;/p&gt;

&lt;h2 id=&quot;garden-path-sentences&quot;&gt;Garden-Path Sentences&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;The horse raced past the barn.&lt;/li&gt;
  &lt;li&gt;The horse raced past the barn fell.&lt;/li&gt;
  &lt;li&gt;The old dog the footsteps of the young.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;parts-of-speech&quot;&gt;Parts-of-Speech&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Classes of words that behave alike:
    &lt;ul&gt;
      &lt;li&gt;Appear in similar contexts.&lt;/li&gt;
      &lt;li&gt;Perform a similar grammatical function in the sentence.&lt;/li&gt;
      &lt;li&gt;Undergo similar morphological transformations.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;~9 traditional parts-of-speech:
    &lt;ul&gt;
      &lt;li&gt;noun, pronoun, determiner, adjective, verb, adverb, preposition, conjunction, interjection&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;parts-of-speech-tagging&quot;&gt;Parts-of-Speech Tagging&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Goal: Translate from a sequence of words \((w_1, w_2, ..., w_n) \in V^*\), to a sequence of tags \((t_1, t_2, ..., t_n ) \in T^*\).&lt;/li&gt;
  &lt;li&gt;NLP is full of translation problems from one structure to another. Basic solution:
    &lt;ol&gt;
      &lt;li&gt;Construct search space of possible translations.&lt;/li&gt;
      &lt;li&gt;Find best paths through this space (decoding) according to some performance measure.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;bayesian-inference-for-sequence-labeling&quot;&gt;Bayesian Inference for Sequence Labeling&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Assume each word wi in the observed sequence \((w_1, w_2, ..., w_n) \in V^*\) was generated by some hidden variable \(t_i\).&lt;/li&gt;
  &lt;li&gt;Infer the most likely sequence of hidden variables given the sequence of observed words.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;markov-chains&quot;&gt;Markov Chains&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;A Markov chain is a sequence of random variables \(X_1, X_2, ...\)&lt;/li&gt;
  &lt;li&gt;The domain of these variables is a set of states.&lt;/li&gt;
  &lt;li&gt;Markov assumption: Next state depends only on current state.
\(P(X_{n+1}|X_1, X_2, ..., X_n) = P(X_{n+1}|X_n)\)&lt;/li&gt;
  &lt;li&gt;This is a special case of a weighted finite state automaton (WFSA).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hidden-markov-models-hmms&quot;&gt;Hidden Markov Models (HMMs)&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Generative (Bayesian) probability model.
  Observations: sequences of words.
  Hidden states: sequence of part-of-speech labels.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Hidden sequence is generated by an n-gram language model (typically a bi-gram model).
    &lt;ul&gt;
      &lt;li&gt;
\[t_0 = START\]
      &lt;/li&gt;
      &lt;li&gt;
\[P(t_1, t_2, ..., t_n) = \prod_{i=1}^nP(t_i|t_{i-1})\]
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;There are two types of probabilities:
  Transition Probabilities and Emission Probabilities.
  \begin{equation}
  P(t_1,t_2, …, t_n, w_1,w_2,…,w_n) = P(t_1|start)P(w_1|t_1)P(t_2|t_1)P(w_2|t_2) \cdots P(t_n|t_{n-1})P(w_n|t_n)
  \end{equation}
  \begin{equation}
  P(t_1,t_2, …, t_n, w_1,w_2,…,w_n) = \prod_{i=1}^nP(t_i|t_{i-1})P(w_i|t_i)
  \end{equation}&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;important-tasks-on-hmms&quot;&gt;Important Tasks on HMMs&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Decoding: 
  Given a sequence of words, find the most likely probability sequence. (Bayesian inference using Viterbi algorithm).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Evaluation: 
  Given a sequence of words, find the total probability for this word sequence given an HMM. Note that we can view the HMM as another type of language model. (Forward algorithm)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Training: 
  Estimate emission and transition probabilities from training data. (MLE, Forward-Backward a.k.a Baum-Welch algorithm)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;decoding-hmms&quot;&gt;Decoding HMMs&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Goal: Find the path with the highest total probability (given the words)
  \begin{equation}
  \arg \max_{t_1, …, t_n} \prod_{i=1}^n P(t_i|t_{i-1}) P(w_i|t_i)
  \end{equation}&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;viterbi-algorithm&quot;&gt;Viterbi Algorithm&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Input: Sequence of observed words \(w_1,..., w_n\).&lt;/li&gt;
  &lt;li&gt;Create a table \(\pi\), such that each entry \(\pi[k,t]\) contains the score of the highest-probability sequence ending in tag \(t\) at time \(k\).&lt;/li&gt;
  &lt;li&gt;initialize \(\pi[0,start]=1.0\) and \(\pi[0,t]=0.0\) for all tags \(t \in T\).&lt;/li&gt;
  &lt;li&gt;for \(k=1\) to \(n\):
    &lt;ul&gt;
      &lt;li&gt;for \(t \in T\):
        &lt;ul&gt;
          &lt;li&gt;
\[\pi[k,t] \leftarrow \max_s \pi[k-1, s]\cdot P(t|s) \cdot P(w_k|t)\]
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;return \(\max_s \pi[n,s]\).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;trigram-language-model&quot;&gt;Trigram Language Model&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Instead of using a unigram context, use a bigram context.
    &lt;ul&gt;
      &lt;li&gt;Think of this as having states that represent pairs of tags.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;So the HMM probability for a given tag and word sequence is:
  \begin{equation}
  \prod_{i=1}^nP(t_i|t_{i-2}t_{i-1})P(w_i|t_i)
  \end{equation}&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Need to handle data sparseness when estimating transition probabilities (for example using backoff or linear interpolation)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;more-pos-tagging-tricks&quot;&gt;More POS tagging tricks&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;It is also often useful in practice to add an end-of-sentence marker (just like we did for n-gram language models).
  \begin{equation}
  P(t_1,…,t_n,w_1,…,w_n) = [\prod_{i=1}^nP(t_i|t_{i-2}t_{i-1})P(w_i|t_i)]P(t_{n+1}|t_n)
  \end{equation}
  where \({t_{-1} = t_{0} = START}\) and \(t_{n+1} = STOP\).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Another useful trick is to replace words with “pseudo words” representing an entire class.&lt;/li&gt;
  &lt;li&gt;Using a smoothed trigram HMM model with these tricks, we can build a tagger that is close to the state-of-the art (~97% accuracy on the Penn Treebank).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hmms-as-language-models&quot;&gt;HMMs as Language Models&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;We can also use an HMM as language models (language generation, MT, …), i.e. evaluate \(P(w_1,...,w_n)\) for a given sentence. What is the advantage over a plain word n-gram model?&lt;/li&gt;
  &lt;li&gt;Problem: There are many tag-sequences that could have generated \(w_1, ..., w_n\). 
  \begin{equation}
  P(w_1,…,w_n,t_1,…,t_n) = \prod_{i=1}^nP(t_i|t_{i-1})P(w_i|t_i)
  \end{equation}&lt;/li&gt;
  &lt;li&gt;This is an example of spurious ambiguity.&lt;/li&gt;
  &lt;li&gt;Need to compute: 
  \begin{equation}
  P(w_1,…,w_n) = \sum_{t_1,…,t_n}P(w_1,…,w_n,t_1,…,t_n) = \sum_{t_1,…,t_n} [\prod_{i=1}^nP(t_i|t_{i-1})P(w_i|t_i)]
  \end{equation}&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;forward-algorithm&quot;&gt;Forward Algorithm&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Input: Sequence of observed words \(w_1,..., w_n\).&lt;/li&gt;
  &lt;li&gt;Create a table \(\pi\), such that each entry \(\pi[k,t]\) contains the score of the highest-probability sequence ending in tag \(t\) at time \(k\).&lt;/li&gt;
  &lt;li&gt;initialize \(\pi[0,start]=1.0\) and \(\pi[0,t]=0.0\) for all tags \(t \in T\).&lt;/li&gt;
  &lt;li&gt;for \(k=1\) to \(n\):
    &lt;ul&gt;
      &lt;li&gt;for \(t \in T\):
        &lt;ul&gt;
          &lt;li&gt;
\[\pi[k,t] \leftarrow \sum_s \pi[k-1, s]\cdot P(t|s) \cdot P(w_k|t)\]
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;return \(\sum_s \pi[n,s]\).&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jiawei Lu</name><email>jl5999@columbia.edu</email></author><category term="studylog" /><category term="nlp" /><summary type="html">Keywords: Sequence Labeling with Hidden Markov Models, Part-of-Speech Tagging.</summary></entry><entry><title type="html">NLP Lecture 02: Naive Bayes’ Classifier</title><link href="http://localhost:4000/studylog/NLP_lecture02.html" rel="alternate" type="text/html" title="NLP Lecture 02: Naive Bayes’ Classifier" /><published>2022-01-26T00:00:00-05:00</published><updated>2022-01-26T00:00:00-05:00</updated><id>http://localhost:4000/studylog/NLP_lecture02</id><content type="html" xml:base="http://localhost:4000/studylog/NLP_lecture02.html">&lt;p&gt;Keywords: Language Classification, Probability Review, Machine Learning Background, Naive Bayes’ Classifier.&lt;/p&gt;

&lt;h2 id=&quot;linguistic-terminology&quot;&gt;Linguistic Terminology&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Sentence: Unit of written language.&lt;/li&gt;
  &lt;li&gt;Utterance: Unit of spoken language.&lt;/li&gt;
  &lt;li&gt;Word Form: the inflected form as it actually appears in the corpus. “produced”&lt;/li&gt;
  &lt;li&gt;Word Stem: The part of the word that never changes between morphological variations. “produc”&lt;/li&gt;
  &lt;li&gt;Lemma: an abstract base form, shared by word forms, having the same stem, part of speech, and word sense – stands for the class of words with stem. “produce”&lt;/li&gt;
  &lt;li&gt;Type: number of distinct words in a corpus (vocabulary size).&lt;/li&gt;
  &lt;li&gt;Token: Total number of word occurrences.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tokenization&quot;&gt;Tokenization&lt;/h2&gt;
&lt;p&gt;The process of segmenting text (a sequence of characters) into a sequence of tokens (words).&lt;/p&gt;

&lt;h2 id=&quot;lemmatization&quot;&gt;Lemmatization&lt;/h2&gt;
&lt;p&gt;Converting Lemmas into their base form.&lt;/p&gt;

&lt;h2 id=&quot;probabilities-in-nlp&quot;&gt;Probabilities in NLP&lt;/h2&gt;
&lt;p&gt;Probabilities make it possible to combine evidence from multiple sources systematically to (using Bayesian statistics).&lt;/p&gt;

&lt;h3 id=&quot;bayesian-statistics&quot;&gt;Bayesian Statistics&lt;/h3&gt;

&lt;p&gt;Typically, we observe some evidence (for example, words in a document) and the goal is to infer the “correct” interpretation (for example, the topic of a text). Probabilities express the degree of belief we have in the possible interpretations.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Prior probabilities: Probability of an interpretation prior to seeing any evidence.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Conditional (Posterior) probability: Probability of an interpretation after taking evidence into account.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;probability-basics&quot;&gt;Probability Basics&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;sample space \(\Omega\)&lt;/li&gt;
  &lt;li&gt;random variable \(X\)&lt;/li&gt;
  &lt;li&gt;probability distribution \(P(\omega)\)&lt;/li&gt;
  &lt;li&gt;joint probability: \(P(A, B)\)&lt;/li&gt;
  &lt;li&gt;conditional probability: 
\(P(A|B) = \frac{P(A,B)}{P(B)}\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;bayes-rule&quot;&gt;Bayes’ Rule&lt;/h3&gt;
&lt;p&gt;\begin{equation}
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;independence&quot;&gt;Independence&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Independent:
    &lt;ul&gt;
      &lt;li&gt;
\[P(A) = P(A|B)\]
      &lt;/li&gt;
      &lt;li&gt;
\[P(A, B) = P(A) \cdot P(B)\]
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Conditionally Independent
    &lt;ul&gt;
      &lt;li&gt;
\[P(B, C|A) = P(B|A) \cdot P(C|A)\]
      &lt;/li&gt;
      &lt;li&gt;
\[P(B|A,C) =P(B|A)\]
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;probabilities-and-supervised-learning&quot;&gt;Probabilities and Supervised Learning&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Given: Training data consisting of training examples \(data = (x_1, y_1), …, (x_n, y_n)\).&lt;/li&gt;
  &lt;li&gt;Goal: Learn a mapping \(h\) from \(x\) to \(y\).&lt;/li&gt;
  &lt;li&gt;Two approaches:
    &lt;ul&gt;
      &lt;li&gt;Discriminative algorithms learn 
  \(P(y | x)\) 
  directly.&lt;/li&gt;
      &lt;li&gt;Generative algorithms use Bayes rule
  \begin{equation}
  P(y|x) = \frac{P(x|y) \cdot P(y)}{P(x)}
  \end{equation}&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;discriminative-algorithms&quot;&gt;Discriminative Algorithms&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Model conditional distribution of the label given the data&lt;/li&gt;
  &lt;li&gt;Learns decision boundaries that separate instances of the different classes.&lt;/li&gt;
  &lt;li&gt;To predict a new example, check on which side of the decision boundary it falls.&lt;/li&gt;
  &lt;li&gt;Examples:
    &lt;ul&gt;
      &lt;li&gt;support vector machine (SVM)&lt;/li&gt;
      &lt;li&gt;decision trees&lt;/li&gt;
      &lt;li&gt;random forests&lt;/li&gt;
      &lt;li&gt;neural networks&lt;/li&gt;
      &lt;li&gt;log-linear models&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;generative-algorithms&quot;&gt;Generative Algorithms&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Assume the observed data is being “generated” by a “hidden” class label.&lt;/li&gt;
  &lt;li&gt;Build a different model for each class.&lt;/li&gt;
  &lt;li&gt;To predict a new example, check it under each of the models and see which one matches best.&lt;/li&gt;
  &lt;li&gt;Estimate \(P(x|y)\) and \(P(y)\). Then use bases rule
  \begin{equation}
  P(y|x) = \frac{P(x|y) \cdot P(y)}{P(x)}
  \end{equation}&lt;/li&gt;
  &lt;li&gt;Examples:
    &lt;ul&gt;
      &lt;li&gt;Naive Bayes&lt;/li&gt;
      &lt;li&gt;Hidden Markov Models&lt;/li&gt;
      &lt;li&gt;Gaussian Mixture Models&lt;/li&gt;
      &lt;li&gt;PCFGs&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;naive-bayes&quot;&gt;Naive Bayes&lt;/h2&gt;

&lt;h3 id=&quot;rules&quot;&gt;Rules&lt;/h3&gt;
&lt;p&gt;\begin{equation}
P(Label, X_1, …, X_d) = P(Label) \Pi_i P(X_i|Label)
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
\begin{split}
    P(Label|X_1, …, X_d) &amp;amp; = \frac{P(Label) \Pi_i P(X_i|Label)}{\Pi_i P(X_i)} &lt;br /&gt;
    &amp;amp; = \alpha [P(Label) \Pi_i P(X_i|Label)]
\end{split}
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;naive-bayes-classifier&quot;&gt;Naive Bayes Classifier&lt;/h3&gt;
&lt;p&gt;\begin{equation}
y* = \arg \max_y P(y) \Pi_i P(x_i|y)
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;training-the-naive-bayes-classifier&quot;&gt;Training the Naive Bayes’ Classifier&lt;/h3&gt;
&lt;p&gt;Estimate the prior and posterior probabilities using Maximum Likelihood Estimates (MLE)&lt;/p&gt;

&lt;p&gt;\begin{equation}
P(y) = \frac{Count(y)}{\sum_{y’\in Y}Count(y’)}
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
P(x_i|y) = \frac{Count(x_i, y)}{Count(y)}
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;some-issues-to-consider&quot;&gt;Some Issues to Consider&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;What if there are words that do not appear in the training set? What if it appears only once?&lt;/li&gt;
  &lt;li&gt;What if the plural of a word never appears in the training set?&lt;/li&gt;
  &lt;li&gt;How are extremely common words (e.g., “the”, “a”) handled?&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jiawei Lu</name><email>jl5999@columbia.edu</email></author><category term="studylog" /><category term="nlp" /><summary type="html">Keywords: Language Classification, Probability Review, Machine Learning Background, Naive Bayes’ Classifier.</summary></entry><entry><title type="html">NLP Lecture 03: n-gram language models</title><link href="http://localhost:4000/studylog/NLP_lecture03.html" rel="alternate" type="text/html" title="NLP Lecture 03: n-gram language models" /><published>2022-01-26T00:00:00-05:00</published><updated>2022-01-26T00:00:00-05:00</updated><id>http://localhost:4000/studylog/NLP_lecture03</id><content type="html" xml:base="http://localhost:4000/studylog/NLP_lecture03.html">&lt;p&gt;Keywords: n-gram language models&lt;/p&gt;

&lt;h2 id=&quot;language-modeling&quot;&gt;Language Modeling&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Task: predict the next word given the context.&lt;/li&gt;
  &lt;li&gt;Used in speech recognition, handwritten character recognition, spelling correction, text entry UI, machine translation,…&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;probability-of-the-next-word&quot;&gt;Probability of the Next Word&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Idea: We do not need to model domain, syntactic, and lexical knowledge perfectly.&lt;/li&gt;
  &lt;li&gt;Instead, we can rely on the notion of probability of a sequence (letters, words…).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;markov-assumption&quot;&gt;Markov Assumption&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;\(P(w_n|w_1, w_2, ..., w_{n-1})\)
  is difficult to estimate.&lt;/li&gt;
  &lt;li&gt;The longer the sequence becomes, the less likely \(w_1 w_2 w_3 ... w_{n-1}\) will appear in training data.&lt;/li&gt;
  &lt;li&gt;Instead, we make the following simple independence assumption (Markov assumption): The probability to see wn depends only on the previous \(k-1\) words.
  \begin{equation}
  P(w_n|w_1, w_2, w_3, …, w_{n-1}) \approx P(w_n|w_{n-k+1}, …, w_{n-1})
  \end{equation}&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;n-grams&quot;&gt;n-grams&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;The sequence \(w_n\) is a unigram.&lt;/li&gt;
  &lt;li&gt;The sequence \(w_{n-1}, w_n\) is a bigram.&lt;/li&gt;
  &lt;li&gt;The sequence \(w_{n-2}, w_{n-1}, w_n\) is a trigram.&lt;/li&gt;
  &lt;li&gt;The sequence \(w_{n-3}, w_{n-2}, w_{n-1}, w_n\) is a quadrigram…&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;bi-gram-language-model&quot;&gt;Bi-gram Language Model&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Using the Markov assumption and the chain rule:
  \begin{equation}
  P(w_1, w_2, w_3, …, w_n) \approx P(w_1) \cdot P(w_2|w_1) \cdot P(w_3|w_2) \cdots P(w_n|w_{n-1})
  \end{equation}&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;More consistent to use only bigrams:
  \begin{equation}
  P(w_1|start) \cdot P(w_2|w_1) \cdot P(w_3|w_2) \cdots P(w_n|w_{n-1}) \cdot P(end|w_n)
  \end{equation}&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;variable-length-language-models&quot;&gt;Variable-Length Language Models&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;We typically don’t know what the length of the sentence is.&lt;/li&gt;
  &lt;li&gt;Instead, we use a special marker END/STOP that indicates the end of a sentence.&lt;/li&gt;
  &lt;li&gt;We typically just augment the sentence with START and END/STOP markers to provide the appropriate context. 
(START i want to eat Chinese food END)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;log-probabilities&quot;&gt;Log Probabilities&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Probabilities can become very small (a few orders of magnitude per token).&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We often work with log probabilities in practice.
  \begin{equation}
  p(w_1…w_n) = \Pi_{i=1}^np(w_i|w_{i-1})
  \end{equation}&lt;/p&gt;

    &lt;p&gt;\begin{equation}
  \log p(w_1…w_n) = \sum_{i=1}^n \log p(w_i|w_{i-1})
  \end{equation}&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;estimating-n-gram-probabilities&quot;&gt;Estimating n-gram Probabilities&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;We can estimate n-gram probabilities using maximum likelihood estimates.
  \begin{equation}
  p(w|u) = \frac{count(u,w)}{count(u)}
  \end{equation}&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Or for trigrams:
  \begin{equation}
  p(w|u, v) = \frac{count(w,u,w)}{count(u, v)}
  \end{equation}&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;unseen-tokens&quot;&gt;Unseen Tokens&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Typical approach to unseen tokens:
    &lt;ul&gt;
      &lt;li&gt;Start with a specific lexicon of known tokens.&lt;/li&gt;
      &lt;li&gt;Replace all tokens in the training and testing corpus that are not in the lexicon with an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UNK&lt;/code&gt; token.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Practical approach:
    &lt;ul&gt;
      &lt;li&gt;Lexicon contains all words that appear more than &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k&lt;/code&gt; times in the training corpus.&lt;/li&gt;
      &lt;li&gt;Replace all other tokens with UNK.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;unseen-contexts&quot;&gt;Unseen Contexts&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Two basic approaches:
    &lt;ul&gt;
      &lt;li&gt;Smoothing / Discounting: Move some probability mass from seen trigrams to unseen trigrams.&lt;/li&gt;
      &lt;li&gt;Back-off: Use n-1-…, n-2-… grams to compute n-gram probability.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Other techniques:
    &lt;ul&gt;
      &lt;li&gt;Class-based backoff, use back-off probability for a specific word class / part-of-speech.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;zipfs-law&quot;&gt;Zipf’s Law&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Problem: n-grams (and most other linguistic phenomena) follow a Zipfian distribution.&lt;/li&gt;
  &lt;li&gt;A few words occur very frequently.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Most words occur very rarely. Many are seen only once.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Zipf’s law: a word’s frequency is approximately inversely proportional to its rank in the word distribution list.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;smoothing&quot;&gt;Smoothing&lt;/h2&gt;
&lt;p&gt;Smoothing flattens spiky distributions.&lt;/p&gt;

&lt;h3 id=&quot;additive-smoothing&quot;&gt;Additive Smoothing&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Classic approach: Laplacian, a.k.a. additive smoothing.&lt;/p&gt;

    &lt;p&gt;\begin{equation}
  P(w_i) = \frac{count(w_i) + 1}{N+V}
  \end{equation}&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;N is the number of tokens&lt;/li&gt;
      &lt;li&gt;V is the number of types (i.e. size of the vocabulary)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;\begin{equation}
  P(w|u) = \frac{count(u, w) + 1}{count(u) + V}
  \end{equation}&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Inaccurate in practice.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;linear-interpolation&quot;&gt;Linear Interpolation&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Use denser distributions of shorter ngrams to “fill in” sparse ngram distributions.&lt;/p&gt;

    &lt;p&gt;\begin{equation}
  P(w|u, v) = \lambda_1 \cdot p_{mle}(w|u,v) + \lambda_2 \cdot p_{mle}(w|v) + \lambda_3 \cdot p_{mle}(w)
  \end{equation}&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Where, \(\lambda_1, \lambda_2, \lambda_3 &amp;gt; 0\) and \(\lambda_1 + \lambda_2 + \lambda_3 = 1\).&lt;/li&gt;
  &lt;li&gt;Works well in practice (but not a lot of theoretical justification why).&lt;/li&gt;
  &lt;li&gt;Parameters can be estimated on development data (for example, using Expectation Maximization).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;discounting&quot;&gt;Discounting&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Idea: set aside some probability mass, then fill in the missing mass using back-off.&lt;/li&gt;
  &lt;li&gt;\(count^*(v, w) = count(v, w) - \beta\) where \(0&amp;lt;\beta&amp;lt;1\).&lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;Then for all seen bigrams: $$ p(w&lt;/td&gt;
          &lt;td&gt;v) = \frac{count^*(v, w)}{count(v)}.&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;For each context v the missing probability mass is
  \begin{equation}
  \alpha(v) = 1 - \sum_{w:c(v,w)&amp;gt;0} \frac{count^*(v, w)}{count(v)}
  \end{equation}&lt;/li&gt;
  &lt;li&gt;We can now divide this held-out mass between the unseen words (evenly or using back-off).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;katz-backoff&quot;&gt;Katz’ Backoff&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Divide the held-out probability mass proportionally to the unigram probability of the unseen words in context v.&lt;/p&gt;

    &lt;p&gt;\begin{equation}
  p(w|v) = 
  \begin{cases}
  &amp;amp; \frac{count^*(v, w)}{count(v)} &amp;amp; if count(v,w) &amp;gt; 0, &lt;br /&gt;
  &amp;amp; \alpha(v) \times \frac{p_{mle}(w)}{\sum_{u:count(v,u) = 0}p_{mle(u)}} &amp;amp; otherwise.
  \end{cases}
  \end{equation}&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;evaluating-n-gram-models&quot;&gt;Evaluating n-gram models&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Extrinsic evaluation: Apply the model in an application (for example language classification). Evaluate the application.&lt;/li&gt;
  &lt;li&gt;Intrinsic evaluation: measure how well the model approximates unseen language data.
    &lt;ul&gt;
      &lt;li&gt;Can compute the probability of each sentence according to the model. Higher probability -&amp;gt; better model.&lt;/li&gt;
      &lt;li&gt;Typically we compute Perplexity instead.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;perplexity&quot;&gt;Perplexity&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Perplexity (per word) measures how well the ngram model predicts the sample.&lt;/li&gt;
  &lt;li&gt;Given a corpus of ‘m’ sentences ‘\(s_i\)’, where ‘M’ is total number of tokens in the corpus&lt;/li&gt;
  &lt;li&gt;Perplexity is defined as \(2^{-l}\), where \(l = \frac{1}{M} \sum_{i=1}^m \log_2 p(s_i)\).&lt;/li&gt;
  &lt;li&gt;Lower perplexity = better model. Intuition:
    &lt;ul&gt;
      &lt;li&gt;Assume we are predicting one word at a time.&lt;/li&gt;
      &lt;li&gt;With uniform distribution, all successor words are equally likely. Perplexity is equal to vocabulary size.
• Perplexity can be thought of as “effective vocabulary size”.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jiawei Lu</name><email>jl5999@columbia.edu</email></author><category term="studylog" /><category term="nlp" /><summary type="html">Keywords: n-gram language models</summary></entry></feed>