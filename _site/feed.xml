<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="zh, en"><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="zh, en" /><updated>2022-05-20T11:17:55-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Jiawei Lu</title><subtitle>Hello World!  I&apos;m Jiawei Lu, a Master Student at Columbia University.  My research interest is Computer Vision, Deep Learning and Reinforcement Learning.
</subtitle><author><name>Jiawei Lu</name><email>jl5999@columbia.edu</email></author><entry><title type="html">[ZK Lab] #2: Platform - Hugging Face</title><link href="http://localhost:4000/studylog/platform_hugging_face.html" rel="alternate" type="text/html" title="[ZK Lab] #2: Platform - Hugging Face" /><published>2022-05-20T00:00:00-04:00</published><updated>2022-05-20T00:00:00-04:00</updated><id>http://localhost:4000/studylog/platform_hugging_face</id><content type="html" xml:base="http://localhost:4000/studylog/platform_hugging_face.html">&lt;h2 id=&quot;hugging-face&quot;&gt;&lt;a href=&quot;https://huggingface.co/&quot;&gt;Hugging Face&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&quot;goal&quot;&gt;Goal&lt;/h3&gt;
&lt;p&gt;Hugging Face Hub is building the largest collection of models, datasets and metrics in order to democratize and advance AI for everyone.&lt;/p&gt;

&lt;h3 id=&quot;repository&quot;&gt;Repository&lt;/h3&gt;
&lt;p&gt;The Hugging Face Hub hosts Git-based repositories which are storage spaces that can contain all your files.&lt;/p&gt;

&lt;p&gt;The Hub currently hosts three different repo types:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;models&lt;/li&gt;
  &lt;li&gt;datasets&lt;/li&gt;
  &lt;li&gt;Spaces, which are ML demo apps&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These repositories have multiple advantages over other hosting solutions:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;versioning&lt;/li&gt;
  &lt;li&gt;commit history and diffs&lt;/li&gt;
  &lt;li&gt;branches&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;On top of that, Hugging Face Hub repositories have many other advantages, for instance for models:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Model repos provide useful metadata about their tasks, languages, metrics, etc.&lt;/li&gt;
  &lt;li&gt;Anyone can play with the model directly in the browser!&lt;/li&gt;
  &lt;li&gt;Training metrics charts are displayed if the repository contains TensorBoard traces.&lt;/li&gt;
  &lt;li&gt;An API is provided to use the models in production settings.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/docs/hub/libraries&quot;&gt;Over 10 frameworks&lt;/a&gt; such as Transformers, Asteroid and ESPnet support using models from the Hugging Face Hub.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;widget&quot;&gt;widget&lt;/h3&gt;
&lt;p&gt;Many model repos have a widget that allows anyone to do inference directly in the browser.&lt;/p&gt;

&lt;p&gt;Some examples:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/spacy/en_core_web_sm?text=My+name+is+Sarah+and+I+live+in+London&quot;&gt;Named Entity Recognition&lt;/a&gt; using spaCy[https://spacy.io/].&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/google/vit-base-patch16-224&quot;&gt;Image Classification&lt;/a&gt; using &lt;a href=&quot;https://github.com/huggingface/transformers&quot;&gt;Transformers&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/julien-c/ljspeech_tts_train_tacotron2_raw_phn_tacotron_g2p_en_no_space_train&quot;&gt;Text to Speech&lt;/a&gt; using &lt;a href=&quot;https://github.com/espnet/espnet&quot;&gt;ESPnet&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/osanseviero/full-sentence-distillroberta3&quot;&gt;Sentence Similarity&lt;/a&gt; using &lt;a href=&quot;https://github.com/UKPLab/sentence-transformers&quot;&gt;Sentence Transformers&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can try out all the widgets &lt;a href=&quot;https://huggingface-widgets.netlify.app/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;inference-api&quot;&gt;Inference API&lt;/h3&gt;
&lt;p&gt;The Inference API allows you to send HTTP requests to models in the Hugging Face Hub. The Inference API is 2x to 10x faster than the widgets.&lt;/p&gt;

&lt;h3 id=&quot;explore-hugging-face-hub&quot;&gt;Explore Hugging Face Hub&lt;/h3&gt;

&lt;iframe width=&quot;735&quot; height=&quot;414&quot; src=&quot;https://www.youtube.com/embed/XvSGPZFEjDY&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h3 id=&quot;models-type-of-inference-api-and-widget&quot;&gt;Model’s Type of Inference API and Widget&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pipeline_tag&lt;/code&gt;: determine which pipeline and widget to display&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;config.json&lt;/code&gt;: transformers&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;loadpush-fromto-the-hub&quot;&gt;Load/Push from/to the Hub&lt;/h3&gt;

&lt;iframe width=&quot;735&quot; height=&quot;414&quot; src=&quot;https://www.youtube.com/embed/rkCly_cbMBk&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;</content><author><name>Jiawei Lu</name><email>jl5999@columbia.edu</email></author><category term="studylog" /><category term="zklab" /><summary type="html">Hugging Face</summary></entry><entry><title type="html">[ZK Lab] #3: Platform - Weights &amp;amp; Biases</title><link href="http://localhost:4000/studylog/platform_weights_and_biases.html" rel="alternate" type="text/html" title="[ZK Lab] #3: Platform - Weights &amp;amp; Biases" /><published>2022-05-20T00:00:00-04:00</published><updated>2022-05-20T00:00:00-04:00</updated><id>http://localhost:4000/studylog/platform_weights_and_biases</id><content type="html" xml:base="http://localhost:4000/studylog/platform_weights_and_biases.html">&lt;p&gt;Weights &amp;amp; Biases is the machine learning platform for developers to build better models faster.&lt;/p&gt;

&lt;p&gt;Use W&amp;amp;B’s lightweight, interoperable tools to&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;track experiments and version quickly&lt;/li&gt;
  &lt;li&gt;iterate on datasets&lt;/li&gt;
  &lt;li&gt;evaluate model performance&lt;/li&gt;
  &lt;li&gt;reproduce models&lt;/li&gt;
  &lt;li&gt;visualize results&lt;/li&gt;
  &lt;li&gt;spot regressions&lt;/li&gt;
  &lt;li&gt;share findings with colleagues&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiment-tracking&quot;&gt;Experiment Tracking&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wandb&lt;/code&gt; Python library to track machine learning experiments with a few lines of code.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Have lightweight &lt;a href=&quot;https://docs.wandb.ai/guides/integrations&quot;&gt;integrations&lt;/a&gt; for PyTorch and Keras.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;integrations&quot;&gt;Integrations&lt;/h2&gt;
&lt;p&gt;Weights &amp;amp; Biases integrations make it fast and easy to set up experiment tracking and data versioning inside existing projects.&lt;/p&gt;

&lt;p&gt;Useful Links:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/wandb/examples&quot;&gt;Examples&lt;/a&gt;: GitHub repo with working, end-to-end code examples for all of our integrations&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/wandb/examples/tree/master/colabs&quot;&gt;Colab&lt;/a&gt;: Try out W&amp;amp;B inside different frameworks, such as PyTorch Lightning, in an interactive notebook – no installation required&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PLD80i8An1OEGajeVo15ohAQYF1Ttle0lk&quot;&gt;Video Tutorials&lt;/a&gt;: Learn to use W&amp;amp;B with YouTube videos for PyTorch, Keras, and more.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;collaborative-reports&quot;&gt;Collaborative Reports&lt;/h2&gt;
&lt;p&gt;Reports let you organize and embed visualizations, describe your findings, share updates with collaborators, and more.&lt;/p&gt;

&lt;iframe width=&quot;735&quot; height=&quot;414&quot; src=&quot;https://www.youtube.com/embed/2xeJIv_K_eI&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h3 id=&quot;typical-use-cases-for-reports&quot;&gt;Typical use cases for reports&lt;/h3&gt;

&lt;h4 id=&quot;notes-add-a-visualization-with-a-quick-summary&quot;&gt;Notes: Add a visualization with a quick summary&lt;/h4&gt;

&lt;p&gt;Capture an important observation, an idea for future work, or a milestone reached in the development of a project. All experiment runs in your report will link to their parameters, metrics, logs, and code, so you can save the full context of your work.&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Labeling and Crowdsourcing&quot; src=&quot;../../../assets/img/blog/2022-05-20/wnb_1.png&quot; width=&quot;550&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;collaboration-share-findings-with-your-colleagues&quot;&gt;Collaboration: Share findings with your colleagues&lt;/h4&gt;

&lt;p&gt;Explain how to get started with a project, share what you’ve observed so far, and synthesize the latest findings. Your colleagues can make suggestions or discuss details using comments on any panel or at the end of the report.&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Labeling and Crowdsourcing&quot; src=&quot;../../../assets/img/blog/2022-05-20/wnb_2.png&quot; width=&quot;550&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;work-log-track-what-youve-tried-and-plan-next-steps&quot;&gt;Work log: Track what you’ve tried and plan next steps&lt;/h4&gt;

&lt;p&gt;Write down your thoughts on experiments, your findings, and any gotchas and next steps as you work through a project, keeping everything organized in one place. This lets you “document” all the important pieces beyond your scripts.&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Labeling and Crowdsourcing&quot; src=&quot;../../../assets/img/blog/2022-05-20/wnb_3.png&quot; width=&quot;550&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;data--model-versioning&quot;&gt;Data + Model Versioning&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://wandb.me/artifacts-quickstart&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Use W&amp;amp;B Artifacts for dataset versioning, model versioning, and tracking dependencies and results across machine learning pipelines. Think of an artifact as a versioned folder of data. You can store entire datasets directly in artifacts, or use artifact references to point to data in other systems like S3, GCP, or your own system.&lt;/p&gt;

&lt;h2 id=&quot;data-visualization&quot;&gt;Data Visualization&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://wandb.me/tables-colab&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Use W&amp;amp;B Tables to log, query, and analyze tabular data. Understand your datasets, visualize model predictions, and share insights in a central dashboard.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Compare changes precisely across models, epochs, or individual examples&lt;/li&gt;
  &lt;li&gt;Understand higher-level patterns in your data&lt;/li&gt;
  &lt;li&gt;Capture and communicate your insights with visual samples&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hyperparameter-tuning&quot;&gt;Hyperparameter Tuning&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://wandb.me/sweeps-colab&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Use Weights &amp;amp; Biases Sweeps to automate hyperparameter optimization and explore the space of possible models.&lt;/p&gt;

&lt;iframe width=&quot;735&quot; height=&quot;414&quot; src=&quot;https://www.youtube.com/embed/9zrmUIlScdY&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;model-management&quot;&gt;Model Management&lt;/h2&gt;
&lt;p&gt;A Model Registry is a system of record for organizing ML Models - often serving as an interface between model producers and consumers. This guide will show you how to use W&amp;amp;B as a Model Registry to track and report on the complete workflow of developing a model:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Model Catalog &amp;amp; Versioning: Save and restore every version of your model &amp;amp; learned parameters - organize versions by use case and objective.&lt;/li&gt;
  &lt;li&gt;Model Metadata: Track training metrics, assign custom metadata, and document rich markdown descriptions of your models.&lt;/li&gt;
  &lt;li&gt;Model Lineage Tracking &amp;amp; Reproducibility: Track the exact code, hyperparameters, &amp;amp; training dataset used to produce the model.&lt;/li&gt;
  &lt;li&gt;Model Lifecycle: Promote promising models to positions like “staging” or “production” - allowing downstream users to fetch the best model automatically.
Model Reporting: Create a Report dashboard to summarize model progression and performance over time&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jiawei Lu</name><email>jl5999@columbia.edu</email></author><category term="studylog" /><category term="zklab" /><summary type="html">Weights &amp;amp; Biases is the machine learning platform for developers to build better models faster.</summary></entry><entry><title type="html">[ZK Lab] #1: Concept - Data Centric AI</title><link href="http://localhost:4000/studylog/concept_data_centric_AI.html" rel="alternate" type="text/html" title="[ZK Lab] #1: Concept - Data Centric AI" /><published>2022-05-19T00:00:00-04:00</published><updated>2022-05-19T00:00:00-04:00</updated><id>http://localhost:4000/studylog/concept_data_centric_AI</id><content type="html" xml:base="http://localhost:4000/studylog/concept_data_centric_AI.html">&lt;h2 id=&quot;data-centric-ai&quot;&gt;&lt;a href=&quot;https://datacentricai.org/&quot;&gt;Data Centric AI&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&quot;definition&quot;&gt;Definition&lt;/h3&gt;
&lt;p&gt;Data-centric AI is the discipline of systematically engineering the data used to build an AI system.&lt;/p&gt;

&lt;h3 id=&quot;data-centric-ai-resource-hub&quot;&gt;Data-centric AI Resource Hub&lt;/h3&gt;
&lt;p&gt;A place to share cutting edge techniques and best practices for using data centric AI methods to build successful machine learning systems.&lt;/p&gt;

&lt;h3 id=&quot;topics&quot;&gt;Topics&lt;/h3&gt;
&lt;h4 id=&quot;labeling-and-crowdsourcing&quot;&gt;&lt;a href=&quot;https://datacentricai.org/labeling-and-crowdsourcing/&quot;&gt;Labeling and Crowdsourcing&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;&lt;img alt=&quot;Labeling and Crowdsourcing&quot; src=&quot;../../../assets/img/blog/2022-05-19/label-and-crowdsourcing.webp&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Suggestions:
    &lt;ul&gt;
      &lt;li&gt;Label many of the examples yourself before designing the task.&lt;/li&gt;
      &lt;li&gt;Pay and treat your workers fairly.&lt;/li&gt;
      &lt;li&gt;Always start with small pilots.&lt;/li&gt;
      &lt;li&gt;Always assume that the annotators are trying hard to build a model of your intentions: when something goes wrong, your reaction should be “what did I do wrong in communicating my intent?”, not “why weren’t they paying attention?”&lt;/li&gt;
      &lt;li&gt;Train with feedback.&lt;/li&gt;
      &lt;li&gt;It can often make sense to hire fewer people, more full time.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;data-augmentation&quot;&gt;&lt;a href=&quot;https://datacentricai.org/data-augmentation/&quot;&gt;Data Augmentation&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;&lt;img alt=&quot;Labeling and Crowdsourcing&quot; src=&quot;../../../assets/img/blog/2022-05-19/data-augmentation.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Data limitations:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Domain gaps&lt;/strong&gt;: The data you train your model with is quite different from the data you have to predict on in the real world.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Data bias&lt;/strong&gt;: When the data you collect has imbalances due to societal bias, how can you design methods that can overcome them?&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Data noise&lt;/strong&gt;: Noise can come from a variety of sources, including where labels are ambiguous, cluttered, or otherwise corrupted.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Definition of Data Augmentation:
    &lt;ul&gt;
      &lt;li&gt;Self-Supervision: When you have limited labeled data, you can try combining it with unlabeled data.
        &lt;ul&gt;
          &lt;li&gt;rotation&lt;/li&gt;
          &lt;li&gt;cropping&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Synthetic Data: While synthetic data is still in its infancy, there has been ongoing advances in generative models and it will become hugely important in the future for testing systems such as autonomous driving or robot learning.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Data-centric principles in data augmentation:
    &lt;ul&gt;
      &lt;li&gt;Core: the balance of positive and negative examples.&lt;/li&gt;
      &lt;li&gt;Also: combine self-supervision with weak supervision.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;data-in-deployment&quot;&gt;&lt;a href=&quot;https://datacentricai.org/data-in-deployment/&quot;&gt;Data in Deployment&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;&lt;img alt=&quot;Labeling and Crowdsourcing&quot; src=&quot;../../../assets/img/blog/2022-05-19/data-in-deployment.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Technical debt in machine learning:
    &lt;ul&gt;
      &lt;li&gt;The concept of technical debt originally comes from the world of software engineering, where it has often been found that pushing to develop software very quickly can create long term maintenance costs that must be paid back later, and that if left unaddressed can compound over time.&lt;/li&gt;
      &lt;li&gt;ML code – the bit that we tend to think of as the cool part – is actually a small component of the overall system.&lt;/li&gt;
      &lt;li&gt;In a lot of settings it is not actually statistically useful to get more data.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Three places to start:
    &lt;ul&gt;
      &lt;li&gt;Audit and monitor data quality.&lt;/li&gt;
      &lt;li&gt;Create data sheets for data sets.&lt;/li&gt;
      &lt;li&gt;Create and apply stress tests using data.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jiawei Lu</name><email>jl5999@columbia.edu</email></author><category term="studylog" /><category term="zklab" /><summary type="html">Data Centric AI</summary></entry><entry><title type="html">Transformer</title><link href="http://localhost:4000/studylog/Transformer.html" rel="alternate" type="text/html" title="Transformer" /><published>2022-04-14T00:00:00-04:00</published><updated>2022-04-14T00:00:00-04:00</updated><id>http://localhost:4000/studylog/Transformer</id><content type="html" xml:base="http://localhost:4000/studylog/Transformer.html">&lt;p&gt;&lt;img src=&quot;/assets/img/blog/2022-04-14/1.png&quot; alt=&quot;transformer struct&quot; /&gt;&lt;/p&gt;

&lt;p class=&quot;figcaption&quot;&gt;The full model architecture of the transformer.[3]&lt;/p&gt;

&lt;p&gt;“&lt;a href=&quot;https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf&quot;&gt;Attention is All you Need&lt;/a&gt;” (Vaswani, et al., 2017), without a doubt, is one of the most impactful and interesting paper in 2017. It presented a lot of improvements to the soft attention and make it possible to do seq2seq modeling without recurrent network units. The proposed “transformer” model is entirely built on the self-attention mechanisms without using sequence-aligned recurrent architecture.&lt;/p&gt;

&lt;p&gt;The secret recipe is carried in its model architecture.&lt;/p&gt;

&lt;h2 id=&quot;key-value-and-query&quot;&gt;Key, Value and Query&lt;/h2&gt;
&lt;p&gt;The major component in the transformer is the unit of multi-head self-attention mechanism. The transformer views the encoded representation of the input as a set of &lt;strong&gt;key-value&lt;/strong&gt; pairs, \((\mathbf{K}, \mathbf{V})\), both of dimension \(n\) (input sequence length); in the context of NMT, both the keys and values are the encoder hidden states. In the decoder, the previous output is compressed into a &lt;strong&gt;query&lt;/strong&gt; (\(\mathbf{Q}\) of dimension \(m\)) and the next output is produced by mapping this query and the set of keys and values.&lt;/p&gt;

&lt;p&gt;The transformer adopts the scaled dot-product attention: the output is a weighted sum of the values, where the weight assigned to each value is determined by the dot-product of the query with all the keys:&lt;/p&gt;

\[\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left( \frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{n}}\right) \mathbf{V}\]

&lt;h2 id=&quot;multi-head-self-attention&quot;&gt;Multi-Head Self-Attention&lt;/h2&gt;
&lt;p&gt;Rather than only computing the attention once, the multi-head mechanism runs through the scaled dot-product attention multiple times in parallel. The independent attention outputs are simply concatenated and linearly transformed into the expected dimensions. I assume the motivation is because ensembling always helps? ;) According to the paper, “multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.”&lt;/p&gt;

\[\begin{align}
\text{MultiHead}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) &amp;amp; = [\text{head}_1; ...; \text{head}_h]\mathbf{W}^O \\
\text{where head_i} &amp;amp; = \text{Attention}(\mathbf{QW}_i^Q, \mathbf{KW}_i^K, \mathbf{VW}_i^V)
\end{align}\]

&lt;p&gt;where \(\mathbf{W}_i^Q\), \(\mathbf{W}_i^K\), \(\mathbf{W}_i^V\) and \(\mathbf{W}_i^O\) are parameter matrices to be learned.&lt;/p&gt;

&lt;h2 id=&quot;encoder&quot;&gt;Encoder&lt;/h2&gt;
&lt;p&gt;The encoder generates an attention-based representation with capability to locate a specific piece of information from a potentially infinitely-large context.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A stack of \(N=6\) identical layers.&lt;/li&gt;
  &lt;li&gt;Each layer has a &lt;strong&gt;multi-head self-attention layer&lt;/strong&gt; and a simple position-wise &lt;strong&gt;fully connected feed-forward network&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Each sub-layer adopts a &lt;strong&gt;residual&lt;/strong&gt; connection and a layer &lt;strong&gt;normalization&lt;/strong&gt;. All the sub-layers output data of the same dimension \(d_\text{model} = 512\).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;decoder&quot;&gt;Decoder&lt;/h2&gt;
&lt;p&gt;The decoder is able to retrieval from the encoded representation.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A stack of N = 6 identical layers&lt;/li&gt;
  &lt;li&gt;Each layer has two sub-layers of multi-head attention mechanisms and one sub-layer of fully-connected feed-forward network.&lt;/li&gt;
  &lt;li&gt;Similar to the encoder, each sub-layer adopts a residual connection and a layer normalization.&lt;/li&gt;
  &lt;li&gt;The first multi-head attention sub-layer is modified to prevent positions from attending to subsequent positions, as we don’t want to look into the future of the target sequence when predicting the current position.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;full-architecture&quot;&gt;Full Architecture&lt;/h2&gt;
&lt;p&gt;Finally here is the complete view of the transformer’s architecture:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Both the source and target sequences first go through embedding layers to produce data of the same dimension \(d_\text{model} = 512\).&lt;/li&gt;
  &lt;li&gt;To preserve the position information, a sinusoid-wave-based positional encoding is applied and summed with the embedding output.&lt;/li&gt;
  &lt;li&gt;A softmax and linear layer are added to the final decoder output.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;p&gt;[1] &lt;a href=&quot;https://lilianweng.github.io/posts/2018-06-24-attention/&quot;&gt;Lil’Log: Attention? Attention!&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&quot;https://jalammar.github.io/illustrated-transformer/&quot;&gt;The Illustrated Transformer&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] &lt;a href=&quot;https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf&quot;&gt;Attention Is All You Need&lt;/a&gt;&lt;/p&gt;</content><author><name>Jiawei Lu</name><email>jl5999@columbia.edu</email></author><category term="studylog" /><category term="dl" /><summary type="html"></summary></entry><entry><title type="html">Attention Mechanism</title><link href="http://localhost:4000/studylog/Attention.html" rel="alternate" type="text/html" title="Attention Mechanism" /><published>2022-04-13T00:00:00-04:00</published><updated>2022-04-13T00:00:00-04:00</updated><id>http://localhost:4000/studylog/Attention</id><content type="html" xml:base="http://localhost:4000/studylog/Attention.html">&lt;h2 id=&quot;encoder-decoder-in-nlp&quot;&gt;Encoder-Decoder in NLP&lt;/h2&gt;

&lt;p&gt;Encoder-Decoder This framework is a good illustration of the core ideas of machine learning:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Transforming real-world problems into mathematical problems and solving real-world problems by solving mathematical problems.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Encoder’s role is to “transform real problems into mathematical problems.”&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Decoder’s role is to “solve mathematical problems and transform them into real-world solutions.”&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;notes&quot;&gt;Notes&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Regardless of the length of the input and output, the length of the vector in the middle (the output of encoder) is fixed (this is also its drawback, as explained below).&lt;/li&gt;
  &lt;li&gt;Different encoders and decoders can be selected depending on the task (can be one RNN But usually its variant LSTM Or CRANE).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As long as it conforms to the above framework, it can be collectively referred to as the Encoder-Decoder model. Speaking of the Encoder-Decoder model, a term is often mentioned - Seq2Seq.&lt;/p&gt;

&lt;h3 id=&quot;seq2seq&quot;&gt;Seq2Seq&lt;/h3&gt;

&lt;p&gt;Seq2Seq (short for Sequence-to-sequence), as literally, enters a sequence and outputs another sequence. The most important aspect of this structure is that the length of the input sequence and the output sequence are variable.&lt;/p&gt;

&lt;h3 id=&quot;origin-of-seq2seq&quot;&gt;Origin of Seq2Seq&lt;/h3&gt;

&lt;p&gt;Before the Seq2Seq framework was proposed, deep neural networks achieved very good results in image classification and other issues. In the problem that it is good at solving, the input and output can usually be represented as a fixed-length vector. If the length is slightly changed, the zero-padding operation is used.&lt;/p&gt;

&lt;p&gt;However, many important issues, such as machine translation, speech recognition, automatic dialogue, etc., are expressed in sequence, and their length is not known in advance. Therefore, how to break through the limitations of the previous deep neural network, so that it can adapt to these scenarios, has become a research hotspot since 2013, and the Seq2Seq framework came into being.&lt;/p&gt;

&lt;h3 id=&quot;relationship-between-seq2seq-and-encoder-decoder&quot;&gt;Relationship between “Seq2Seq” and “Encoder-Decoder”&lt;/h3&gt;

&lt;p&gt;Seq2Seq does not specifically refer to specific methods. For the purpose of “input is a sequence, output is also a sequence”, it can be collectively referred to as Seq2Seq model.&lt;/p&gt;

&lt;p&gt;The specific methods used by Seq2Seq are basically in the scope of the Encoder-Decoder model.&lt;/p&gt;

&lt;p&gt;To sum up:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Seq2Seq belongs to the broad category of Encoder-Decoder&lt;/li&gt;
  &lt;li&gt;Seq2Seq emphasizes the purpose, Encoder-Decoder emphasizes the method&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;defects-of-encoder-decoder&quot;&gt;Defects of Encoder-Decoder&lt;/h3&gt;

&lt;p&gt;When the input information is too long, some information will be lost in the output.&lt;/p&gt;

&lt;p&gt;Attention solves the problem of information loss.&lt;/p&gt;

&lt;h2 id=&quot;principle-of-attention&quot;&gt;Principle of Attention&lt;/h2&gt;

&lt;p&gt;Attention is a technique that mimics cognitive attention. The effect enhances some parts of the input data while diminishing other parts — the thought being that the network should devote more focus to that small but important part of the data. Learning which part of the data is more important than others depends on the context and is trained by gradient descent.&lt;/p&gt;

&lt;p&gt;In a nutshell, attention in deep learning can be broadly interpreted as a vector of importance weights: in order to predict or infer one element, such as a pixel in an image or a word in a sentence, we estimate using the attention vector how strongly it is correlated with (or “attends to” as you may have read in many papers) other elements and take the sum of their values weighted by the attention vector as the approximation of the target.&lt;/p&gt;

&lt;h2 id=&quot;definition-of-attention-mechanism-in-neural-machine-translation&quot;&gt;Definition of Attention Mechanism in Neural Machine Translation&lt;/h2&gt;

&lt;p&gt;Say, we have a source sequence \(\mathbf{x}\) of length \(n\) and try to output a target sequence \(y\) of length \(m\):&lt;/p&gt;

\[\begin{align}
    \mathbf{x} &amp;amp; = [x_1, x_2, ..., x_n]\\
    \mathbf{y} &amp;amp; = [y_1, y_2, ..., y_m]
\end{align}\]

&lt;p&gt;The encoder is a bidirectional RNN (or other recurrent network setting of your choice) with a forward hidden state \(\overrightarrow{\mathbf{h}_i}\) and a backward one \(\overleftarrow{\mathbf{h}_i}\). A simple concatenation of two represents the encoder state. The motivation is to include both the preceding and following words in the annotation of one word.&lt;/p&gt;

\[\mathbf{h}_i = [\overrightarrow{\mathbf{h}_i}^\top; \overleftarrow{\mathbf{h}_i}^\top]^\top, \ i = 1,...,n\]

&lt;p&gt;The decoder network has hidden state \(\mathbf{s}_t = f(\mathbf{s}_{t-1}, y_{t-1}, \mathbf{c}_t)\) for the output word at position \(t\), \(t = 1, ..., m\), where the context vector \(\mathbf{c}_t\) is a sum of hidden states of the input sequence, weighted by alignment scores:&lt;/p&gt;

\[\begin{align}
    \mathbf{c}_t &amp;amp; = \sum_{i=1}^n \alpha_{t,i} \mathbf{h_i} \\
    \alpha_{t,i} &amp;amp; = \text{align}(y_t, x_i) \\
    &amp;amp; = \frac{\exp(\text{score}(\mathbf{s}_{t-1}, \mathbf{h}_i))}{\sum_{i&apos; = 1}^n \exp(\text{score}(\mathbf{s}_{t-1}, \mathbf{h}_{i&apos;}))}
\end{align}\]

&lt;p&gt;The alignment model assigns a score \(\alpha_{t, i}\) to the pair of input at position \(i\) and output at position \(t\), \((y_t, x_i)\),  based on how well they match. The set of \(\{\alpha_{t, i\}\) are weights defining how much of each source hidden state should be considered for each output. In Bahdanau’s paper, the alignment score \(\alpha\) is parametrized by a &lt;strong&gt;feed-forward network&lt;/strong&gt; with a single hidden layer and this network is jointly trained with other parts of the model. The score function is therefore in the following form, given that tanh is used as the non-linear activation function:&lt;/p&gt;

&lt;p&gt;\(\text{score}(\mathbf{s}_t, \mathbf{h}_i) = \mathbf{v}_a^\top \tanh \left( \mathbf{W}_a \left[ \mathbf{s}_t; \mathbf{h}_i \right] \right)\)
where both \(\mathbf{v}_a\) and \(\mathbf{W}_a\) are weight matrices to be learned in the alignment model.&lt;/p&gt;

&lt;h2 id=&quot;self-attention&quot;&gt;Self-Attention&lt;/h2&gt;
&lt;p&gt;Self-attention, also known as intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the same sequence. It has been shown to be very useful in machine reading, abstractive summarization, or image description generation.&lt;/p&gt;

&lt;p&gt;The long short-term memory network paper used self-attention to do machine reading. In the example below, the self-attention mechanism enables us to learn the correlation between the current words and the previous part of the sentence.&lt;/p&gt;

&lt;h2 id=&quot;soft-vs-hard-attention&quot;&gt;Soft vs Hard Attention&lt;/h2&gt;
&lt;p&gt;In the &lt;a href=&quot;http://proceedings.mlr.press/v37/xuc15.pdf&quot;&gt;show, attend and tell&lt;/a&gt; paper, attention mechanism is applied to images to generate captions. The image is first encoded by a CNN to extract features. Then a LSTM decoder consumes the convolution features to produce descriptive words one by one, where the weights are learned through attention. The visualization of the attention weights clearly demonstrates which regions of the image the model is paying attention to so as to output a certain word.&lt;/p&gt;

&lt;p&gt;This paper first proposed the distinction between “soft” vs “hard” attention, based on whether the attention has access to the entire image or only a patch:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Soft Attention: the alignment weights are learned and placed “softly” over all patches in the source image; essentially the same type of attention as in &lt;a href=&quot;https://arxiv.org/abs/1409.0473&quot;&gt;Bahdanau et al., 2015&lt;/a&gt;.
    &lt;ul&gt;
      &lt;li&gt;Pro: the model is smooth and differentiable.&lt;/li&gt;
      &lt;li&gt;Con: expensive when the source input is large.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Hard Attention: only selects one patch of the image to attend to at a time.
    &lt;ul&gt;
      &lt;li&gt;Pro: less calculation at the inference time.&lt;/li&gt;
      &lt;li&gt;Con: the model is non-differentiable and requires more complicated techniques such as variance reduction or reinforcement learning to train. (&lt;a href=&quot;https://arxiv.org/abs/1508.04025&quot;&gt;Luong, et al., 2015&lt;/a&gt;)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;advantage-of-attention-mechanism&quot;&gt;Advantage of Attention mechanism&lt;/h2&gt;

&lt;p&gt;3 main reasons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Less source&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In comparison, the complexity is smaller and the parameters are less than CNN and RNN based model. Therefore, the requirements for computing power are even smaller.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;High speed&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Attention solves the problem that RNN cannot be computed in parallel. Each step of the Attention mechanism calculation does not depend on the calculation results of the previous step, so it can be processed in parallel as CNN.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Good result&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Before the introduction of the Attention mechanism, there was a problem that everyone had been annoyed: long-distance information would be weakened, just like people with weak memory, and the same thing could not be remembered in the past.&lt;/p&gt;

&lt;p&gt;However, Attention can make model focus without losing important information.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;p&gt;[1] Lil’Log: Attention? Attention! (&lt;a href=&quot;https://lilianweng.github.io/posts/2018-06-24-attention/#definition&quot;&gt;Link&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;[2] Zhihu BLOG (&lt;a href=&quot;https://zhuanlan.zhihu.com/p/91839581&quot;&gt;Link&lt;/a&gt;)&lt;/p&gt;</content><author><name>Jiawei Lu</name><email>jl5999@columbia.edu</email></author><category term="studylog" /><category term="dl" /><summary type="html">Encoder-Decoder in NLP</summary></entry><entry><title type="html">CV II Lecture 06</title><link href="http://localhost:4000/studylog/CV_lecture06.html" rel="alternate" type="text/html" title="CV II Lecture 06" /><published>2022-03-04T00:00:00-05:00</published><updated>2022-03-04T00:00:00-05:00</updated><id>http://localhost:4000/studylog/CV_lecture06</id><content type="html" xml:base="http://localhost:4000/studylog/CV_lecture06.html">&lt;h2 id=&quot;behavior-continuum&quot;&gt;Behavior Continuum&lt;/h2&gt;

&lt;h2 id=&quot;representing-video&quot;&gt;Representing Video&lt;/h2&gt;

&lt;h2 id=&quot;3d-convolutional-networks&quot;&gt;3D convolutional Networks&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;2D convolutions vs. 3D convolutions&lt;/li&gt;
  &lt;li&gt;3D filters at the first layer.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-stream-network&quot;&gt;2-Stream Network&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Spatial Stream ConvNet&lt;/li&gt;
  &lt;li&gt;Tenporal Stream ConvNet&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;recurrent-neural-network&quot;&gt;Recurrent Neural Network&lt;/h2&gt;

&lt;h3 id=&quot;forward-function&quot;&gt;forward function&lt;/h3&gt;

&lt;p&gt;\begin{equation}
h_i = f(w_{hx}^T x_i + w_{hh}^Th_{i-1})
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
y_i = f(w_{yh}^T h_i)
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;vanishingexploding-gradient&quot;&gt;Vanishing/Exploding Gradient&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Forward pass:
  \begin{equation}
  z_i = f(w_x^T x_i + w^T z_{i-1})
  \end{equation}&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Gradients:
  \begin{equation}
  \frac{dL}{dw} = \frac{dL}{dz_{i+1}} \frac{dz_{i+1}}{dz_i} \frac{z_i}{dw}
  \end{equation}&lt;/p&gt;

    &lt;p&gt;\begin{equation}
  \frac{dL}{dw} = \frac{dL}{dz_T} (\prod_{j=i}^{T-1}\frac{dz_{j+1}}{dz_j}) \frac{dz_i}{dw}
  \end{equation}&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;long-short-term-memory-lstm&quot;&gt;Long Short-Term Memory (LSTM)&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;&gt;colah’s blog&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;future-generation&quot;&gt;Future Generation&lt;/h2&gt;
&lt;p&gt;\begin{equation}
x_t \rightarrow f(x_t; w) \rightarrow x_{t+1}
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;minimize-euclidean-distance&quot;&gt;Minimize Euclidean distance:&lt;/h3&gt;
&lt;p&gt;\begin{equation}
\min_w \sum_i || f(x_t^i;w) - x_{t+1}^i||_2^2
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;train&quot;&gt;Train&lt;/h3&gt;
&lt;p&gt;\begin{equation}
\min_f \sum_i ||f(x_t^i) - g(x_{t+1}^i)||_2^2
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
\min_{f,\delta} \sum_i \sum_k^K \delta_k^i \cdot ||f_k(x_t^i) - g(x_{t+1}^i)||_2^2
\end{equation}
s.t. \(\delta^i \in{0,1}^K\) and \(||\delta^i||_1 = 1 \ \forall_i\)&lt;/p&gt;

&lt;h3 id=&quot;expectation-maximization&quot;&gt;Expectation-Maximization&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;E Step: Fill in missing variables and estimate latent variables&lt;/li&gt;
  &lt;li&gt;M Step: Fit the model (with back-propagation in this case)&lt;/li&gt;
  &lt;li&gt;Repeat&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Jiawei Lu</name><email>jl5999@columbia.edu</email></author><category term="studylog" /><category term="cv" /><summary type="html">Behavior Continuum</summary></entry><entry><title type="html">Object Detection for Dummies Part 1: Gradient Vector, HOG, and SS</title><link href="http://localhost:4000/studylog/ODDP1.html" rel="alternate" type="text/html" title="Object Detection for Dummies Part 1: Gradient Vector, HOG, and SS" /><published>2022-02-09T00:00:00-05:00</published><updated>2022-02-09T00:00:00-05:00</updated><id>http://localhost:4000/studylog/ODDP1</id><content type="html" xml:base="http://localhost:4000/studylog/ODDP1.html">&lt;p&gt;Jan 28, 2021.
The raw blog &lt;a href=&quot;https://lilianweng.github.io/lil-log/2017/10/29/object-recognition-for-dummies-part-1.html&quot;&gt;URL&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;image-gradient-vector&quot;&gt;Image Gradient Vector&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Derivative
    &lt;ul&gt;
      &lt;li&gt;Scalar&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Directional Derivative
    &lt;ul&gt;
      &lt;li&gt;Scalar&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Gradient
    &lt;ul&gt;
      &lt;li&gt;Vector
        &lt;pre&gt;&lt;code class=&quot;language-python=&quot;&gt;import numpy as np
import scipy.signal as sig
data = np.array([[0, 105, 0], [40, 255, 90], [0, 55, 0]])
G_x = sig.convolve2d(data, np.array([[-1, 0, 1]]), mode=&apos;valid&apos;)
G_y = sig.convolve2d(data, np.array([[-1], [0], [1]]), mode=&apos;valid&apos;)
&lt;/code&gt;&lt;/pre&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Common Image Processing Kernels(&lt;a href=&quot;https://zhuanlan.zhihu.com/p/67197912&quot;&gt;Useful URL&lt;/a&gt;)
    &lt;ul&gt;
      &lt;li&gt;Prewitt operator&lt;/li&gt;
      &lt;li&gt;Sobel operator&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;histogram-of-oriented-gradients-hog&quot;&gt;Histogram of Oriented Gradients (HOG)&lt;/h2&gt;
&lt;p&gt;Useful &lt;a href=&quot;https://zhuanlan.zhihu.com/p/85829145&quot;&gt;URL&lt;/a&gt; in zhihu.&lt;/p&gt;

&lt;h3 id=&quot;how-hog-works&quot;&gt;How HOG works&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Preprocess the image, including resizing and color normalization.
    &lt;ul&gt;
      &lt;li&gt;Gamma Correction&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;\begin{equation}
f(x) = x^{\gamma}
\end{equation}&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python=&quot;&gt;# Gamma Correction
import cv2
import numpy as np
img = cv2.imread(&apos;gamma.jpg&apos;, 0)
img2 = np.power(img/float(np.max(img)), 1.5)
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
  &lt;li&gt;Compute the gradient vector of every pixel, as well as its magnitude and direction&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;\begin{equation}
g = \sqrt{g_x^2+g_y^2} \ 
\theta = \arctan \frac{g_y}{g_x}
\end{equation}&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python=&quot;&gt;import cv2
import numpy as np

# Read image
img = cv2.imread(&apos;runner.jpg&apos;)
img = np.float32(img) / 255.0  # 归一化

# x,y gradient
gx = cv2.Sobel(img, cv2.CV_32F, 1, 0, ksize=1)
gy = cv2.Sobel(img, cv2.CV_32F, 0, 1, ksize=1)

# gradient
mag, angle = cv2.cartToPolar(gx, gy, angleInDegrees=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Divide the image into many 8x8 pixel cells. In each cell, the magnitude values of these 64 cells are binned and cumulatively added into 9 buckets of unsigned direction (no sign, so 0-180 degree rather than 0-360 degree; this is a practical choice based on empirical experiments).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Then we slide a 2x2 cells (thus 16x16 pixels) block across the image. In each block region, 4 histograms of 4 cells are concatenated into one-dimensional vector of 36 values and then normalized to have an unit weight. The final HOG feature vector is the concatenation of all the block vectors. It can be fed into a classifier like SVM for learning object recognition tasks.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&quot;language-python=&quot;&gt;# HOG
from skimage import feature, exposure
import cv2
image = cv2.imread(&apos;/home/zxd/Pictures/Selection_018.jpg&apos;)
fd, hog_image = feature.hog(image, orientations=9, pixels_per_cell=(16, 16),
                    cells_per_block=(2, 2), visualize=True)

# Rescale histogram for better display
hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))

cv2.imshow(&apos;img&apos;, image)
cv2.imshow(&apos;hog&apos;, hog_image_rescaled)
cv2.waitKey(0)==ord(&apos;q&apos;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;image-segmentation-felzenszwalbs-algorithm&quot;&gt;Image Segmentation (Felzenszwalb’s Algorithm)&lt;/h2&gt;

&lt;h3 id=&quot;graph-construction&quot;&gt;Graph Construction&lt;/h3&gt;

&lt;p&gt;There are two approaches to constructing a graph out of an image.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Grid Graph: Each pixel is only connected with surrounding neighbours (8 other cells in total). The edge weight is the absolute difference between the intensity values of the pixels.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Nearest Neighbor Graph: Each pixel is a point in the feature space (x, y, r, g, b), in which (x, y) is the pixel location and (r, g, b) is the color values in RGB. The weight is the Euclidean distance between two pixels’ feature vectors.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>Jiawei Lu</name><email>jl5999@columbia.edu</email></author><category term="studylog" /><category term="dl" /><summary type="html">Jan 28, 2021. The raw blog URL.</summary></entry><entry><title type="html">NLP Lecture 05: Introduction to Syntax and Formal Languages</title><link href="http://localhost:4000/studylog/NLP_lecture05.html" rel="alternate" type="text/html" title="NLP Lecture 05: Introduction to Syntax and Formal Languages" /><published>2022-02-09T00:00:00-05:00</published><updated>2022-02-09T00:00:00-05:00</updated><id>http://localhost:4000/studylog/NLP_lecture05</id><content type="html" xml:base="http://localhost:4000/studylog/NLP_lecture05.html">&lt;p&gt;Keywords: Introduction to Syntax and Formal Languages.&lt;/p&gt;

&lt;h2 id=&quot;syntax&quot;&gt;Syntax&lt;/h2&gt;
&lt;h3 id=&quot;syntax-as-an-interface&quot;&gt;Syntax as an Interface&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Syntax can be seen as the interface between morphology (structure of words) and semantics.&lt;/li&gt;
  &lt;li&gt;Can judge if a sentence is grammatical or not, even if it doesn’t make sense semantically.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;key-concepts-of-syntax&quot;&gt;Key Concepts of Syntax&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Constituency and Recursion.&lt;/li&gt;
  &lt;li&gt;Dependency.&lt;/li&gt;
  &lt;li&gt;Grammatical Relations.&lt;/li&gt;
  &lt;li&gt;Subcategorization.&lt;/li&gt;
  &lt;li&gt;Long-distance dependencies&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;constituents&quot;&gt;Constituents&lt;/h2&gt;
&lt;p&gt;A constituent is a group of words that behave as a single unit (within a hierarchical structure).&lt;/p&gt;

&lt;h3 id=&quot;constituency&quot;&gt;Constituency&lt;/h3&gt;
&lt;p&gt;There is a great number of constituency tests. They typically involve moving constituents around or replacing them.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Topicalization&lt;/li&gt;
  &lt;li&gt;Pro-form Substitution&lt;/li&gt;
  &lt;li&gt;Wh-question test.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;constituent-labels&quot;&gt;Constituent Labels&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Choose constituents so each one has one non-bracketed word: the head.&lt;/li&gt;
  &lt;li&gt;Category of Constituent: XP, where X is the part-of-speech of the head: NP, VP, AdjP, AdvP, DetP&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;recursion-in-language&quot;&gt;Recursion in Language&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;One of the most important attributes of Natural Languages is that they are recursive.&lt;/li&gt;
  &lt;li&gt;There are infinitely many sentences in a language, but in predictable structures.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;context-free-grammars-cfg&quot;&gt;Context Free Grammars (CFG)&lt;/h2&gt;
&lt;p&gt;A context free grammar is defined by:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Set of &lt;strong&gt;terminal symbols&lt;/strong&gt; \(\Sigma\).&lt;/li&gt;
  &lt;li&gt;Set of &lt;strong&gt;non-terminal symbols&lt;/strong&gt; \(N\).&lt;/li&gt;
  &lt;li&gt;A &lt;strong&gt;start symbol&lt;/strong&gt; \(S \in N\).&lt;/li&gt;
  &lt;li&gt;Set \(R\) of &lt;strong&gt;productions&lt;/strong&gt; of the form \(A \rightarrow \beta\), where \(A \in N\) and \(\beta \in (\Sigma \cup N)^*\), i.e. \(\beta\) is a string of terminals and non-terminals.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;language-of-a-cfg&quot;&gt;Language of a CFG&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Given a CFG \(G=(N, \Sigma,R,S)\):
    &lt;ul&gt;
      &lt;li&gt;Given a string \(\alpha A \gamma\), where \(A \in N\), we can derive \(\alpha \beta \gamma\) if there is a production \(A \rightarrow \beta \in R\).&lt;/li&gt;
      &lt;li&gt;\(\alpha \implies \beta\) means that \(G\) can derive \(\beta\) from \(\alpha\) in a single step.&lt;/li&gt;
      &lt;li&gt;\(\alpha \implies \beta^*\) means that \(G\) can derive \(\beta\) from \(\alpha\) in a finite number of steps.&lt;/li&gt;
      &lt;li&gt;The language of G is defined as the set of all terminal strings that can be derived from the start symbol.&lt;/li&gt;
      &lt;li&gt;
\[L(G) = \{\beta \in \Sigma ^*, s.t. S\implies ^* \beta\}\]
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;derivations-and-derived-strings&quot;&gt;Derivations and Derived Strings&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;CFG is a string rewriting formalism, so the derived objects are strings.&lt;/li&gt;
  &lt;li&gt;A derivation is a sequence of rewriting steps.&lt;/li&gt;
  &lt;li&gt;CFGs are context free: applicability of a rule depends only on the nonterminal symbol, not on its context.&lt;/li&gt;
  &lt;li&gt;Therefore, the order in which multiple non-terminals in a partially derived string are replaced does not matter. We can represent identical derivations in a derivation tree.&lt;/li&gt;
  &lt;li&gt;The derivation tree implies a parse tree.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;regular-grammars&quot;&gt;Regular Grammars&lt;/h2&gt;
&lt;p&gt;A regular grammar is defined by:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Set of &lt;strong&gt;terminal symbols&lt;/strong&gt; \(\Sigma\).&lt;/li&gt;
  &lt;li&gt;Set of &lt;strong&gt;non-terminal symbols&lt;/strong&gt; \(N\).&lt;/li&gt;
  &lt;li&gt;A &lt;strong&gt;start symbol&lt;/strong&gt; \(S \in N\).&lt;/li&gt;
  &lt;li&gt;Set \(R\) of &lt;strong&gt;productions&lt;/strong&gt; of the form \(A \rightarrow aB\) or \(A \rightarrow a\), where \(A,B \in N\) and \(a \in \Sigma\).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;finite-state-automata&quot;&gt;Finite State Automata&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Regular grammars can be implemented as finite state automata.&lt;/li&gt;
  &lt;li&gt;The set of all regular languages is strictly smaller than the set of context-free languages.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;center-embeddings&quot;&gt;Center Embeddings&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Problem: Regular grammars cannot capture long-distance dependencies.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;formal-grammar-and-parsing&quot;&gt;Formal Grammar and Parsing&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Formal Grammars are used in linguistics, NLP, programming languages.&lt;/li&gt;
  &lt;li&gt;We want to build a compact model that describes a complete language.&lt;/li&gt;
  &lt;li&gt;Need efficient algorithms to determine if a sentence is in the language or not (recognition problem).&lt;/li&gt;
  &lt;li&gt;We also want to recover the structure imposed by the grammar (parsing problem).&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jiawei Lu</name><email>jl5999@columbia.edu</email></author><category term="studylog" /><category term="nlp" /><summary type="html">Keywords: Introduction to Syntax and Formal Languages.</summary></entry><entry><title type="html">Leetcode</title><link href="http://localhost:4000/studylog/leetcode.html" rel="alternate" type="text/html" title="Leetcode" /><published>2022-02-02T00:00:00-05:00</published><updated>2022-02-02T00:00:00-05:00</updated><id>http://localhost:4000/studylog/leetcode</id><content type="html" xml:base="http://localhost:4000/studylog/leetcode.html">&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Two sum&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Hashmap&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Add two numbers
    &lt;ul&gt;
      &lt;li&gt;Linked List&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Longest Substring Without Repeating Characters
    &lt;ul&gt;
      &lt;li&gt;Two pointers&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Median of Two Sorted Arrays
    &lt;ul&gt;
      &lt;li&gt;Median&lt;/li&gt;
      &lt;li&gt;Binary Search&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Longest Palindromic Substring
    &lt;ul&gt;
      &lt;li&gt;DP&lt;/li&gt;
      &lt;li&gt;Java vs Python&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Regular Expression Matching
    &lt;ul&gt;
      &lt;li&gt;Recursion&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Container With Most Water
    &lt;ul&gt;
      &lt;li&gt;Two pointers&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;3 Sum
    &lt;ul&gt;
      &lt;li&gt;2 sum&lt;/li&gt;
      &lt;li&gt;Hashmap&lt;/li&gt;
      &lt;li&gt;sort&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Letter Combinations of a Phone Number
    &lt;ul&gt;
      &lt;li&gt;Hashmap&lt;/li&gt;
      &lt;li&gt;Recursion&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Remove Nth Node From End of List
    &lt;ul&gt;
      &lt;li&gt;Symmetric&lt;/li&gt;
      &lt;li&gt;Two pointers&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Valid Parentheses
    &lt;ul&gt;
      &lt;li&gt;Stack&lt;/li&gt;
      &lt;li&gt;Hashmap for pair&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Generate Parentheses
    &lt;ul&gt;
      &lt;li&gt;DFS&lt;/li&gt;
      &lt;li&gt;Stack&lt;/li&gt;
      &lt;li&gt;DP&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Merge k Sorted Lists
    &lt;ul&gt;
      &lt;li&gt;Priority Queue
        &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;Queue&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PriorityQueue&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;Python &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sorted(list)&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Reverse Nodes in k-Group
    &lt;ul&gt;
      &lt;li&gt;Reverse list&lt;/li&gt;
      &lt;li&gt;6 pointers: dummy, jump, l, r, prev, cur&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Next Permutation
    &lt;ul&gt;
      &lt;li&gt;Find Pattern: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nums[i-1] &amp;lt; nums[i]&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Decreasing List&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Longest Valid Parentheses
    &lt;ul&gt;
      &lt;li&gt;Find Pattern: If ‘)’ more than ‘(‘, reset&lt;/li&gt;
      &lt;li&gt;Stack&lt;/li&gt;
      &lt;li&gt;Two traverse&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Search in Rotated Sorted Array
    &lt;ul&gt;
      &lt;li&gt;Binary Search&lt;/li&gt;
      &lt;li&gt;Mind &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;left &amp;lt;= mid&lt;/code&gt; since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mid = (left + right)//2&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Find First and Last Position of Element in Sorted Array
    &lt;ul&gt;
      &lt;li&gt;Binary Search&lt;/li&gt;
      &lt;li&gt;Find left most: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;left = mid + 1, right = mid&lt;/code&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;python = 
  while left &amp;lt; right:
      mid = (left + right) // 2
      if nums[mid] &amp;lt; target:
          left = mid + 1
      else:
          right = mid
 &lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Find right most: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;left = mid, right = mid - 1&lt;/code&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;python = 
  while left &amp;lt; right:
      mid = (left + right + 1) // 2
      if nums[mid] &amp;gt; target:
          right = mid - 1
      else:
          left = mid
 &lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Search Insert Position
    &lt;ul&gt;
      &lt;li&gt;Binary Search&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Combination Sum
    &lt;ul&gt;
      &lt;li&gt;Backtracking/DFS
        &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;dfs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;dfs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;First Missing Positive
    &lt;ul&gt;
      &lt;li&gt;Find Pattern: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[1,2,...,n+1]&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Hash &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nums[nums[i]%n] += n&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Trapping Rain Water
    &lt;ul&gt;
      &lt;li&gt;DP: store leftMax and rightMax&lt;/li&gt;
      &lt;li&gt;Two pointers&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Jump Game II
    &lt;ul&gt;
      &lt;li&gt;Two pointers: left for n steps, right for n+1 steps&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Permutations
    &lt;ul&gt;
      &lt;li&gt;DFS&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Rotate Image
    &lt;ul&gt;
      &lt;li&gt;List transportation with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zip()&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rotate = flip + trans&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Group Anagrams
    &lt;ul&gt;
      &lt;li&gt;permutations have the same characters: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sorted()&lt;/code&gt; + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dict&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Maximum Subarray
    &lt;ul&gt;
      &lt;li&gt;Divide and Conquer&lt;/li&gt;
      &lt;li&gt;DP&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Jump Game
    &lt;ul&gt;
      &lt;li&gt;two pointers&lt;/li&gt;
      &lt;li&gt;from end to start: where is the last reachable point&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Merge Intervals
    &lt;ul&gt;
      &lt;li&gt;Graph&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sort()&lt;/code&gt; + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max()&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Unique Paths
    &lt;ul&gt;
      &lt;li&gt;DP&lt;/li&gt;
      &lt;li&gt;Math $C_m^n$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Minimum Path Sum
    &lt;ul&gt;
      &lt;li&gt;DP&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Climbing Stairs
    &lt;ul&gt;
      &lt;li&gt;DP&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Edit Distance
    &lt;ul&gt;
      &lt;li&gt;DP&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Search a 2D Matrix
    &lt;ul&gt;
      &lt;li&gt;Binary Search: $m \times n$ sorted array&lt;/li&gt;
      &lt;li&gt;if element not in list, find the left boundary: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mid = (left + right)//2&lt;/code&gt;,  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;right -= 1&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Sort Colors
    &lt;ul&gt;
      &lt;li&gt;count sort&lt;/li&gt;
      &lt;li&gt;*Dutch National Flag Problem&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Minimum Window Substring
    &lt;ul&gt;
      &lt;li&gt;Sliding window, two pointers&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Subsets
    &lt;ul&gt;
      &lt;li&gt;Backtracking&lt;/li&gt;
      &lt;li&gt;Bitmask&lt;/li&gt;
      &lt;li&gt;DP&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Word Search
    &lt;ul&gt;
      &lt;li&gt;Backtracking / DFS&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;oa&quot;&gt;OA&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;Merge Sort: Counting Inversions&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&quot;language-python=&quot;&gt;#!/bin/python3

import math
import os
import random
import re
import sys

#
# Complete the &apos;countInversions&apos; function below.
#
# The function is expected to return a LONG_INTEGER.
# The function accepts INTEGER_ARRAY arr as parameter.
#

def countInversions(arr):
    # Write your code here
    temp_arr = [0]*len(arr)
    return mergeSort(arr, temp_arr, 0, len(arr) - 1)

def mergeSort(arr, temp_arr, left, right):
    inv_cnt = 0
    if left &amp;lt; right:
        mid = (left + right) // 2
        inv_cnt += mergeSort(arr, temp_arr, left, mid)
        inv_cnt += mergeSort(arr, temp_arr, mid + 1, right)
        inv_cnt += merge(arr, temp_arr, left, mid, right)
    return inv_cnt

def merge(arr, temp_arr, left, mid, right):
    i, j, k = left, mid+1, left
    inv_cnt = 0
    while i &amp;lt;= mid and j &amp;lt;= right:
        if arr[i] &amp;gt; arr[j]:
            inv_cnt += mid - i + 1
            temp_arr[k] = arr[j]
            j += 1

        else:
            temp_arr[k] = arr[i]
            i += 1
        k += 1
    
    while i &amp;lt;= mid:
        temp_arr[k] = arr[i]
        i += 1
        k += 1
    while j &amp;lt;= right:
        temp_arr[k] = arr[j]
        j += 1
        k += 1
    
    for x in range(left, right + 1):
        arr[x] = temp_arr[x]
    
    return inv_cnt
    

if __name__ == &apos;__main__&apos;:
    fptr = open(os.environ[&apos;OUTPUT_PATH&apos;], &apos;w&apos;)

    t = int(input().strip())

    for t_itr in range(t):
        n = int(input().strip())

        arr = list(map(int, input().rstrip().split()))

        result = countInversions(arr)

        fptr.write(str(result) + &apos;\n&apos;)

    fptr.close()

&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Stars and Bars:
```python=
def stars_and_bars(s, startIndex, endIndex):
 bars = {}&lt;/p&gt;

    &lt;p&gt;cnt = 0
 for i in range(len(s)):
     if s[i] == “|”:
         bars[i] = cnt
         cnt += 1
 lst = list(bars)
 startIndex, endIndex = [x-1 for x in startIndex], &lt;br /&gt;
                        [x-1 for x in endIndex]
 start_bar, end_bar = [], []&lt;/p&gt;

    &lt;p&gt;for i in range(len(startIndex)):
     start_bar.append(binary_start(lst, startIndex[i]))
 for i in range(len(endIndex)):
     end_bar.append(binary_end(lst, endIndex[i]))&lt;/p&gt;

    &lt;p&gt;print(bars)
 print(start_bar, end_bar)&lt;/p&gt;

    &lt;p&gt;res = 0
 for i in range(len(start_bar)):&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; res += max(0, (end_bar[i] - start_bar[i] - 1) - \
            (bars[end_bar[i]] - bars[start_bar[i]] - 1))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;return res&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;def binary_start(nums, target):
    left, right = 0, len(nums) -1&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;while left &amp;lt; right:
    mid = (left+right)//2
    if nums[mid] == target:
        return nums[mid]
    elif nums[mid] &amp;gt; target:
        right = mid
    else:
        left = mid + 1
return nums[right]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;def binary_end(nums, target):
    left, right = 0, len(nums) -1&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;while left &amp;lt; right:
    mid = (left+right+1)//2
    if nums[mid] == target:
        return nums[mid]
    elif nums[mid] &amp;gt; target:
        right = mid - 1
    else:
        left = mid  
return nums[left] ```
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Jiawei Lu</name><email>jl5999@columbia.edu</email></author><category term="studylog" /><category term="coding-interview" /><summary type="html">Two sum</summary></entry><entry><title type="html">NLP Lecture 04: Sequence Labeling with HMMs, P.O.S Tagging</title><link href="http://localhost:4000/studylog/NLP_lecture04.html" rel="alternate" type="text/html" title="NLP Lecture 04: Sequence Labeling with HMMs, P.O.S Tagging" /><published>2022-02-02T00:00:00-05:00</published><updated>2022-02-02T00:00:00-05:00</updated><id>http://localhost:4000/studylog/NLP_lecture04</id><content type="html" xml:base="http://localhost:4000/studylog/NLP_lecture04.html">&lt;p&gt;Keywords: Sequence Labeling with Hidden Markov Models, Part-of-Speech Tagging.&lt;/p&gt;

&lt;h2 id=&quot;garden-path-sentences&quot;&gt;Garden-Path Sentences&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;The horse raced past the barn.&lt;/li&gt;
  &lt;li&gt;The horse raced past the barn fell.&lt;/li&gt;
  &lt;li&gt;The old dog the footsteps of the young.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;parts-of-speech&quot;&gt;Parts-of-Speech&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Classes of words that behave alike:
    &lt;ul&gt;
      &lt;li&gt;Appear in similar contexts.&lt;/li&gt;
      &lt;li&gt;Perform a similar grammatical function in the sentence.&lt;/li&gt;
      &lt;li&gt;Undergo similar morphological transformations.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;~9 traditional parts-of-speech:
    &lt;ul&gt;
      &lt;li&gt;noun, pronoun, determiner, adjective, verb, adverb, preposition, conjunction, interjection&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;parts-of-speech-tagging&quot;&gt;Parts-of-Speech Tagging&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Goal: Translate from a sequence of words \((w_1, w_2, ..., w_n) \in V^*\), to a sequence of tags \((t_1, t_2, ..., t_n ) \in T^*\).&lt;/li&gt;
  &lt;li&gt;NLP is full of translation problems from one structure to another. Basic solution:
    &lt;ol&gt;
      &lt;li&gt;Construct search space of possible translations.&lt;/li&gt;
      &lt;li&gt;Find best paths through this space (decoding) according to some performance measure.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;bayesian-inference-for-sequence-labeling&quot;&gt;Bayesian Inference for Sequence Labeling&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Assume each word wi in the observed sequence \((w_1, w_2, ..., w_n) \in V^*\) was generated by some hidden variable \(t_i\).&lt;/li&gt;
  &lt;li&gt;Infer the most likely sequence of hidden variables given the sequence of observed words.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;markov-chains&quot;&gt;Markov Chains&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;A Markov chain is a sequence of random variables \(X_1, X_2, ...\)&lt;/li&gt;
  &lt;li&gt;The domain of these variables is a set of states.&lt;/li&gt;
  &lt;li&gt;Markov assumption: Next state depends only on current state.
\(P(X_{n+1}|X_1, X_2, ..., X_n) = P(X_{n+1}|X_n)\)&lt;/li&gt;
  &lt;li&gt;This is a special case of a weighted finite state automaton (WFSA).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hidden-markov-models-hmms&quot;&gt;Hidden Markov Models (HMMs)&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Generative (Bayesian) probability model.
  Observations: sequences of words.
  Hidden states: sequence of part-of-speech labels.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Hidden sequence is generated by an n-gram language model (typically a bi-gram model).
    &lt;ul&gt;
      &lt;li&gt;
\[t_0 = START\]
      &lt;/li&gt;
      &lt;li&gt;
\[P(t_1, t_2, ..., t_n) = \prod_{i=1}^nP(t_i|t_{i-1})\]
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;There are two types of probabilities:
  Transition Probabilities and Emission Probabilities.
  \begin{equation}
  P(t_1,t_2, …, t_n, w_1,w_2,…,w_n) = P(t_1|start)P(w_1|t_1)P(t_2|t_1)P(w_2|t_2) \cdots P(t_n|t_{n-1})P(w_n|t_n)
  \end{equation}
  \begin{equation}
  P(t_1,t_2, …, t_n, w_1,w_2,…,w_n) = \prod_{i=1}^nP(t_i|t_{i-1})P(w_i|t_i)
  \end{equation}&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;important-tasks-on-hmms&quot;&gt;Important Tasks on HMMs&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Decoding: 
  Given a sequence of words, find the most likely probability sequence. (Bayesian inference using Viterbi algorithm).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Evaluation: 
  Given a sequence of words, find the total probability for this word sequence given an HMM. Note that we can view the HMM as another type of language model. (Forward algorithm)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Training: 
  Estimate emission and transition probabilities from training data. (MLE, Forward-Backward a.k.a Baum-Welch algorithm)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;decoding-hmms&quot;&gt;Decoding HMMs&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Goal: Find the path with the highest total probability (given the words)
  \begin{equation}
  \arg \max_{t_1, …, t_n} \prod_{i=1}^n P(t_i|t_{i-1}) P(w_i|t_i)
  \end{equation}&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;viterbi-algorithm&quot;&gt;Viterbi Algorithm&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Input: Sequence of observed words \(w_1,..., w_n\).&lt;/li&gt;
  &lt;li&gt;Create a table \(\pi\), such that each entry \(\pi[k,t]\) contains the score of the highest-probability sequence ending in tag \(t\) at time \(k\).&lt;/li&gt;
  &lt;li&gt;initialize \(\pi[0,start]=1.0\) and \(\pi[0,t]=0.0\) for all tags \(t \in T\).&lt;/li&gt;
  &lt;li&gt;for \(k=1\) to \(n\):
    &lt;ul&gt;
      &lt;li&gt;for \(t \in T\):
        &lt;ul&gt;
          &lt;li&gt;
\[\pi[k,t] \leftarrow \max_s \pi[k-1, s]\cdot P(t|s) \cdot P(w_k|t)\]
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;return \(\max_s \pi[n,s]\).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;trigram-language-model&quot;&gt;Trigram Language Model&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Instead of using a unigram context, use a bigram context.
    &lt;ul&gt;
      &lt;li&gt;Think of this as having states that represent pairs of tags.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;So the HMM probability for a given tag and word sequence is:
  \begin{equation}
  \prod_{i=1}^nP(t_i|t_{i-2}t_{i-1})P(w_i|t_i)
  \end{equation}&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Need to handle data sparseness when estimating transition probabilities (for example using backoff or linear interpolation)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;more-pos-tagging-tricks&quot;&gt;More POS tagging tricks&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;It is also often useful in practice to add an end-of-sentence marker (just like we did for n-gram language models).
  \begin{equation}
  P(t_1,…,t_n,w_1,…,w_n) = [\prod_{i=1}^nP(t_i|t_{i-2}t_{i-1})P(w_i|t_i)]P(t_{n+1}|t_n)
  \end{equation}
  where \({t_{-1} = t_{0} = START}\) and \(t_{n+1} = STOP\).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Another useful trick is to replace words with “pseudo words” representing an entire class.&lt;/li&gt;
  &lt;li&gt;Using a smoothed trigram HMM model with these tricks, we can build a tagger that is close to the state-of-the art (~97% accuracy on the Penn Treebank).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hmms-as-language-models&quot;&gt;HMMs as Language Models&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;We can also use an HMM as language models (language generation, MT, …), i.e. evaluate \(P(w_1,...,w_n)\) for a given sentence. What is the advantage over a plain word n-gram model?&lt;/li&gt;
  &lt;li&gt;Problem: There are many tag-sequences that could have generated \(w_1, ..., w_n\). 
  \begin{equation}
  P(w_1,…,w_n,t_1,…,t_n) = \prod_{i=1}^nP(t_i|t_{i-1})P(w_i|t_i)
  \end{equation}&lt;/li&gt;
  &lt;li&gt;This is an example of spurious ambiguity.&lt;/li&gt;
  &lt;li&gt;Need to compute: 
  \begin{equation}
  P(w_1,…,w_n) = \sum_{t_1,…,t_n}P(w_1,…,w_n,t_1,…,t_n) = \sum_{t_1,…,t_n} [\prod_{i=1}^nP(t_i|t_{i-1})P(w_i|t_i)]
  \end{equation}&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;forward-algorithm&quot;&gt;Forward Algorithm&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Input: Sequence of observed words \(w_1,..., w_n\).&lt;/li&gt;
  &lt;li&gt;Create a table \(\pi\), such that each entry \(\pi[k,t]\) contains the score of the highest-probability sequence ending in tag \(t\) at time \(k\).&lt;/li&gt;
  &lt;li&gt;initialize \(\pi[0,start]=1.0\) and \(\pi[0,t]=0.0\) for all tags \(t \in T\).&lt;/li&gt;
  &lt;li&gt;for \(k=1\) to \(n\):
    &lt;ul&gt;
      &lt;li&gt;for \(t \in T\):
        &lt;ul&gt;
          &lt;li&gt;
\[\pi[k,t] \leftarrow \sum_s \pi[k-1, s]\cdot P(t|s) \cdot P(w_k|t)\]
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;return \(\sum_s \pi[n,s]\).&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jiawei Lu</name><email>jl5999@columbia.edu</email></author><category term="studylog" /><category term="nlp" /><summary type="html">Keywords: Sequence Labeling with Hidden Markov Models, Part-of-Speech Tagging.</summary></entry></feed>